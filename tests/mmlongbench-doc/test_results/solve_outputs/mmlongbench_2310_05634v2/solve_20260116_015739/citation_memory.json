{
  "version": "1.1",
  "created_at": "2026-01-16T01:57:39.644208",
  "updated_at": "2026-01-16T01:58:55.283018",
  "citations": [
    {
      "cite_id": "[rag-1]",
      "tool_type": "rag_hybrid",
      "query": "What are the steps involved in dataset construction and how are they described in detail?",
      "raw_result": "The dataset construction process, particularly as it pertains to the **BioKaLMA** dataset, is implemented through an **Automatic Pipeline** that consists of three distinct and iterative steps: **Person Selection**, **Name Disambiguation**, and **Evolutionary Question Generation**. These steps are designed to generate high-quality, complex, and attributable biographical questions at scale by leveraging structured knowledge from **WikiData** and human-curated sources.\n\n---\n\n### Step 1: Person Selection\n\nThis is the foundational step in the pipeline. It involves selecting individuals — typically notable persons — from a **Biographical Database** using **SPARQL queries**. The goal is to extract name pairs and short paragraphs that contain relation triples (e.g., “X was born in Y” or “X worked with Z”). These selections are crucial for ensuring that the generated questions are grounded in real-world biographical facts and are complex enough to challenge language models.\n\nThe selected name pairs are drawn from a **Human Annotated Set**, which guarantees accuracy in the relation triples extracted. This step ensures that the data source is both rich and reliable, forming the basis for subsequent disambiguation and question generation.\n\n---\n\n### Step 2: Name Disambiguation\n\nOnce candidate persons are selected, the next step is to resolve any ambiguity in their names. For example, multiple people may share the same name (e.g., “John Smith”), so this step maps each name to a unique entity in **WikiData** using its **QID** (a unique identifier assigned to every entity in WikiData).\n\nThis disambiguation is performed via **SPARQL queries** that retrieve entity-centered sub-graphs from WikiData. The output of this step is the **Knowledge Pool** — a collection of one-hop sub-graphs centered around the selected person nodes, containing properties such as gender, birth date, occupation, and relationships with other entities.\n\nThis step ensures that all subsequent knowledge injection and question generation are anchored to unambiguous, machine-readable entities, enabling precise attribution and retrieval.\n\n---\n\n### Step 3: Evolutionary Question Generation\n\nThis is the most innovative and iterative phase of the pipeline. Inspired by techniques like **WizardLM** and **DKGen**, this step uses **Large Language Models (LLMs)** to iteratively extend the initial paragraph with additional knowledge from the **Knowledge Pool** and generate increasingly complex questions.\n\nDuring each iteration, a **Data Selection Algorithm** selects appropriate relational knowledge based on a composite scoring function called **Score_r**, which balances **informativeness** and **readability**. The score considers:\n- The frequency of a specific relation type across the dataset (`Count_r`)\n- The total number of relations (`N`), used for normalization\n- A specificity component that favors less common but more informative relations\n\nAs knowledge is incrementally injected into the paragraph, new questions are generated that require understanding of the newly added information. This process continues for up to five iterations, as detailed in **Appendix E**, to ensure sufficient complexity without becoming overly tedious.\n\nThe final output of this step is a **Minimum Knowledge Set** — the smallest set of structured knowledge triplets required to answer each generated question — which serves as the ground truth for evaluating citation recall and precision.\n\n---\n\n### Evaluation and Validation\n\nThe resulting **BioKaLMA dataset** is evaluated using four key metrics:\n- **Authenticity**: Whether the generated question aligns with objective facts (verified against Wikipedia).\n- **Relevance**: Whether the knowledge set supports the question without redundancy.\n- **Naturalness**: How concise and understandable the question is to human readers (scored 1–5).\n- **Significance**: How meaningful or interesting the question is to annotators.\n\nThese evaluations are conducted via **Human Evaluation**, where trained annotators assess each metric based on predefined instructions (detailed in **Table 8** and **Appendix C**). The results show strong correlation between human judgments and automated metrics, validating the reliability of the automatic evaluation framework.\n\nAdditionally, the entire pipeline is documented in detail in **Appendix A**, providing transparency and reproducibility for researchers.\n\n---\n\n### Summary\n\nIn essence, the dataset construction pipeline for BioKaLMA is a sophisticated, multi-stage process that transforms raw biographical data into a benchmark-quality dataset through:\n1. **Curated selection** of persons and relations,\n2. **Precise disambiguation** using WikiData’s structured knowledge,\n3. **Iterative enrichment** and question generation powered by LLMs and guided by a knowledge-scoring algorithm.\n\nThis approach not only scales dataset creation but also ensures high fidelity, complexity, and traceability — critical features for evaluating **Knowledge-aware Language Model Attribution (KaLMA)** systems.\n\n### References\n\n- [1] Appendix A\n- [2] Appendix C\n- [3] Appendix E\n- [4] Table 1\n- [5] Human Evaluation Scores for BioKaLMA Dataset Questions (table)",
      "source": "Appendix A, Appendix C, Appendix E, Table 1, Human Evaluation Scores for BioKaLMA Dataset Questions (table)",
      "content": "Among the three steps in the BioKaLMA dataset construction pipeline — Person Selection, Name Disambiguation, and Evolutionary Question Generation — the third step, Evolutionary Question Generation, is described in the most detail. It involves iterative knowledge injection using LLMs, a composite scoring function (Score_r) balancing informativeness and readability, up to five iterations, and produces a Minimum Knowledge Set for evaluation. This step includes algorithmic details, scoring components, and references to appendices (Appendix E), making it the most verbose and technically elaborated phase.",
      "stage": "analysis",
      "step_id": null,
      "metadata": {
        "identifier": null,
        "extracted_sources": [
          {
            "reference_id": "[1]",
            "source": "Appendix A",
            "content": "Documents the entire pipeline for transparency and reproducibility."
          },
          {
            "reference_id": "[2]",
            "source": "Appendix C",
            "content": "Details human evaluation instructions for authenticity, relevance, naturalness, and significance."
          },
          {
            "reference_id": "[3]",
            "source": "Appendix E",
            "content": "Describes the iterative process of evolutionary question generation, including up to five iterations."
          },
          {
            "reference_id": "[4]",
            "source": "Table 1",
            "content": "Likely contains metrics or structure related to the dataset construction process."
          },
          {
            "reference_id": "[5]",
            "source": "Human Evaluation Scores for BioKaLMA Dataset Questions (table)",
            "content": "Presents annotated scores for generated questions across evaluation dimensions."
          }
        ]
      },
      "created_at": "2026-01-16T01:58:28.898262",
      "updated_at": "2026-01-16T01:58:47.090613"
    }
  ],
  "tool_counters": {
    "rag": 1
  }
}