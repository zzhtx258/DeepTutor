{
  "version": "1.1",
  "created_at": "2026-01-16T02:17:25.051445",
  "updated_at": "2026-01-16T02:22:01.708944",
  "citations": [
    {
      "cite_id": "[rag-1]",
      "tool_type": "rag_hybrid",
      "query": "What figure in the document has a tree shape, and what are the names of its branches along with their respective leaf counts?",
      "raw_result": "Based on the provided context, the figure with a tree shape is the **RAG Technology Tree Evolution Diagram (image)**. This diagram visually maps the evolution of Retrieval-Augmented Generation (RAG) research from 2020 to 2024 and is also referred to as the **Technology Tree Diagram**.\n\nThe diagram categorizes RAG research into three main stages, which serve as its primary branches:\n\n1.  **Pre-Training**: This branch represents research focused on enhancing language models during their initial training phase using retrieval-augmented techniques.\n2.  **Fine-Tuning**: This branch represents research where retrieval-augmented techniques are increasingly integrated with model adaptation after initial training.\n3.  **Inference**: This branch represents the stage that initially dominated early RAG work, leveraging LLMs' in-context learning abilities for generating outputs.\n\nHowever, the provided context does *not* contain specific information about the number of \"leaf\" nodes (individual methods or systems) associated with each of these three main branches. While it mentions that the diagram categorizes \"over 50 methods\" and lists several specific methods like RAPTOR, PROMPTAGATOR, RePLUG, RA-DIT, HyKGE, etc., it does not provide a count of leaves per branch.\n\nTherefore, while the structure (three main branches: Pre-Training, Fine-Tuning, Inference) is identified, the leaf counts for each branch cannot be determined from the given information.\n\n### References\n\n- [1] RAG Technology Tree Evolution Diagram (image)\n- [2] Technology Tree Diagram\n- [3] Technology Tree of RAG Research\n- [4] Pre-Training\n- [5] Fine-Tuning",
      "source": "RAG Technology Tree Evolution Diagram (image), Technology Tree Diagram, Technology Tree of RAG Research, Pre-Training, Fine-Tuning",
      "content": "The figure with a tree shape is the RAG Technology Tree Evolution Diagram, which has three main branches: Pre-Training, Fine-Tuning, and Inference. However, the provided information does not specify the number of leaf nodes (individual methods) for each branch, so it is not possible to determine which branch has the least leaves. The diagram mentions over 50 methods in total but does not break them down by branch.",
      "stage": "analysis",
      "step_id": null,
      "metadata": {
        "identifier": null,
        "extracted_sources": [
          {
            "reference_id": "[1]",
            "source": "RAG Technology Tree Evolution Diagram (image)",
            "content": "The figure with a tree shape is the RAG Technology Tree Evolution Diagram."
          },
          {
            "reference_id": "[2]",
            "source": "Technology Tree Diagram",
            "content": "Also referred to as the Technology Tree Diagram, it maps RAG research evolution from 2020 to 2024."
          },
          {
            "reference_id": "[3]",
            "source": "Technology Tree of RAG Research",
            "content": "Categorizes RAG research into three main stages: Pre-Training, Fine-Tuning, and Inference."
          },
          {
            "reference_id": "[4]",
            "source": "Pre-Training",
            "content": "Branch representing research focused on enhancing language models during initial training using retrieval-augmented techniques."
          },
          {
            "reference_id": "[5]",
            "source": "Fine-Tuning",
            "content": "Branch representing research where retrieval-augmented techniques are integrated with model adaptation after initial training."
          }
        ]
      },
      "created_at": "2026-01-16T02:17:58.560018",
      "updated_at": "2026-01-16T02:18:08.731871"
    },
    {
      "cite_id": "[rag-2]",
      "tool_type": "rag_hybrid",
      "query": "What are the exact numbers of leaf nodes (individual methods) shown for each branch (Pre-Training, Fine-Tuning, Inference) in the RAG Technology Tree Evolution Diagram?",
      "raw_result": "Based on the provided context, there is no explicit numerical breakdown of leaf nodes (individual methods) for each branch — Pre-Training, Fine-Tuning, and Inference — in the **RAG Technology Tree Evolution Diagram**.\n\nWhile the diagram is described as visually mapping the evolution of RAG research from 2020 to 2024 and categorizing “over 50 methods” into the three stages, the exact count of leaf nodes per branch is not specified in any of the knowledge graph entities or relationships. The context mentions that:\n\n- The diagram illustrates how RAG research shifted from focusing on **Inference** to increasingly incorporating **Fine-tuning** and **Pre-training** techniques.\n- It includes specific method nodes such as **RAPTOR**, **HyKGE**, **PROMPTAGATOR**, **RA-DIT**, **RePLUG**, **GPT-3**, **ChatGPT**, and **GPT-4**, but does not quantify how many fall under each branch.\n- The **Summary of RAG Methods Table (table)** catalogs 77 RAG methods and links them to augmentation phases (including Pre-training, Tuning/Fine-tuning, and Inference), but this table is not explicitly tied to the leaf node counts in the diagram itself.\n\nTherefore, without direct enumeration from the diagram’s visual structure or an accompanying data source within the context, the exact numbers of leaf nodes per branch cannot be determined.\n\n### References\n\n- [1] RAG Technology Tree Evolution Diagram (image)\n- [2] Technology Tree of RAG Research\n- [3] Summary of RAG Methods Table (table)",
      "source": "RAG Technology Tree Evolution Diagram (image), Summary of RAG Methods Table (table)",
      "content": "The RAG Technology Tree Evolution Diagram visually maps over 50 methods across three branches (Pre-Training, Fine-Tuning, Inference), but does not provide exact counts of leaf nodes for each branch. While specific methods like RAPTOR, HyKGE, and GPT-4 are mentioned, their categorization into branches is not quantified in the available context. The Summary of RAG Methods Table lists 77 methods with phase associations, but it is not directly tied to the diagram’s leaf node structure.",
      "stage": "analysis",
      "step_id": null,
      "metadata": {
        "identifier": null,
        "extracted_sources": [
          {
            "reference_id": "[1]",
            "source": "RAG Technology Tree Evolution Diagram (image)",
            "content": "Described as mapping RAG research evolution from 2020–2024 with over 50 methods categorized into Pre-Training, Fine-Tuning, and Inference, but without explicit leaf node counts per branch."
          },
          {
            "reference_id": "[3]",
            "source": "Summary of RAG Methods Table (table)",
            "content": "Catalogs 77 RAG methods linked to augmentation phases including Pre-training, Tuning/Fine-tuning, and Inference, but not explicitly tied to the diagram’s leaf node enumeration."
          }
        ]
      },
      "created_at": "2026-01-16T02:18:39.714158",
      "updated_at": "2026-01-16T02:18:47.688320"
    },
    {
      "cite_id": "[rag-3]",
      "tool_type": "rag_hybrid",
      "query": "What are all the methods listed in the Summary of RAG Methods Table, including their associated phases (Pre-Training, Fine-Tuning, Inference)?",
      "raw_result": "The **Summary of RAG Methods Table** catalogs 77 distinct Retrieval-Augmented Generation (RAG) methods, each associated with specific augmentation phases: **Pre-Training**, **Fine-Tuning**, or **Inference**. These phases indicate when the retrieval-augmented technique is applied during the model’s lifecycle.\n\nBelow is a comprehensive list of all methods extracted from the knowledge graph, grouped by their associated augmentation phase:\n\n---\n\n### **Pre-Training Phase Methods**\n\nThese methods enhance language models during their initial training phase using retrieval-augmented techniques:\n\n- **CoG [29]** — Uses Pre-training augmentation.\n- **Atlas [42]** — Applies Pre-training augmentation.\n- **RETRO++ [44]** — Uses Pre-training Corpus as retrieval source with Pre-training augmentation.\n- **INSTRUCTRETRO [45]** — Uses Pre-training Corpus with Pre-training augmentation and Iterative retrieval.\n\n---\n\n### **Fine-Tuning Phase Methods**\n\nThese methods involve adapting pre-trained models to specific tasks by adjusting parameters using task-specific data during fine-tuning:\n\n- **EAR [31]** — Uses Tuning augmentation.\n- **FLARE [24]** — Uses Tuning augmentation.\n- **RRR [7]** — Uses Search Engine as retrieval source with Tuning augmentation.\n- **RA-DIT [27]** — Uses Common Crawl and Wikipedia with Tuning augmentation and Once retrieval.\n- **UPRISE [20]** — Uses Dataset-base with Tuning augmentation and Once retrieval.\n- **CT-RAG [41]** — Uses Synthesized dataset with Tuning augmentation and Once retrieval.\n- **RAST [32]** — Uses Dataset-base with Tuning augmentation and Once retrieval.\n- **R-GQA [37]** — Uses Dataset-base with Tuning augmentation and Once retrieval.\n- **Self-RAG [25]** — Uses Wikipedia with Tuning augmentation and Once retrieval.\n- **RA-e2e [46]** — Uses Dataset-base with Tuning augmentation and Once Once retrieval.\n- **RA-Long-Form [49]** — Uses Dataset-base with Tuning augmentation and Once Once retrieval.\n- **PROMPTAGATOR [21]** — Uses BEIR as retrieval source with Tuning augmentation.\n- **RAG-Robust [48]** — Uses Wikipedia with Tuning augmentation.\n- **PKG** — Integrates knowledge via directive fine-tuning by substituting the retriever module.\n\n---\n\n### **Inference Phase Methods**\n\nThese methods leverage LLMs’ in-context learning abilities during inference to generate outputs conditioned on retrieved information:\n\n- **ITER-RETGEN [14]** — Uses Wikipedia with Inference augmentation and Iterative retrieval.\n- **RECITE [22]** — Uses LLMs as retrieval source with Inference augmentation and Once retrieval.\n- **PGRA [33]** — Uses Wikipedia with Inference augmentation and Once retrieval.\n- **LLM-R [38]** — Uses Dataset-base with Inference augmentation and Iterative retrieval.\n- **ToC [57]** — Uses Search Engine and Wikipedia with Inference augmentation and Recursive retrieval.\n- **Token-Elimination [52]** — Uses Wikipedia with Inference augmentation and Iterative retrieval.\n- **RePLUG [72]** — Uses Pile with Inference augmentation and Iterative retrieval.\n- **RAPTOR [63]** — Uses Dataset-base with Inference augmentation and Recursive retrieval.\n- **UniMS-RAG [74]** — Uses Dataset-base with Inference augmentation and Iterative retrieval.\n\n---\n\n### Notes\n\n- The table also includes methods like **CREA-ICL [19]** (Multi granularity), **TIGER [39]** (Item-base), **LM-Indexer [40]** (Item-base), and **BEQUE [9]** (Item-base), but their specific augmentation phase is not explicitly listed in the provided context.\n- **Inference** is the most frequent augmentation strategy, used in 28 methods.\n- **Fine-Tuning** is applied in multiple methods including EAR, FLARE, Self-Mem, and others, showing that RAG and Fine-Tuning are often combined rather than mutually exclusive.\n- Some methods, such as **Self-Mem [17]** and **EAR [31]**, are noted for combining RAG with fine-tuning, highlighting hybrid approaches.\n\nThis structured overview reflects the diversity and evolution of RAG methodologies across different stages of model development.\n\n### References\n\n- [1] TABLE I SUMMARY OF RAG METHODS\n- [2] Summary of RAG Methods Table (table)\n- [3] Retrieval-Augmented Generation\n- [4] Fine-tuning\n- [5] Inference",
      "source": "TABLE I SUMMARY OF RAG METHODS, Summary of RAG Methods Table (table), Retrieval-Augmented Generation, Fine-tuning, Inference",
      "content": "The Summary of RAG Methods Table lists 77 distinct Retrieval-Augmented Generation (RAG) methods, categorized by their augmentation phase: Pre-Training, Fine-Tuning, or Inference. Pre-Training methods include CoG, Atlas, RETRO++, and INSTRUCTRETRO. Fine-Tuning methods include EAR, FLARE, RRR, RA-DIT, UPRISE, CT-RAG, RAST, R-GQA, Self-RAG, RA-e2e, RA-Long-Form, PROMPTAGATOR, RAG-Robust, and PKG. Inference methods include ITER-RETGEN, RECITE, PGRA, LLM-R, ToC, Token-Elimination, RePLUG, RAPTOR, and UniMS-RAG. Inference is the most common phase, used in 28 methods. Some methods like CREA-ICL, TIGER, LM-Indexer, and BEQUE are mentioned but lack explicit phase assignments. Hybrid approaches combining RAG with fine-tuning (e.g., Self-Mem, EAR) are also noted.",
      "stage": "analysis",
      "step_id": null,
      "metadata": {
        "identifier": null,
        "extracted_sources": [
          {
            "reference_id": "[1]",
            "source": "TABLE I SUMMARY OF RAG METHODS",
            "content": "The table catalogs 77 distinct RAG methods grouped by augmentation phase: Pre-Training, Fine-Tuning, or Inference."
          },
          {
            "reference_id": "[2]",
            "source": "Summary of RAG Methods Table (table)",
            "content": "Lists all methods with their associated phases and retrieval sources."
          },
          {
            "reference_id": "[3]",
            "source": "Retrieval-Augmented Generation",
            "content": "General context for RAG methodologies across model development stages."
          },
          {
            "reference_id": "[4]",
            "source": "Fine-tuning",
            "content": "Describes adaptation of pre-trained models using task-specific data during fine-tuning."
          },
          {
            "reference_id": "[5]",
            "source": "Inference",
            "content": "Describes use of LLMs’ in-context learning during inference to generate outputs conditioned on retrieved information."
          }
        ]
      },
      "created_at": "2026-01-16T02:19:38.794249",
      "updated_at": "2026-01-16T02:19:54.290719"
    },
    {
      "cite_id": "[rag-4]",
      "tool_type": "rag_hybrid",
      "query": "What are the exact counts of methods categorized under Pre-Training and Fine-Tuning in the Summary of RAG Methods Table?",
      "raw_result": "Based on the provided context, the **Summary of RAG Methods Table** catalogs 77 Retrieval-Augmented Generation (RAG) methods. While the table includes dimensions such as *Augmentation Phase* (which includes Pre-training, Tuning/Fine-tuning, and Inference), the exact counts of methods categorized under **Pre-Training** and **Fine-Tuning** are not explicitly stated in the available knowledge graph data.\n\nThe context confirms that:\n- **Pre-training** is an augmentation strategy used in methods like **CoG [29]**, **Atlas [42]**, **RETRO++ [44]**, **INSTRUCTRETRO [45]**, **RAVEN [43]**, and **TIGER [39]**.\n- **Tuning (Fine-tuning)** is an augmentation strategy used in methods like **EAR [31]**, **Self-Mem [17]**, **FLARE [24]**, **RAST [32]**, **R-GQA [37]**, **UPRISE [20]**, **CT-RAG [41]**, **LM-Indexer [40]**, **CREA-ICL [19]**, **RAG-Robust [48]**, **CoN [50]**, **Self-RAG [25]**, **RA-DIT [27]**, **PROMPTAGATOR [21]**, **Retrieve-and-Sample [65]**, and **RRR [7]**.\n\nHowever, without a direct count from the table or an explicit summary of how many of the 77 methods fall under each category, it is not possible to provide exact numerical counts for Pre-Training and Fine-Tuning based solely on the given information.\n\n### References\n\n- [1] Summary of RAG Methods Table (table)\n- [2] TABLE I SUMMARY OF RAG METHODS\n- [3] Pre-training\n- [4] Tuning\n- [5] Fine-tuning",
      "source": "Summary of RAG Methods Table (table), TABLE I SUMMARY OF RAG METHODS, Pre-training, Tuning, Fine-tuning",
      "content": "The Summary of RAG Methods Table includes 77 methods categorized by augmentation phase, including Pre-training and Fine-tuning. While specific methods are listed under each category (e.g., CoG, Atlas for Pre-training; EAR, Self-Mem for Fine-tuning), the exact numerical counts for each category are not provided in the available data. The document references the table and related terms but does not contain the aggregated counts requested.",
      "stage": "analysis",
      "step_id": null,
      "metadata": {
        "identifier": null,
        "extracted_sources": [
          {
            "reference_id": "[1]",
            "source": "Summary of RAG Methods Table (table)",
            "content": "Catalogs 77 RAG methods with dimensions including Augmentation Phase (Pre-training, Tuning/Fine-tuning, Inference)."
          },
          {
            "reference_id": "[2]",
            "source": "TABLE I SUMMARY OF RAG METHODS",
            "content": "Lists methods grouped by augmentation strategy, but does not provide exact counts per category."
          },
          {
            "reference_id": "[3]",
            "source": "Pre-training",
            "content": "Mentions methods using pre-training as an augmentation strategy: CoG, Atlas, RETRO++, INSTRUCTRETRO, RAVEN, TIGER."
          },
          {
            "reference_id": "[4]",
            "source": "Tuning",
            "content": "Refers to tuning as an augmentation phase used in multiple RAG methods."
          },
          {
            "reference_id": "[5]",
            "source": "Fine-tuning",
            "content": "Lists methods using fine-tuning: EAR, Self-Mem, FLARE, RAST, R-GQA, UPRISE, CT-RAG, LM-Indexer, CREA-ICL, RAG-Robust, CoN, Self-RAG, RA-DIT, PROMPTAGATOR, Retrieve-and-Sample, RRR."
          }
        ]
      },
      "created_at": "2026-01-16T02:20:26.343550",
      "updated_at": "2026-01-16T02:20:37.711250"
    },
    {
      "cite_id": "[rag-5]",
      "tool_type": "rag_hybrid",
      "query": "What are the exact numbers of leaf nodes (individual methods) for each branch (Pre-Training, Fine-Tuning, Inference) in the RAG Technology Tree Evolution Diagram?",
      "raw_result": "Based on the provided context, there is no explicit numerical breakdown of leaf nodes (individual methods) for each branch — Pre-Training, Fine-Tuning, and Inference — in the RAG Technology Tree Evolution Diagram.\n\nWhile the diagram is described as visually mapping the evolution of RAG research from 2020 to 2024 and categorizing “over 50 methods” into these three stages, the exact count of methods per branch is not specified in any of the entities or relationships. The Summary of RAG Methods Table (table) catalogs 77 RAG methods and associates them with augmentation phases (Pre-training, Tuning, Inference), but it does not explicitly map those counts back to the visual structure of the RAG Technology Tree Evolution Diagram.\n\nAdditionally, no document chunk or reference provides a direct enumeration of leaf nodes per branch in the diagram.\n\nTherefore, the exact numbers of leaf nodes for each branch cannot be determined from the available information.\n\n### References\n\n- [1] RAG Technology Tree Evolution Diagram (image)\n- [2] Summary of RAG Methods Table (table)\n- [3] Technology Tree of RAG Research",
      "source": "RAG Technology Tree Evolution Diagram (image), Summary of RAG Methods Table (table), Technology Tree of RAG Research",
      "content": "No exact numbers of leaf nodes (individual methods) are provided for each branch (Pre-Training, Fine-Tuning, Inference) in the RAG Technology Tree Evolution Diagram. While the diagram categorizes over 50 methods into these three stages and a separate table lists 77 RAG methods with their associated phases, neither source maps specific leaf node counts to the visual tree structure. The document mentions the existence of the diagram and table but does not provide the requested numerical breakdown per branch.",
      "stage": "analysis",
      "step_id": null,
      "metadata": {
        "identifier": null,
        "extracted_sources": [
          {
            "reference_id": "[1]",
            "source": "RAG Technology Tree Evolution Diagram (image)",
            "content": "Described as visually mapping RAG research evolution from 2020 to 2024, categorizing over 50 methods into Pre-Training, Fine-Tuning, and Inference stages, but no exact leaf node counts per branch are given."
          },
          {
            "reference_id": "[2]",
            "source": "Summary of RAG Methods Table (table)",
            "content": "Catalogs 77 RAG methods and associates them with augmentation phases (Pre-training, Tuning, Inference), but does not map these counts back to the visual structure of the RAG Technology Tree Evolution Diagram."
          },
          {
            "reference_id": "[3]",
            "source": "Technology Tree of RAG Research",
            "content": "General reference to the conceptual framework; does not provide explicit enumeration of leaf nodes per branch in the diagram."
          }
        ]
      },
      "created_at": "2026-01-16T02:21:01.828367",
      "updated_at": "2026-01-16T02:21:10.671194"
    },
    {
      "cite_id": "[rag-6]",
      "tool_type": "rag_hybrid",
      "query": "What are the EXACT numbers of leaf nodes (individual methods) for each branch (Pre-Training, Fine-Tuning, Inference) in the RAG Technology Tree Evolution Diagram?",
      "raw_result": "Based on the provided context, there is no explicit statement or data that gives the **exact numbers of leaf nodes (individual methods)** for each branch — **Pre-Training**, **Fine-Tuning**, and **Inference** — in the *RAG Technology Tree Evolution Diagram*.\n\nWhile the context mentions:\n\n- The diagram visually maps over 50 RAG methods across three stages: Pre-training, Fine-tuning, and Inference.\n- It categorizes these methods into the three branches.\n- Some specific methods are listed under each stage (e.g., CoG [29], Atlas [42] under Pre-training; EAR [31], Self-Mem [17] under Fine-tuning; FILCO [34], PGRA [33] under Inference).\n\nHowever, **no exact count of leaf nodes per branch is provided** in the knowledge graph or document chunks. The “Summary of RAG Methods Table (table)” catalogs 77 methods, but it does not break them down by branch in a way that allows for an exact leaf node count per tree branch.\n\nTherefore, based strictly on the available context:\n\n> I do not have enough information to answer the question about the exact numbers of leaf nodes for each branch in the RAG Technology Tree Evolution Diagram.\n\n### References\n\n- [1] RAG Technology Tree Evolution Diagram (image)\n- [2] Summary of RAG Methods Table (table)\n- [3] Technology Tree of RAG Research\n- [4] Retrieval-Augmented Generation\n- [5] Inference",
      "source": "RAG Technology Tree Evolution Diagram (image), Summary of RAG Methods Table (table), Technology Tree of RAG Research",
      "content": "No exact numbers of leaf nodes (individual methods) are provided for each branch (Pre-Training, Fine-Tuning, Inference) in the RAG Technology Tree Evolution Diagram. The diagram visually maps over 50 methods across these three stages and lists some examples per branch, but no precise leaf node counts per branch are available. A separate table catalogs 77 RAG methods overall but does not break them down by branch for exact counting.",
      "stage": "analysis",
      "step_id": null,
      "metadata": {
        "identifier": null,
        "extracted_sources": [
          {
            "reference_id": "[1]",
            "source": "RAG Technology Tree Evolution Diagram (image)"
          },
          {
            "reference_id": "[2]",
            "source": "Summary of RAG Methods Table (table)"
          },
          {
            "reference_id": "[3]",
            "source": "Technology Tree of RAG Research"
          }
        ]
      },
      "created_at": "2026-01-16T02:21:39.612964",
      "updated_at": "2026-01-16T02:21:46.007238"
    }
  ],
  "tool_counters": {
    "rag": 6
  }
}