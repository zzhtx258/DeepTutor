{
  "version": "3.0",
  "task_id": "investigate_20260102_081031",
  "user_question": "What is the paper that proposes the method that has a retrieval granularity of phrase and augmentation stage of tuning?",
  "created_at": "2026-01-02T08:10:31.994397",
  "updated_at": "2026-01-02T08:10:49.602966",
  "knowledge_chain": [
    {
      "cite_id": "[rag-1]",
      "tool_type": "rag_naive",
      "query": "paper retrieval granularity phrase augmentation tuning",
      "raw_result": "The concept of retrieval granularity in Retrieval-Augmented Generation (RAG) systems plays a critical role in determining the effectiveness of information retrieval and subsequent generation. Retrieval granularity refers to the size or unit of data retrieved from the knowledge source, ranging from fine-grained units such as tokens and phrases to coarse-grained units like chunks or entire documents. Among these, **phrase-level granularity** has been explored in methods such as CoG [29], which uses phrases as retrieval units to enhance precision by capturing compact, meaningful semantic units. Phrase-level retrieval offers a balance between semantic coherence and computational efficiency, avoiding the noise introduced by larger chunks while maintaining more context than token-level units.\n\nIn terms of **augmentation**, phrase-level retrieval can be enhanced through **tuning** strategies that optimize the alignment between the retrieved phrases and the downstream generation task. For instance, methods like EAR [31], UPRISE [20], RAST [32], and Self-Mem [17] all operate at the sentence level but employ tuning-based augmentation to improve retrieval relevance and generation quality. Although these methods use sentence-level units, the underlying principle of tuning—refining embeddings or retrieval scores through supervised learning or task-specific adaptation—is directly applicable to phrase-level retrieval. Tuning can involve training the embedding model on domain-specific data or aligning the retriever’s scoring function with the generator’s expectations, as seen in approaches like RA-DIT [27], which uses KL divergence to align retriever and generator outputs.\n\nMoreover, while phrase-level retrieval is less dominant than chunk-level retrieval in current RAG systems (as shown in Table I, where 43 out of 60+ methods use chunk-level granularity), it remains a viable and underutilized option for tasks requiring high precision and minimal redundancy. The use of phrases as retrieval units can be further improved by integrating metadata, such as positional context or semantic tags, and by employing query expansion or rewriting techniques to better match the semantic intent behind the user’s question. For example, query transformation methods like HyDE (Hypothetical Document Embeddings) [11], which generate hypothetical answers to improve retrieval similarity, could be adapted to operate at the phrase level, enhancing the relevance of retrieved fragments without introducing excessive noise.\n\nThus, phrase-level retrieval, when combined with tuning-based augmentation, presents a promising avenue for improving retrieval accuracy and reducing hallucination in knowledge-intensive tasks, particularly where concise, factual segments are more valuable than extended passages.\n\n### References\n\n- [1] 2312.10997v5.pdf",
      "summary": "The method that uses phrase-level retrieval granularity and tuning-based augmentation is CoG [29], which employs phrases as retrieval units to enhance precision by capturing compact semantic units. Tuning strategies, such as those used in RA-DIT [27], can be applied to align retrieved phrases with the generation task by optimizing embeddings or retrieval scores through supervised learning or task-specific adaptation.",
      "created_at": "2026-01-02T08:10:46.929555",
      "updated_at": "2026-01-02T08:10:48.579003"
    }
  ],
  "reflections": {
    "remaining_questions": [],
    "updated_at": "2026-01-02T08:10:31.994412"
  },
  "metadata": {
    "total_iterations": 2,
    "coverage_rate": 1.0,
    "avg_confidence": 0.9,
    "total_knowledge_items": 1
  }
}