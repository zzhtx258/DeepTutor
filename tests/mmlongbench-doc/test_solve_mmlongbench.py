#!/usr/bin/env python
"""
æµ‹è¯•è„šæœ¬ï¼šä½¿ç”¨ DeepTutor solve æ¨¡å—æµ‹è¯• MMLongBench-Doc åŸºå‡†

è¯¥è„šæœ¬ä¼šï¼š
1. è¯»å– MMLongBench-Doc çš„ samples.json
2. å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œä½¿ç”¨ MainSolver æ¥è§£å†³
3. ä»ç­”æ¡ˆä¸­æå–ç»“æœå¹¶è¯„ä¼°
4. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

æ³¨æ„ï¼š
- æ‰€æœ‰æ“ä½œå¿…é¡»åœ¨ deeptutor conda ç¯å¢ƒä¸­è¿›è¡Œ
- è¿è¡Œå‰è¯·ç¡®ä¿ï¼šconda activate deeptutor
- Web search åŠŸèƒ½åœ¨æµ‹è¯•æ—¶è‡ªåŠ¨ç¦ç”¨ï¼ˆé€šè¿‡è®¾ç½® config.tools.web_search.enabled = Falseï¼‰
"""

import argparse
import asyncio
import json
import os
import re
import shutil
import sys
from pathlib import Path
from typing import Any, Dict, List
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# æ·»åŠ  MMLongBench-Doc è¯„ä¼°æ¨¡å—åˆ°è·¯å¾„
mmlongbench_root = project_root.parent / "MMLongBench-Doc"
if mmlongbench_root.exists():
    sys.path.insert(0, str(mmlongbench_root))

from dotenv import load_dotenv
from tqdm import tqdm

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=False)

# å¯¼å…¥ DeepTutor solve æ¨¡å—
from src.agents.solve.main_solver import MainSolver
from src.knowledge.initializer import KnowledgeBaseInitializer
from src.knowledge.add_documents import DocumentAdder

# å¯¼å…¥ MMLongBench-Doc è¯„ä¼°æ¨¡å—ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å… OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–é—®é¢˜ï¼‰
def _import_eval_modules():
    """å»¶è¿Ÿå¯¼å…¥è¯„ä¼°æ¨¡å—"""
    try:
        # å…ˆå¯¼å…¥ eval_scoreï¼ˆä¸ä¾èµ– OpenAIï¼‰
        from eval.eval_score import eval_score, eval_acc_and_f1, show_results
        
        # å»¶è¿Ÿå¯¼å…¥ extract_answerï¼ˆéœ€è¦ OpenAI API keyï¼‰
        def extract_answer_lazy(question, output, prompt, model_name="gpt-4o"):
            """å»¶è¿Ÿå¯¼å…¥ extract_answer"""
            from eval.extract_answer import extract_answer
            return extract_answer(question, output, prompt, model_name)
        
        return {
            "eval_score": eval_score,
            "eval_acc_and_f1": eval_acc_and_f1,
            "show_results": show_results,
            "extract_answer": extract_answer_lazy,
        }
    except ImportError as e:
        print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥ MMLongBench-Doc è¯„ä¼°æ¨¡å—: {e}")
        print(f"é¢„æœŸè·¯å¾„: {mmlongbench_root}")
        return None


def _create_extract_answer_function(api_key: str, base_url: str, model_name: str = None):
    """
    åˆ›å»ºæ”¯æŒè‡ªå®šä¹‰ base_url çš„ extract_answer å‡½æ•°
    
    Args:
        api_key: API å¯†é’¥
        base_url: API ç«¯ç‚¹åœ°å€
        model_name: æ¨¡å‹åç§°ï¼ˆå¦‚æœä¸º Noneï¼Œä¼šä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼è·å–ï¼‰
    
    Returns:
        åŒ…è£…åçš„ extract_answer å‡½æ•°
    """
    from openai import OpenAI
    
    # åˆ›å»ºæ”¯æŒè‡ªå®šä¹‰ base_url çš„å®¢æˆ·ç«¯
    client = OpenAI(api_key=api_key, base_url=base_url)
    
    # å¦‚æœæœªæŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼
    if model_name is None:
        import os
        model_name = os.getenv("LLM_MODEL", "gpt-4o")
    
    def extract_answer(question, output, prompt, model_name_override=None):
        """
        ä»å“åº”ä¸­æå–ç­”æ¡ˆï¼ˆæ”¯æŒè‡ªå®šä¹‰ base_urlï¼‰
        
        Args:
            question: é—®é¢˜
            output: æ¨¡å‹è¾“å‡º
            prompt: æå–æç¤º
            model_name_override: æ¨¡å‹åç§°è¦†ç›–ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æå–çš„ç­”æ¡ˆ
        """
        try:
            use_model = model_name_override or model_name
            response = client.chat.completions.create(
                model=use_model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt,
                    },
                    {
                        "role": "assistant",
                        "content": "\n\nQuestion:{}\nAnalysis:{}\n".format(question, output)
                    }
                ],
                temperature=0.0,
                max_tokens=256,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0
            )
            response = response.choices[0].message.content
        except Exception as e:
            print(f"ç­”æ¡ˆæå– API è°ƒç”¨å¤±è´¥: {e}")
            response = "Failed"
        
        return response
    
    return extract_answer


class MMLongBenchTester:
    """MMLongBench-Doc æµ‹è¯•å™¨"""

    def __init__(
        self,
        samples_path: str,
        document_path: str,
        output_dir: str,
        kb_name: str = None,  # å·²å¼ƒç”¨ï¼Œç°åœ¨æ¯ä¸ªæ–‡æ¡£ä½¿ç”¨ç‹¬ç«‹çŸ¥è¯†åº“
        extractor_prompt_path: str = None,
        max_samples: int = None,
        start_index: int = 0,
        force_rerun: bool = False,
    ):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨

        Args:
            samples_path: samples.json æ–‡ä»¶è·¯å¾„
            document_path: PDF æ–‡æ¡£ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            kb_name: çŸ¥è¯†åº“åç§°ï¼ˆå·²å¼ƒç”¨ï¼Œç°åœ¨æ¯ä¸ªæ–‡æ¡£ä½¿ç”¨ç‹¬ç«‹çŸ¥è¯†åº“ï¼‰
            extractor_prompt_path: ç­”æ¡ˆæå–æç¤ºæ–‡ä»¶è·¯å¾„
            max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
            start_index: èµ·å§‹ç´¢å¼•ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        """
        self.samples_path = Path(samples_path)
        self.document_path = Path(document_path)
        self.output_dir = Path(output_dir)
        if kb_name:
            print(f"è­¦å‘Š: --kb_name å‚æ•°å·²å¼ƒç”¨ï¼Œç°åœ¨æ¯ä¸ªæ–‡æ¡£ä½¿ç”¨ç‹¬ç«‹çŸ¥è¯†åº“")
        self.max_samples = max_samples
        self.start_index = start_index
        self.force_rerun = force_rerun

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.results_file = self.output_dir / "results.json"
        self.report_file = self.output_dir / "report.txt"
        
        # å¦‚æœå¼ºåˆ¶é‡æ–°è¿è¡Œï¼Œåˆ é™¤æ—§çš„ç»“æœæ–‡ä»¶
        if self.force_rerun and self.results_file.exists():
            print(f"å¼ºåˆ¶é‡æ–°è¿è¡Œï¼šåˆ é™¤æ—§çš„ç»“æœæ–‡ä»¶ {self.results_file}")
            self.results_file.unlink()

        # åŠ è½½ç­”æ¡ˆæå–æç¤º
        if extractor_prompt_path:
            self.extractor_prompt_path = Path(extractor_prompt_path)
        else:
            # é»˜è®¤ä½¿ç”¨ MMLongBench-Doc çš„æç¤ºæ–‡ä»¶
            default_prompt = mmlongbench_root / "eval" / "prompt_for_answer_extraction.md"
            if default_prompt.exists():
                self.extractor_prompt_path = default_prompt
            else:
                raise FileNotFoundError(
                    f"æ‰¾ä¸åˆ°ç­”æ¡ˆæå–æç¤ºæ–‡ä»¶: {default_prompt}"
                )

        with open(self.extractor_prompt_path, "r", encoding="utf-8") as f:
            self.extractor_prompt = f.read()

        # å¯¼å…¥è¯„ä¼°æ¨¡å—
        self.eval_modules = _import_eval_modules()
        if self.eval_modules is None:
            raise ImportError("æ— æ³•å¯¼å…¥ MMLongBench-Doc è¯„ä¼°æ¨¡å—")
        
        # åˆ›å»ºæ”¯æŒè‡ªå®šä¹‰ base_url çš„ extract_answer å‡½æ•°
        import os
        from dotenv import load_dotenv
        project_root = Path(__file__).parent.parent.parent
        load_dotenv(project_root / ".env", override=False)
        
        # è·å– LLM é…ç½®
        api_key = os.getenv("LLM_BINDING_API_KEY")
        base_url = os.getenv("LLM_BINDING_HOST")
        model_name = os.getenv("LLM_MODEL", "gpt-4o")
        
        if not api_key or not base_url:
            raise ValueError(
                "LLM_BINDING_API_KEY å’Œ LLM_BINDING_HOST å¿…é¡»è®¾ç½®æ‰èƒ½è¿›è¡Œç­”æ¡ˆæå–"
            )
        
        # åˆ›å»ºæ”¯æŒè‡ªå®šä¹‰ä¾›åº”å•†çš„ extract_answer å‡½æ•°
        self.extract_answer_func = _create_extract_answer_function(
            api_key=api_key,
            base_url=base_url,
            model_name=model_name
        )

        # åŠ è½½æ ·æœ¬
        self.samples = self._load_samples()

        # çŸ¥è¯†åº“ç¼“å­˜ï¼ˆæ¯ä¸ªæ–‡æ¡£å¯¹åº”ä¸€ä¸ªçŸ¥è¯†åº“ï¼‰
        self.kb_cache = {}  # doc_id -> kb_name
        self.solver_cache = {}  # kb_name -> solver

        # çŸ¥è¯†åº“åŸºç¡€ç›®å½•
        self.kb_base_dir = Path("./data/knowledge_bases")

    def _load_samples(self) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ ·æœ¬"""
        if self.results_file.exists():
            # å¦‚æœç»“æœæ–‡ä»¶å­˜åœ¨ï¼Œä»ä¸­åŠ è½½ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
            print(f"ä»ç°æœ‰ç»“æœæ–‡ä»¶åŠ è½½: {self.results_file}")
            with open(self.results_file, "r", encoding="utf-8") as f:
                samples = json.load(f)
        else:
            # ä»åŸå§‹ samples.json åŠ è½½
            with open(self.samples_path, "r", encoding="utf-8") as f:
                samples = json.load(f)

        # é™åˆ¶æ ·æœ¬æ•°é‡
        if self.max_samples:
            samples = samples[: self.max_samples]

        # ä»æŒ‡å®šç´¢å¼•å¼€å§‹
        if self.start_index > 0:
            samples = samples[self.start_index:]

        print(f"åŠ è½½äº† {len(samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return samples

    def _get_kb_name_from_doc_id(self, doc_id: str) -> str:
        """
        ä»æ–‡æ¡£IDç”ŸæˆçŸ¥è¯†åº“åç§°
        
        Args:
            doc_id: æ–‡æ¡£IDï¼ˆå¦‚ "PH_2016.06.08_Economy-Final.pdf"ï¼‰
        
        Returns:
            çŸ¥è¯†åº“åç§°ï¼ˆå¦‚ "mmlongbench_PH_2016_06_08_Economy_Final"ï¼‰
        """
        # ç§»é™¤ .pdf æ‰©å±•å
        name = doc_id.replace(".pdf", "")
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
        name = re.sub(r'[^\w\-_]', '_', name)
        # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
        name = re.sub(r'_+', '_', name)
        # ç¡®ä¿ä»¥ mmlongbench_ å¼€å¤´
        if not name.startswith("mmlongbench_"):
            name = f"mmlongbench_{name}"
        return name

    async def _ensure_kb_for_document(self, doc_id: str) -> str:
        """
        ç¡®ä¿æ–‡æ¡£å¯¹åº”çš„çŸ¥è¯†åº“å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºå¹¶æ·»åŠ æ–‡æ¡£
        
        Args:
            doc_id: æ–‡æ¡£ID
        
        Returns:
            çŸ¥è¯†åº“åç§°
        """
        # æ£€æŸ¥ç¼“å­˜
        if doc_id in self.kb_cache:
            return self.kb_cache[doc_id]

        # ç”ŸæˆçŸ¥è¯†åº“åç§°
        kb_name = self._get_kb_name_from_doc_id(doc_id)
        self.kb_cache[doc_id] = kb_name

        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        kb_dir = self.kb_base_dir / kb_name
        doc_file = self.document_path / doc_id

        if not doc_file.exists():
            raise FileNotFoundError(f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_file}")

        if not kb_dir.exists():
            print(f"\nä¸ºæ–‡æ¡£ {doc_id} åˆ›å»ºçŸ¥è¯†åº“: {kb_name}")
            # åˆ›å»ºçŸ¥è¯†åº“
            initializer = KnowledgeBaseInitializer(
                kb_name=kb_name,
                base_dir=str(self.kb_base_dir),
            )
            initializer.create_directory_structure()

            # æ·»åŠ æ–‡æ¡£
            adder = DocumentAdder(
                kb_name=kb_name,
                base_dir=str(self.kb_base_dir),
            )
            added_files = adder.add_documents(
                source_files=[str(doc_file)],
                skip_duplicates=True,
            )
            print(f"  å·²æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“: {len(added_files)} ä¸ªæ–‡ä»¶")
            
            # å¤„ç†æ–‡æ¡£ï¼ˆå¼‚æ­¥ï¼‰
            if added_files:
                print(f"  æ­£åœ¨å¤„ç†æ–‡æ¡£ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
                await adder.process_new_documents(added_files)
                print(f"  æ–‡æ¡£å¤„ç†å®Œæˆ")
        else:
            # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å·²å¤„ç†ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰ rag_storage å†…å®¹ï¼‰
            rag_storage_dir = kb_dir / "rag_storage"
            if not rag_storage_dir.exists() or not any(rag_storage_dir.iterdir()):
                print(f"  çŸ¥è¯†åº“å­˜åœ¨ä½†æœªå¤„ç†ï¼Œæ­£åœ¨å¤„ç†æ–‡æ¡£...")
                adder = DocumentAdder(
                    kb_name=kb_name,
                    base_dir=str(self.kb_base_dir),
                )
                # æ£€æŸ¥ raw ç›®å½•ä¸­çš„æ–‡ä»¶
                raw_dir = kb_dir / "raw"
                if raw_dir.exists():
                    raw_files = list(raw_dir.glob("*.pdf"))
                    if raw_files:
                        await adder.process_new_documents(raw_files)
                        print(f"  æ–‡æ¡£å¤„ç†å®Œæˆ")
            else:
                print(f"  ä½¿ç”¨ç°æœ‰çŸ¥è¯†åº“: {kb_name}")

        return kb_name

    def _get_solver_for_kb(self, kb_name: str) -> MainSolver:
        """
        è·å–æŒ‡å®šçŸ¥è¯†åº“çš„ solverï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
        
        Returns:
            MainSolver å®ä¾‹
        """
        if kb_name not in self.solver_cache:
            # è®¾ç½®è¾“å‡ºç›®å½•
            solver_output_dir = self.output_dir / "solve_outputs" / kb_name
            solver_output_dir.mkdir(parents=True, exist_ok=True)

            solver = MainSolver(
                kb_name=kb_name,
                output_base_dir=str(solver_output_dir),
            )
            
            # æŒ‰ç…§å®˜æ–¹æ–¹æ³•ç¦ç”¨ web searchï¼šä¿®æ”¹ config.tools.web_search.enabled
            # å‚è€ƒï¼šconfig/README.md å’Œ config/main.yaml
            # InvestigateAgent ä¼šè¯»å– config.get("tools", {}).get("web_search", {}).get("enabled", True)
            if "tools" not in solver.config:
                solver.config["tools"] = {}
            if "web_search" not in solver.config["tools"]:
                solver.config["tools"]["web_search"] = {}
            solver.config["tools"]["web_search"]["enabled"] = False
            
            # æ›´æ–°å·²åˆå§‹åŒ–çš„ InvestigateAgentï¼ˆagents åœ¨ MainSolver.__init__ æ—¶å·²åˆå§‹åŒ–ï¼‰
            # InvestigateAgent åœ¨ __init__ æ—¶è¯»å–äº† self.enable_web_search
            # éœ€è¦æ›´æ–°å®ƒä»¥åæ˜ æ–°çš„é…ç½®
            if hasattr(solver, "investigate_agent"):
                solver.investigate_agent.enable_web_search = False
            
            self.solver_cache[kb_name] = solver

        return self.solver_cache[kb_name]

    def _extract_answer_from_response(
        self, question: str, response: str
    ) -> tuple[str, str]:
        """
        ä»å“åº”ä¸­æå–ç­”æ¡ˆï¼ˆæ”¯æŒè‡ªå®šä¹‰ä¾›åº”å•†å’Œ base_urlï¼‰

        Returns:
            (predicted_answer, extracted_result)
        """
        try:
            # ä½¿ç”¨æ”¯æŒè‡ªå®šä¹‰ base_url çš„ extract_answer å‡½æ•°
            extracted_res = self.extract_answer_func(
                question, response, self.extractor_prompt
            )
            # å°è¯•ä»æå–ç»“æœä¸­è§£æç­”æ¡ˆ
            if "Extracted answer:" in extracted_res:
                pred_ans = (
                    extracted_res.split("Answer format:")[0]
                    .split("Extracted answer:")[1]
                    .strip()
                )
            else:
                # å¦‚æœæå–å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä»å“åº”ä¸­æå–
                pred_ans = response.strip()[:200]  # æˆªå–å‰200å­—ç¬¦ä½œä¸ºå¤‡é€‰
                extracted_res = f"Failed to extract properly. Raw response: {response[:500]}"

            return pred_ans, extracted_res
        except Exception as e:
            print(f"ç­”æ¡ˆæå–å¤±è´¥: {e}")
            return "Failed to extract", f"Extraction error: {str(e)}"

    async def test_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        æµ‹è¯•å•ä¸ªæ ·æœ¬

        Args:
            sample: æµ‹è¯•æ ·æœ¬

        Returns:
            æ›´æ–°åçš„æ ·æœ¬ï¼ˆåŒ…å« response, pred, score ç­‰å­—æ®µï¼‰
        """
        # å¦‚æœå·²ç»æœ‰ç»“æœï¼Œè·³è¿‡
        if "score" in sample and sample.get("skip", False) is False:
            return sample

        question = sample["question"]
        doc_id = sample["doc_id"]

        print(f"\nå¤„ç†é—®é¢˜: {question[:80]}...")
        print(f"æ–‡æ¡£: {doc_id}")

        try:
            # ç¡®ä¿æ–‡æ¡£å¯¹åº”çš„çŸ¥è¯†åº“å­˜åœ¨
            kb_name = await self._ensure_kb_for_document(doc_id)
            print(f"ä½¿ç”¨çŸ¥è¯†åº“: {kb_name}")

            # è·å–å¯¹åº”çš„ solver
            solver = self._get_solver_for_kb(kb_name)

            # ä½¿ç”¨ solver è§£å†³é—®é¢˜
            result = await solver.solve(question=question, verbose=False)

            # è·å–æœ€ç»ˆç­”æ¡ˆ
            final_answer = result.get("final_answer", "")
            if not final_answer:
                # å¦‚æœæ²¡æœ‰ final_answerï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
                final_answer = result.get("formatted_solution", "")

            # æå–ç­”æ¡ˆ
            pred_ans, extracted_res = self._extract_answer_from_response(
                question, final_answer
            )

            # è¯„ä¼°ç­”æ¡ˆ
            try:
                score = self.eval_modules["eval_score"](
                    sample["answer"], pred_ans, sample["answer_format"]
                )
            except Exception as e:
                print(f"è¯„ä¼°å¤±è´¥: {e}")
                score = 0.0

            # æ›´æ–°æ ·æœ¬
            sample["response"] = final_answer
            sample["extracted_res"] = extracted_res
            sample["pred"] = pred_ans
            sample["score"] = score
            sample["output_dir"] = result.get("output_dir", "")
            sample["kb_name"] = kb_name  # è®°å½•ä½¿ç”¨çš„çŸ¥è¯†åº“

            print(f"é¢„æµ‹ç­”æ¡ˆ: {pred_ans}")
            print(f"æ­£ç¡®ç­”æ¡ˆ: {sample['answer']}")
            print(f"å¾—åˆ†: {score}")

        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            sample["response"] = f"Error: {str(e)}"
            sample["pred"] = "Failed"
            sample["score"] = 0.0
            sample["error"] = str(e)

        return sample

    async def run_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print(f"\nå¼€å§‹æµ‹è¯•ï¼Œå…± {len(self.samples)} ä¸ªæ ·æœ¬")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")

        # è®¡ç®—å·²å®Œæˆçš„æ ·æœ¬æ•°
        completed = sum(1 for s in self.samples if "score" in s)
        print(f"å·²å®Œæˆ: {completed}/{len(self.samples)}")

        # è¿è¡Œæµ‹è¯•
        for i, sample in enumerate(tqdm(self.samples, desc="æµ‹è¯•è¿›åº¦")):
            try:
                sample = await self.test_sample(sample)
                self.samples[i] = sample

                # æ¯å¤„ç†ä¸€ä¸ªæ ·æœ¬å°±ä¿å­˜ä¸€æ¬¡ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
                with open(self.results_file, "w", encoding="utf-8") as f:
                    json.dump(self.samples, f, ensure_ascii=False, indent=2)

                # è®¡ç®—å¹¶æ˜¾ç¤ºå½“å‰ç´¯è®¡å‡†ç¡®ç‡
                completed_samples = [s for s in self.samples if "score" in s]
                if completed_samples:
                    total_score = sum(s.get("score", 0) for s in completed_samples)
                    current_acc = total_score / len(completed_samples)
                    print(f"ğŸ“Š ç´¯è®¡å‡†ç¡®ç‡: {current_acc:.2%} ({int(total_score)}/{len(completed_samples)})")

            except KeyboardInterrupt:
                print("\n\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
                print("å·²ä¿å­˜å½“å‰è¿›åº¦ï¼Œå¯ä»¥ä½¿ç”¨ --start_index å‚æ•°ç»§ç»­")
                break
            except Exception as e:
                print(f"\nå¤„ç†æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
                import traceback

                traceback.print_exc()
                continue

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_report()

    def _generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        print("\nç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")

        # MMLongBench-Doc çš„ show_results å‡½æ•°æœŸæœ› evidence_pages å’Œ evidence_sources æ˜¯å­—ç¬¦ä¸²æ ¼å¼
        # å¦‚æœå®ƒä»¬å·²ç»æ˜¯åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢å›å­—ç¬¦ä¸²è¡¨ç¤º
        for sample in self.samples:
            # å¤„ç† evidence_pages
            evidence_pages = sample.get("evidence_pages")
            if evidence_pages is not None:
                if isinstance(evidence_pages, list):
                    # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¡¨ç¤ºï¼ˆä¾› eval ä½¿ç”¨ï¼‰
                    sample["evidence_pages"] = repr(evidence_pages)
                elif not isinstance(evidence_pages, str):
                    # å¦‚æœä¸æ˜¯åˆ—è¡¨ä¹Ÿä¸æ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨å†è½¬ä¸ºå­—ç¬¦ä¸²
                    sample["evidence_pages"] = repr([evidence_pages])
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œä¿æŒä¸å˜ï¼ˆè®© show_results ä¸­çš„ eval å¤„ç†ï¼‰
            
            # å¤„ç† evidence_sources
            evidence_sources = sample.get("evidence_sources")
            if evidence_sources is not None:
                if isinstance(evidence_sources, list):
                    # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¡¨ç¤ºï¼ˆä¾› eval ä½¿ç”¨ï¼‰
                    sample["evidence_sources"] = repr(evidence_sources)
                elif not isinstance(evidence_sources, str):
                    # å¦‚æœä¸æ˜¯åˆ—è¡¨ä¹Ÿä¸æ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨å†è½¬ä¸ºå­—ç¬¦ä¸²
                    sample["evidence_sources"] = repr([evidence_sources])
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œä¿æŒä¸å˜ï¼ˆè®© show_results ä¸­çš„ eval å¤„ç†ï¼‰

        # ç”ŸæˆæŠ¥å‘Š
        self.eval_modules["show_results"](self.samples, show_path=str(self.report_file))

        # æ‰“å°æŠ¥å‘Šå†…å®¹
        print("\n" + "=" * 60)
        print("è¯„ä¼°æŠ¥å‘Š")
        print("=" * 60)
        with open(self.report_file, "r", encoding="utf-8") as f:
            print(f.read())

        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {self.results_file}")
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.report_file}")


def main():
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ DeepTutor solve æ¨¡å—æµ‹è¯• MMLongBench-Docï¼ˆå¿…é¡»åœ¨ deeptutor conda ç¯å¢ƒä¸­è¿è¡Œï¼‰"
    )
    parser.add_argument(
        "--samples_path",
        type=str,
        default="../MMLongBench-Doc/data/samples.json",
        help="samples.json æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--document_path",
        type=str,
        default="../MMLongBench-Doc/data/documents",
        help="PDF æ–‡æ¡£ç›®å½•è·¯å¾„",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./test_results",
        help="è¾“å‡ºç›®å½•",
    )
    parser.add_argument(
        "--kb_name",
        type=str,
        default=None,
        help="çŸ¥è¯†åº“åç§°ï¼ˆå·²å¼ƒç”¨ï¼šç°åœ¨æ¯ä¸ªæ–‡æ¡£ä½¿ç”¨ç‹¬ç«‹çŸ¥è¯†åº“ï¼‰",
    )
    parser.add_argument(
        "--extractor_prompt_path",
        type=str,
        default=None,
        help="ç­”æ¡ˆæå–æç¤ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨ MMLongBench-Doc çš„æç¤ºæ–‡ä»¶ï¼‰",
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰",
    )
    parser.add_argument(
        "--start_index",
        type=int,
        default=0,
        help="èµ·å§‹ç´¢å¼•ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°è¿è¡Œï¼Œåˆ é™¤æ—§çš„ç»“æœæ–‡ä»¶",
    )

    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æ­£ç¡®çš„ conda ç¯å¢ƒä¸­
    python_path = sys.executable
    if "deeptutor" not in python_path.lower():
        print("âš ï¸  è­¦å‘Š: å½“å‰å¯èƒ½ä¸åœ¨ deeptutor conda ç¯å¢ƒä¸­")
        print(f"å½“å‰ Python è·¯å¾„: {python_path}")
        print("å»ºè®®è¿è¡Œ: conda activate deeptutor")
        response = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            print("å·²å–æ¶ˆ")
            sys.exit(1)
    
    # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
    import os
    from dotenv import load_dotenv
    project_root = Path(__file__).parent.parent.parent
    load_dotenv(project_root / ".env", override=False)
    
    required_vars = {
        "LLM_MODEL": os.getenv("LLM_MODEL"),
        "LLM_BINDING_API_KEY": os.getenv("LLM_BINDING_API_KEY"),
        "LLM_BINDING_HOST": os.getenv("LLM_BINDING_HOST"),
        "EMBEDDING_MODEL": os.getenv("EMBEDDING_MODEL"),
        "EMBEDDING_BINDING_API_KEY": os.getenv("EMBEDDING_BINDING_API_KEY"),
        "EMBEDDING_BINDING_HOST": os.getenv("EMBEDDING_BINDING_HOST"),
    }
    
    missing_vars = [var for var, value in required_vars.items() if not value]
    if missing_vars:
        print("âŒ é”™è¯¯: ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡é…ç½®")
        print("è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•çš„ .env æ–‡ä»¶ä¸­é…ç½®ä»¥ä¸‹å˜é‡ï¼š")
        for var in missing_vars:
            print(f"  - {var}")
        print("\nç¤ºä¾‹é…ç½®ï¼š")
        print("  LLM_MODEL=gpt-4o")
        print("  LLM_BINDING_API_KEY=your_api_key")
        print("  LLM_BINDING_HOST=https://api.openai.com/v1")
        print("  EMBEDDING_MODEL=text-embedding-3-large")
        print("  EMBEDDING_BINDING_API_KEY=your_api_key")
        print("  EMBEDDING_BINDING_HOST=https://api.openai.com/v1")
        sys.exit(1)
    
    # éªŒè¯ç­”æ¡ˆæå–æ‰€éœ€çš„é…ç½®
    # ç°åœ¨ç­”æ¡ˆæå–ä½¿ç”¨ LLM_BINDING_API_KEY å’Œ LLM_BINDING_HOSTï¼Œæ”¯æŒä»»ä½•å…¼å®¹ OpenAI API çš„ä¾›åº”å•†
    llm_api_key = os.getenv("LLM_BINDING_API_KEY")
    llm_base_url = os.getenv("LLM_BINDING_HOST")
    if llm_api_key and llm_base_url:
        print(f"â„¹ï¸  ç­”æ¡ˆæå–å°†ä½¿ç”¨: {llm_base_url} (æ¨¡å‹: {os.getenv('LLM_MODEL', 'gpt-4o')})")
    else:
        print("âš ï¸  è­¦å‘Š: LLM_BINDING_API_KEY æˆ– LLM_BINDING_HOST æœªè®¾ç½®ï¼Œç­”æ¡ˆæå–å¯èƒ½å¤±è´¥")

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = MMLongBenchTester(
        samples_path=args.samples_path,
        document_path=args.document_path,
        output_dir=args.output_dir,
        kb_name=args.kb_name,  # å·²å¼ƒç”¨ï¼Œä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç 
        extractor_prompt_path=args.extractor_prompt_path,
        max_samples=args.max_samples,
        start_index=args.start_index,
        force_rerun=args.force,
    )

    # è¿è¡Œæµ‹è¯•
    asyncio.run(tester.run_tests())


if __name__ == "__main__":
    main()

