#!/usr/bin/env python
"""
æµ‹è¯•è„šæœ¬ï¼šä½¿ç”¨ DeepTutor solve æ¨¡å—æµ‹è¯• MMLongBench-Doc åŸºå‡†

è¯¥è„šæœ¬ä¼šï¼š
1. è¯»å– MMLongBench-Doc çš„ samples.jsonï¼ˆä»…ç”¨äºæä¾›é—®é¢˜å’Œæ­£ç¡®ç­”æ¡ˆæ•°æ®ï¼‰
2. å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œä½¿ç”¨ MainSolver æ¥è§£å†³
3. ä½¿ç”¨ LLM-as-a-Judge (GPT-4o-mini) è¯„ä¼°ç­”æ¡ˆå‡†ç¡®æ€§
4. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

è¯„ä¼°æ–¹å¼ï¼š
- ä½¿ç”¨ LLMAnswerEvaluator è¿›è¡Œ LLM-as-a-Judge è¯„ä¼°
- ä¸å†ä¾èµ– MMLongBench-Doc çš„è§„åˆ™è¯„ä¼°ï¼ˆANLS/ç¼–è¾‘è·ç¦»ï¼‰
- LLM è¯„ä¼°æ›´å‡†ç¡®åœ°åˆ¤æ–­ç­”æ¡ˆè¯­ä¹‰æ­£ç¡®æ€§

æ³¨æ„ï¼š
- æ‰€æœ‰æ“ä½œå¿…é¡»åœ¨ deeptutor conda ç¯å¢ƒä¸­è¿›è¡Œ
- è¿è¡Œå‰è¯·ç¡®ä¿ï¼šconda activate deeptutor
- Web search åŠŸèƒ½åœ¨æµ‹è¯•æ—¶è‡ªåŠ¨ç¦ç”¨ï¼ˆé€šè¿‡è®¾ç½® config.tools.web_search.enabled = Falseï¼‰
"""

import argparse
import asyncio
import atexit
import json
import os
import re
import shutil
import sys
import warnings
from pathlib import Path
from typing import Any, Dict, List
from datetime import datetime

# æŠ‘åˆ¶å¸¸è§çš„å¼‚æ­¥ç›¸å…³è­¦å‘Š
warnings.filterwarnings("ignore", message=".*no current event loop.*")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="asyncio")

# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸º WARNINGï¼Œå…³é—­ INFO æ—¥å¿—
import logging
logging.getLogger().setLevel(logging.WARNING)
# å…³é—­å¸¸è§æ¨¡å—çš„ INFO æ—¥å¿—
for logger_name in ["httpx", "httpcore", "openai", "urllib3", "asyncio", "lightrag", "Solver", "InvestigateAgent", "NoteAgent", "ManagerAgent", "SolveAgent", "ToolAgent", "ResponseAgent", "PrecisionAnswerAgent"]:
    logging.getLogger(logger_name).setLevel(logging.WARNING)

# å­˜å‚¨éœ€è¦åœ¨é€€å‡ºæ—¶æ¸…ç†çš„ RAGAnything å®ä¾‹
_raganything_instances = []

def _cleanup_raganything():
    """åœ¨ç¨‹åºé€€å‡ºå‰æ¸…ç† RAGAnything å®ä¾‹ï¼Œé¿å…è­¦å‘Š"""
    for instance in _raganything_instances:
        try:
            # å°è¯•åŒæ­¥æ¸…ç†
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(instance.finalize_storages())
            loop.close()
        except Exception:
            pass  # é™é»˜å¿½ç•¥æ¸…ç†é”™è¯¯

atexit.register(_cleanup_raganything)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# MMLongBench-Doc æ•°æ®è·¯å¾„ï¼ˆåªç”¨äºè¯»å–æ ·æœ¬æ•°æ®ï¼‰
mmlongbench_root = project_root.parent / "MMLongBench-Doc"

from dotenv import load_dotenv
from tqdm import tqdm

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=False)

# å¯¼å…¥ DeepTutor solve æ¨¡å—
from src.agents.solve.main_solver import MainSolver
from src.knowledge.initializer import KnowledgeBaseInitializer
from src.knowledge.add_documents import DocumentAdder

# å¯¼å…¥ LLM è¯„ä¼°æ¨¡å—
from llm_answer_evaluator import LLMAnswerEvaluator


class LLMEvaluatorConfig:
    """LLMè¯„ä¼°å™¨é…ç½®"""
    def __init__(self, output_dir: str, api_key: str, base_url: str, model: str = "gpt-4o", quiet: bool = False):
        self.output_dir = output_dir
        self.api_key = api_key
        self.base_url = base_url
        self.model = model  # è¯„ä¼°æ¨¡å‹åç§°
        self.quiet = quiet  # é™é»˜æ¨¡å¼ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°


class MMLongBenchTester:
    """MMLongBench-Doc æµ‹è¯•å™¨"""

    def __init__(
        self,
        samples_path: str,
        document_path: str,
        output_dir: str,
        kb_name: str = None,  # å·²å¼ƒç”¨ï¼Œç°åœ¨æ¯ä¸ªæ–‡æ¡£ä½¿ç”¨ç‹¬ç«‹çŸ¥è¯†åº“
        max_samples: int = None,
        start_index: int = 0,
        force_rerun: bool = False,
    ):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨ï¼ˆä½¿ç”¨ LLM-as-a-Judge è¯„ä¼°ï¼‰

        Args:
            samples_path: samples.json æ–‡ä»¶è·¯å¾„
            document_path: PDF æ–‡æ¡£ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            kb_name: çŸ¥è¯†åº“åç§°ï¼ˆå·²å¼ƒç”¨ï¼Œç°åœ¨æ¯ä¸ªæ–‡æ¡£ä½¿ç”¨ç‹¬ç«‹çŸ¥è¯†åº“ï¼‰
            max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰
            start_index: èµ·å§‹ç´¢å¼•ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰
        """
        self.samples_path = Path(samples_path)
        self.document_path = Path(document_path)
        self.output_dir = Path(output_dir)
        if kb_name:
            print(f"è­¦å‘Š: --kb_name å‚æ•°å·²å¼ƒç”¨ï¼Œç°åœ¨æ¯ä¸ªæ–‡æ¡£ä½¿ç”¨ç‹¬ç«‹çŸ¥è¯†åº“")
        self.max_samples = max_samples
        self.start_index = start_index
        self.force_rerun = force_rerun

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.results_file = self.output_dir / "results.json"
        self.report_file = self.output_dir / "report.txt"
        
        # å¦‚æœå¼ºåˆ¶é‡æ–°è¿è¡Œï¼Œåˆ é™¤æ—§çš„ç»“æœæ–‡ä»¶
        if self.force_rerun and self.results_file.exists():
            print(f"å¼ºåˆ¶é‡æ–°è¿è¡Œï¼šåˆ é™¤æ—§çš„ç»“æœæ–‡ä»¶ {self.results_file}")
            self.results_file.unlink()

        # è·å– LLM é…ç½®
        import os
        from dotenv import load_dotenv
        project_root = Path(__file__).parent.parent.parent
        load_dotenv(project_root / ".env", override=False)
        
        self.api_key = os.getenv("LLM_BINDING_API_KEY")
        self.base_url = os.getenv("LLM_BINDING_HOST")
        self.model_name = os.getenv("LLM_MODEL", "gpt-4o")
        
        if not self.api_key or not self.base_url:
            raise ValueError(
                "LLM_BINDING_API_KEY å’Œ LLM_BINDING_HOST å¿…é¡»è®¾ç½®æ‰èƒ½è¿›è¡Œ LLM è¯„ä¼°"
            )
        
        # åˆ›å»º LLM è¯„ä¼°å™¨
        evaluator_config = LLMEvaluatorConfig(
            output_dir=str(self.output_dir),
            api_key=self.api_key,
            base_url=self.base_url,
            model=self.model_name,  # ä½¿ç”¨ä¸ solver ç›¸åŒçš„æ¨¡å‹
            quiet=True  # é™é»˜æ¨¡å¼
        )
        self.llm_evaluator = LLMAnswerEvaluator(evaluator_config)

        # åŠ è½½æ ·æœ¬
        self.samples = self._load_samples()

        # çŸ¥è¯†åº“ç¼“å­˜ï¼ˆæ¯ä¸ªæ–‡æ¡£å¯¹åº”ä¸€ä¸ªçŸ¥è¯†åº“ï¼‰
        self.kb_cache = {}  # doc_id -> kb_name
        self.solver_cache = {}  # kb_name -> solver

        # çŸ¥è¯†åº“åŸºç¡€ç›®å½•
        self.kb_base_dir = Path("./data/knowledge_bases")

    def _load_samples(self) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ ·æœ¬"""
        # å§‹ç»ˆä»åŸå§‹ samples.json åŠ è½½
        with open(self.samples_path, "r", encoding="utf-8") as f:
            samples = json.load(f)

        # é™åˆ¶æ ·æœ¬æ•°é‡
        if self.max_samples:
            samples = samples[: self.max_samples]

        # ä»æŒ‡å®šç´¢å¼•å¼€å§‹
        if self.start_index > 0:
            samples = samples[self.start_index:]

        # å¦‚æœç»“æœæ–‡ä»¶å­˜åœ¨ï¼Œåˆå¹¶å·²å®Œæˆçš„ç»“æœï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        if self.results_file.exists() and not self.force_rerun:
            print(f"ä»ç°æœ‰ç»“æœæ–‡ä»¶åŠ è½½å·²å®Œæˆçš„ç»“æœ: {self.results_file}")
            with open(self.results_file, "r", encoding="utf-8") as f:
                existing_results = json.load(f)
            
            # åˆ›å»ºå·²å®Œæˆç»“æœçš„æŸ¥æ‰¾å­—å…¸ï¼ˆåŸºäº question + doc_idï¼‰
            results_dict = {}
            for r in existing_results:
                key = (r.get("question", ""), r.get("doc_id", ""))
                if key:
                    results_dict[key] = r
            
            # åˆå¹¶å·²å®Œæˆçš„ç»“æœåˆ°æ ·æœ¬ä¸­
            merged_count = 0
            for sample in samples:
                key = (sample.get("question", ""), sample.get("doc_id", ""))
                if key in results_dict:
                    # ç”¨å·²å®Œæˆçš„ç»“æœæ›´æ–°æ ·æœ¬
                    existing = results_dict[key]
                    sample.update(existing)
                    merged_count += 1
            
            if merged_count > 0:
                print(f"  åˆå¹¶äº† {merged_count} ä¸ªå·²å®Œæˆçš„ç»“æœ")

        print(f"åŠ è½½äº† {len(samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return samples

    def _get_kb_name_from_doc_id(self, doc_id: str) -> str:
        """
        ä»æ–‡æ¡£IDç”ŸæˆçŸ¥è¯†åº“åç§°
        
        Args:
            doc_id: æ–‡æ¡£IDï¼ˆå¦‚ "PH_2016.06.08_Economy-Final.pdf"ï¼‰
        
        Returns:
            çŸ¥è¯†åº“åç§°ï¼ˆå¦‚ "mmlongbench_PH_2016_06_08_Economy_Final"ï¼‰
        """
        # ç§»é™¤ .pdf æ‰©å±•å
        name = doc_id.replace(".pdf", "")
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
        name = re.sub(r'[^\w\-_]', '_', name)
        # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
        name = re.sub(r'_+', '_', name)
        # ç¡®ä¿ä»¥ mmlongbench_ å¼€å¤´
        if not name.startswith("mmlongbench_"):
            name = f"mmlongbench_{name}"
        return name

    async def _ensure_kb_for_document(self, doc_id: str) -> str:
        """
        ç¡®ä¿æ–‡æ¡£å¯¹åº”çš„çŸ¥è¯†åº“å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºå¹¶æ·»åŠ æ–‡æ¡£
        
        Args:
            doc_id: æ–‡æ¡£ID
        
        Returns:
            çŸ¥è¯†åº“åç§°
        """
        # æ£€æŸ¥ç¼“å­˜
        if doc_id in self.kb_cache:
            return self.kb_cache[doc_id]

        # ç”ŸæˆçŸ¥è¯†åº“åç§°
        kb_name = self._get_kb_name_from_doc_id(doc_id)
        self.kb_cache[doc_id] = kb_name

        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        kb_dir = self.kb_base_dir / kb_name
        doc_file = self.document_path / doc_id

        if not doc_file.exists():
            raise FileNotFoundError(f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_file}")

        if not kb_dir.exists():
            print(f"\nä¸ºæ–‡æ¡£ {doc_id} åˆ›å»ºçŸ¥è¯†åº“: {kb_name}")
            # åˆ›å»ºçŸ¥è¯†åº“
            initializer = KnowledgeBaseInitializer(
                kb_name=kb_name,
                base_dir=str(self.kb_base_dir),
            )
            initializer.create_directory_structure()

            # æ·»åŠ æ–‡æ¡£
            adder = DocumentAdder(
                kb_name=kb_name,
                base_dir=str(self.kb_base_dir),
            )
            added_files = adder.add_documents(
                source_files=[str(doc_file)],
                skip_duplicates=True,
            )
            print(f"  å·²æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“: {len(added_files)} ä¸ªæ–‡ä»¶")
            
            # å¤„ç†æ–‡æ¡£ï¼ˆå¼‚æ­¥ï¼‰
            if added_files:
                print(f"  æ­£åœ¨å¤„ç†æ–‡æ¡£ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
                await adder.process_new_documents(added_files)
                print(f"  æ–‡æ¡£å¤„ç†å®Œæˆ")
        else:
            # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å·²å¤„ç†ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰ rag_storage å†…å®¹ï¼‰
            rag_storage_dir = kb_dir / "rag_storage"
            if not rag_storage_dir.exists() or not any(rag_storage_dir.iterdir()):
                print(f"  çŸ¥è¯†åº“å­˜åœ¨ä½†æœªå¤„ç†ï¼Œæ­£åœ¨å¤„ç†æ–‡æ¡£...")
                adder = DocumentAdder(
                    kb_name=kb_name,
                    base_dir=str(self.kb_base_dir),
                )
                # æ£€æŸ¥ raw ç›®å½•ä¸­çš„æ–‡ä»¶
                raw_dir = kb_dir / "raw"
                if raw_dir.exists():
                    raw_files = list(raw_dir.glob("*.pdf"))
                    if raw_files:
                        await adder.process_new_documents(raw_files)
                        print(f"  æ–‡æ¡£å¤„ç†å®Œæˆ")
            else:
                print(f"  ä½¿ç”¨ç°æœ‰çŸ¥è¯†åº“: {kb_name}")

        return kb_name

    def _get_solver_for_kb(self, kb_name: str) -> MainSolver:
        """
        è·å–æŒ‡å®šçŸ¥è¯†åº“çš„ solverï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            kb_name: çŸ¥è¯†åº“åç§°
        
        Returns:
            MainSolver å®ä¾‹
        """
        if kb_name not in self.solver_cache:
            # è®¾ç½®è¾“å‡ºç›®å½•
            solver_output_dir = self.output_dir / "solve_outputs" / kb_name
            solver_output_dir.mkdir(parents=True, exist_ok=True)

            solver = MainSolver(
                kb_name=kb_name,
                output_base_dir=str(solver_output_dir),
            )
            
            # æŒ‰ç…§å®˜æ–¹æ–¹æ³•ç¦ç”¨ web searchï¼šä¿®æ”¹ config.tools.web_search.enabled
            # å‚è€ƒï¼šconfig/README.md å’Œ config/main.yaml
            # InvestigateAgent ä¼šè¯»å– config.get("tools", {}).get("web_search", {}).get("enabled", True)
            if "tools" not in solver.config:
                solver.config["tools"] = {}
            if "web_search" not in solver.config["tools"]:
                solver.config["tools"]["web_search"] = {}
            solver.config["tools"]["web_search"]["enabled"] = False
            
            # æ›´æ–°å·²åˆå§‹åŒ–çš„ InvestigateAgentï¼ˆagents åœ¨ MainSolver.__init__ æ—¶å·²åˆå§‹åŒ–ï¼‰
            # InvestigateAgent åœ¨ __init__ æ—¶è¯»å–äº† self.enable_web_search
            # éœ€è¦æ›´æ–°å®ƒä»¥åæ˜ æ–°çš„é…ç½®
            if hasattr(solver, "investigate_agent"):
                solver.investigate_agent.enable_web_search = False
            
            self.solver_cache[kb_name] = solver

        return self.solver_cache[kb_name]

    def _extract_concise_answer(self, response: str) -> str:
        """
        ä»å“åº”ä¸­æå– Concise Answer
        
        æŸ¥æ‰¾ "## Concise Answer" æˆ–ç±»ä¼¼æ ‡è®°åçš„å†…å®¹
        
        Returns:
            æå–çš„ç®€æ´ç­”æ¡ˆï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        import re
        
        # å°è¯•å¤šç§æ¨¡å¼æå– Concise Answer
        patterns = [
            # ## Concise Answer\n\nXXX\n\n---
            r"## Concise Answer\s*\n\n(.+?)\n\n---",
            # ## Concise Answer\n\nXXX (åˆ°æ–‡ä»¶æœ«å°¾)
            r"## Concise Answer\s*\n\n(.+?)(?:\n\n|$)",
            # **Concise Answer:** XXX
            r"\*\*Concise Answer[:\*]*\s*(.+?)(?:\n|$)",
            # Concise Answer: XXX
            r"Concise Answer[:\s]+(.+?)(?:\n|$)",
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
            if match:
                answer = match.group(1).strip()
                # æ¸…ç†ç­”æ¡ˆï¼šç§»é™¤å¤šä½™çš„ç©ºç™½å’Œæ¢è¡Œ
                answer = re.sub(r'\s+', ' ', answer).strip()
                if answer:
                    return answer
        
        return ""

    async def test_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        æµ‹è¯•å•ä¸ªæ ·æœ¬

        Args:
            sample: æµ‹è¯•æ ·æœ¬

        Returns:
            æ›´æ–°åçš„æ ·æœ¬ï¼ˆåŒ…å« response, pred, score ç­‰å­—æ®µï¼‰
        """
        # å¦‚æœå·²ç»æœ‰ç»“æœï¼Œè·³è¿‡
        if "score" in sample and sample.get("skip", False) is False:
            return sample

        question = sample["question"]
        doc_id = sample["doc_id"]

        print(f"\nå¤„ç†é—®é¢˜: {question}...")
        print(f"æ–‡æ¡£: {doc_id}")

        try:
            # ç¡®ä¿æ–‡æ¡£å¯¹åº”çš„çŸ¥è¯†åº“å­˜åœ¨
            kb_name = await self._ensure_kb_for_document(doc_id)
            print(f"ä½¿ç”¨çŸ¥è¯†åº“: {kb_name}")

            # è·å–å¯¹åº”çš„ solver
            solver = self._get_solver_for_kb(kb_name)

            # ä½¿ç”¨ solver è§£å†³é—®é¢˜
            result = await solver.solve(question=question, verbose=False)

            # è·å–æœ€ç»ˆç­”æ¡ˆ
            final_answer = result.get("final_answer", "")
            if not final_answer:
                # å¦‚æœæ²¡æœ‰ final_answerï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
                final_answer = result.get("formatted_solution", "")

            # æå– Concise Answerï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            concise_answer = self._extract_concise_answer(final_answer)
            # ç”¨äºè¯„ä¼°çš„ç­”æ¡ˆï¼šä¼˜å…ˆä½¿ç”¨ concise_answer
            eval_answer = concise_answer if concise_answer else final_answer

            # ä½¿ç”¨ LLM-as-a-Judge è¯„ä¼°ç­”æ¡ˆ
            try:
                eval_result = await self.llm_evaluator.evaluate_single_answer(
                    question=question,
                    expected_answer=str(sample["answer"]),
                    generated_answer=eval_answer,
                    evidence_pages=str(sample.get("evidence_pages", "")),
                    evidence_sources=str(sample.get("evidence_sources", "")),
                    doc_id=sample["doc_id"],
                    evaluation_type="accuracy_only"  # åªè¯„ä¼°å‡†ç¡®æ€§ï¼Œé€Ÿåº¦æ›´å¿«
                )
                
                # æå–è¯„ä¼°ç»“æœ
                score = float(eval_result.get("accuracy", 0))
                reasoning = eval_result.get("reasoning", "")
                
            except Exception as e:
                print(f"LLMè¯„ä¼°å¤±è´¥: {e}")
                score = 0.0
                reasoning = f"Evaluation error: {str(e)}"

            # æ›´æ–°æ ·æœ¬
            sample["response"] = final_answer
            sample["concise_answer"] = concise_answer  # è®°å½•æå–çš„ç®€æ´ç­”æ¡ˆ
            sample["eval_answer"] = eval_answer  # ç”¨äºè¯„ä¼°çš„ç­”æ¡ˆ
            sample["score"] = score
            sample["llm_reasoning"] = reasoning
            output_dir = result.get("output_dir", "")
            sample["output_dir"] = output_dir
            sample["kb_name"] = kb_name  # è®°å½•ä½¿ç”¨çš„çŸ¥è¯†åº“

            # ç®€æ´è¾“å‡ºï¼šåŸé¢˜ã€ç­”æ¡ˆã€å¾—åˆ†ã€æ—¥å¿—ä½ç½®
            score_icon = "âœ…" if score >= 0.5 else "âŒ"
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹
            q_short = question 
            ans_short = eval_answer[:100] + "..." if len(eval_answer) > 100 else eval_answer
            print(f"\nğŸ“ é—®é¢˜: {q_short}")
            print(f"ğŸ’¬ è¾“å‡º: {ans_short}")
            print(f"âœ“  æ­£ç¡®: {sample['answer']}")
            print(f"{score_icon} å¾—åˆ†: {score} | æ—¥å¿—: {output_dir}")

        except Exception as e:
            sample["response"] = f"Error: {str(e)}"
            sample["pred"] = "Failed"
            sample["score"] = 0.0
            sample["error"] = str(e)
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")

        return sample

    async def run_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        # è®¡ç®—å·²å®Œæˆçš„æ ·æœ¬æ•°
        completed = sum(1 for s in self.samples if "score" in s)
        print(f"\nğŸš€ å¼€å§‹æµ‹è¯• | æ€»æ ·æœ¬: {len(self.samples)} | å·²å®Œæˆ: {completed} | è¾“å‡º: {self.output_dir}")

        # è¿è¡Œæµ‹è¯•
        for i, sample in enumerate(tqdm(self.samples, desc="æµ‹è¯•è¿›åº¦")):
            # è·³è¿‡å·²å®Œæˆçš„æ ·æœ¬ï¼ˆé™¤éå¼ºåˆ¶é‡æ–°è¿è¡Œï¼‰
            if not self.force_rerun and "score" in sample:
                continue
            
            try:
                sample = await self.test_sample(sample)
                self.samples[i] = sample

                # æ¯å¤„ç†ä¸€ä¸ªæ ·æœ¬å°±ä¿å­˜ä¸€æ¬¡ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
                with open(self.results_file, "w", encoding="utf-8") as f:
                    json.dump(self.samples, f, ensure_ascii=False, indent=2)

                # è®¡ç®—å¹¶æ˜¾ç¤ºå½“å‰ç´¯è®¡å‡†ç¡®ç‡
                completed_samples = [s for s in self.samples if "score" in s]
                if completed_samples:
                    total_score = sum(s.get("score", 0) for s in completed_samples)
                    current_acc = total_score / len(completed_samples)
                    print(f"ğŸ“Š ç´¯è®¡å‡†ç¡®ç‡: {current_acc:.2%} ({int(total_score)}/{len(completed_samples)})")

            except KeyboardInterrupt:
                print("\n\nâš ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­ï¼Œå·²ä¿å­˜è¿›åº¦")
                break
            except Exception as e:
                print(f"\nâŒ æ ·æœ¬ {i} å‡ºé”™: {e}")
                continue

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_report()

    def _generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šï¼ˆä½¿ç”¨ LLM-as-a-Judge ç»“æœï¼‰"""
        print("\nç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")

        # ç»Ÿè®¡ç»“æœ
        evaluated_samples = [s for s in self.samples if "score" in s]
        if not evaluated_samples:
            print("æ²¡æœ‰å·²è¯„ä¼°çš„æ ·æœ¬")
            return
        
        total_samples = len(evaluated_samples)
        correct_samples = sum(1 for s in evaluated_samples if s.get("score", 0) >= 0.5)
        accuracy = correct_samples / total_samples if total_samples > 0 else 0
        
        # æŒ‰æ–‡æ¡£ç±»å‹ç»Ÿè®¡
        doc_type_stats = {}
        for sample in evaluated_samples:
            doc_type = sample.get("doc_type", "Unknown")
            if doc_type not in doc_type_stats:
                doc_type_stats[doc_type] = {"total": 0, "correct": 0}
            doc_type_stats[doc_type]["total"] += 1
            if sample.get("score", 0) >= 0.5:
                doc_type_stats[doc_type]["correct"] += 1
        
        # æŒ‰è¯æ®æ¥æºç»Ÿè®¡
        source_stats = {}
        for sample in evaluated_samples:
            sources = sample.get("evidence_sources", "[]")
            if isinstance(sources, str):
                try:
                    sources = eval(sources)
                except:
                    sources = [sources]
            if not isinstance(sources, list):
                sources = [sources]
            
            for source in sources:
                if source not in source_stats:
                    source_stats[source] = {"total": 0, "correct": 0}
                source_stats[source]["total"] += 1
                if sample.get("score", 0) >= 0.5:
                    source_stats[source]["correct"] += 1
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_lines = [
            f"Overall Accuracy (LLM-as-Judge): {accuracy:.4f} | Question Number: {total_samples}",
            f"Correct Answers: {correct_samples} | Total Evaluated: {total_samples}",
            "-" * 50,
        ]
        
        # æ–‡æ¡£ç±»å‹ç»Ÿè®¡
        report_lines.append("\næŒ‰æ–‡æ¡£ç±»å‹ç»Ÿè®¡:")
        for doc_type, stats in doc_type_stats.items():
            type_acc = stats["correct"] / stats["total"] if stats["total"] > 0 else 0
            report_lines.append(
                f"  {doc_type}: Accuracy: {type_acc:.4f} | Questions: {stats['total']}"
            )
        
        # è¯æ®æ¥æºç»Ÿè®¡
        report_lines.append("\næŒ‰è¯æ®æ¥æºç»Ÿè®¡:")
        for source, stats in source_stats.items():
            source_acc = stats["correct"] / stats["total"] if stats["total"] > 0 else 0
            report_lines.append(
                f"  {source}: Accuracy: {source_acc:.4f} | Questions: {stats['total']}"
            )
        
        report_content = "\n".join(report_lines)
        
        # ä¿å­˜æŠ¥å‘Š
        with open(self.report_file, "w", encoding="utf-8") as f:
            f.write(report_content)

        # æ‰“å°æŠ¥å‘Šå†…å®¹
        print("\n" + "=" * 60)
        print("è¯„ä¼°æŠ¥å‘Š (LLM-as-a-Judge)")
        print("=" * 60)
        print(report_content)

        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {self.results_file}")
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.report_file}")


def main():
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ DeepTutor solve æ¨¡å—æµ‹è¯• MMLongBench-Docï¼ˆå¿…é¡»åœ¨ deeptutor conda ç¯å¢ƒä¸­è¿è¡Œï¼‰"
    )
    parser.add_argument(
        "--samples_path",
        type=str,
        default="../MMLongBench-Doc/data/samples.json",
        help="samples.json æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--document_path",
        type=str,
        default="../MMLongBench-Doc/data/documents",
        help="PDF æ–‡æ¡£ç›®å½•è·¯å¾„",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./test_results",
        help="è¾“å‡ºç›®å½•",
    )
    parser.add_argument(
        "--kb_name",
        type=str,
        default=None,
        help="çŸ¥è¯†åº“åç§°ï¼ˆå·²å¼ƒç”¨ï¼šç°åœ¨æ¯ä¸ªæ–‡æ¡£ä½¿ç”¨ç‹¬ç«‹çŸ¥è¯†åº“ï¼‰",
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰",
    )
    parser.add_argument(
        "--start_index",
        type=int,
        default=0,
        help="èµ·å§‹ç´¢å¼•ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°è¿è¡Œï¼Œåˆ é™¤æ—§çš„ç»“æœæ–‡ä»¶",
    )

    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æ­£ç¡®çš„ conda ç¯å¢ƒä¸­
    python_path = sys.executable
    if "deeptutor" not in python_path.lower():
        print("âš ï¸  è­¦å‘Š: å½“å‰å¯èƒ½ä¸åœ¨ deeptutor conda ç¯å¢ƒä¸­")
        print(f"å½“å‰ Python è·¯å¾„: {python_path}")
        print("å»ºè®®è¿è¡Œ: conda activate deeptutor")
        response = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            print("å·²å–æ¶ˆ")
            sys.exit(1)
    
    # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
    import os
    from dotenv import load_dotenv
    project_root = Path(__file__).parent.parent.parent
    load_dotenv(project_root / ".env", override=False)
    
    required_vars = {
        "LLM_MODEL": os.getenv("LLM_MODEL"),
        "LLM_BINDING_API_KEY": os.getenv("LLM_BINDING_API_KEY"),
        "LLM_BINDING_HOST": os.getenv("LLM_BINDING_HOST"),
        "EMBEDDING_MODEL": os.getenv("EMBEDDING_MODEL"),
        "EMBEDDING_BINDING_API_KEY": os.getenv("EMBEDDING_BINDING_API_KEY"),
        "EMBEDDING_BINDING_HOST": os.getenv("EMBEDDING_BINDING_HOST"),
    }
    
    missing_vars = [var for var, value in required_vars.items() if not value]
    if missing_vars:
        print("âŒ é”™è¯¯: ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡é…ç½®")
        print("è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•çš„ .env æ–‡ä»¶ä¸­é…ç½®ä»¥ä¸‹å˜é‡ï¼š")
        for var in missing_vars:
            print(f"  - {var}")
        print("\nç¤ºä¾‹é…ç½®ï¼š")
        print("  LLM_MODEL=gpt-4o")
        print("  LLM_BINDING_API_KEY=your_api_key")
        print("  LLM_BINDING_HOST=https://api.openai.com/v1")
        print("  EMBEDDING_MODEL=text-embedding-3-large")
        print("  EMBEDDING_BINDING_API_KEY=your_api_key")
        print("  EMBEDDING_BINDING_HOST=https://api.openai.com/v1")
        sys.exit(1)
    
    # éªŒè¯ LLM-as-a-Judge è¯„ä¼°æ‰€éœ€çš„é…ç½®
    llm_api_key = os.getenv("LLM_BINDING_API_KEY")
    llm_base_url = os.getenv("LLM_BINDING_HOST")
    llm_model = os.getenv("LLM_MODEL", "gpt-4o")
    if llm_api_key and llm_base_url:
        print(f"â„¹ï¸  LLM-as-a-Judge è¯„ä¼°å°†ä½¿ç”¨: {llm_base_url} (æ¨¡å‹: {llm_model})")
    else:
        print("âš ï¸  è­¦å‘Š: LLM_BINDING_API_KEY æˆ– LLM_BINDING_HOST æœªè®¾ç½®ï¼ŒLLMè¯„ä¼°å¯èƒ½å¤±è´¥")

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = MMLongBenchTester(
        samples_path=args.samples_path,
        document_path=args.document_path,
        output_dir=args.output_dir,
        kb_name=args.kb_name,  # å·²å¼ƒç”¨ï¼Œä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç 
        max_samples=args.max_samples,
        start_index=args.start_index,
        force_rerun=args.force,
    )

    # è¿è¡Œæµ‹è¯•
    asyncio.run(tester.run_tests())


if __name__ == "__main__":
    main()

