#!/usr/bin/env python
"""
ä½¿ç”¨ RAGAnything åŸç”Ÿ API æµ‹è¯• MMLongBench-Doc é—®é¢˜

ç›´æ¥ä½¿ç”¨ RAGAnything çš„ aquery() æ–¹æ³•ï¼Œä¸ç»è¿‡ DeepTutor çš„å°è£…
å¤ç”¨å·²æœ‰çš„çŸ¥è¯†åº“ï¼ˆrag_storageï¼‰
"""

import argparse
import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# æ·»åŠ  RAGAnything è·¯å¾„
raganything_path = project_root.parent / "RAG-Anything"
sys.path.insert(0, str(raganything_path))

from dotenv import load_dotenv
from tqdm import tqdm

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=False)

from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc
from raganything import RAGAnything, RAGAnythingConfig

# å¯¼å…¥ LLM è¯„ä¼°æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))
from llm_answer_evaluator import LLMAnswerEvaluator

# æŠ‘åˆ¶æ—¥å¿—
logging.getLogger().setLevel(logging.ERROR)
for logger_name in ["httpx", "httpcore", "openai", "urllib3", "asyncio", "lightrag", "raganything"]:
    logging.getLogger(logger_name).setLevel(logging.ERROR)

# MMLongBench-Doc æ•°æ®è·¯å¾„
mmlongbench_root = project_root.parent / "MMLongBench-Doc"


class LLMEvaluatorConfig:
    """LLMè¯„ä¼°å™¨é…ç½®"""
    def __init__(self, output_dir: str, api_key: str, base_url: str, model: str = "gpt-4o", quiet: bool = False):
        self.output_dir = output_dir
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.quiet = quiet


class RAGNativeTester:
    """RAGAnything åŸç”Ÿ API æµ‹è¯•å™¨"""

    def __init__(
        self,
        samples_path: str,
        results_file: str,
        kb_base_dir: str,
        max_samples: int = None,
        start_index: int = 0,
        force_rerun: bool = False,
    ):
        self.samples_path = Path(samples_path)
        self.results_file = Path(results_file)
        self.kb_base_dir = Path(kb_base_dir)
        self.max_samples = max_samples
        self.start_index = start_index
        self.force_rerun = force_rerun

        # åŠ è½½æ ·æœ¬
        self.samples = self._load_samples()
        
        # LLM è¯„ä¼°å™¨é…ç½®
        self.llm_evaluator = self._init_llm_evaluator()
        
        # RAG å®ä¾‹ç¼“å­˜ï¼ˆæ¯ä¸ªçŸ¥è¯†åº“ä¸€ä¸ªï¼‰
        self.rag_instances = {}
        
        # é¦–æ¬¡æŸ¥è¯¢æ ‡è®°
        self._first_query = True

    def _init_llm_evaluator(self):
        """åˆå§‹åŒ– LLM è¯„ä¼°å™¨"""
        llm_api_key = os.getenv("LLM_BINDING_API_KEY")
        llm_base_url = os.getenv("LLM_BINDING_HOST")
        llm_model = os.getenv("LLM_MODEL", "qwen-plus")
        
        config = LLMEvaluatorConfig(
            output_dir=str(self.results_file.parent),
            api_key=llm_api_key,
            base_url=llm_base_url,
            model=llm_model,
            quiet=True
        )
        return LLMAnswerEvaluator(config)

    def _load_samples(self) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ ·æœ¬"""
        # ä»åŸå§‹ samples.json åŠ è½½æ‰€æœ‰æ ·æœ¬
        with open(self.samples_path, "r", encoding="utf-8") as f:
            all_samples = json.load(f)

        # å¦‚æœæŒ‡å®šäº† max_samplesï¼Œåˆ™æˆªæ–­
        if self.max_samples:
            all_samples = all_samples[: self.max_samples]

        # åŠ è½½å·²å®Œæˆçš„ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        completed_results_map = {}
        if self.results_file.exists():
            print(f"ä»ç°æœ‰ç»“æœæ–‡ä»¶åŠ è½½: {self.results_file}")
            with open(self.results_file, "r", encoding="utf-8") as f:
                existing_results = json.load(f)
                for res in existing_results:
                    key = (res.get("question"), res.get("doc_id"))
                    completed_results_map[key] = res

        # åˆå¹¶å·²å®Œæˆçš„ç»“æœ
        for i, sample in enumerate(all_samples):
            key = (sample.get("question"), sample.get("doc_id"))
            if key in completed_results_map:
                all_samples[i] = completed_results_map[key]

        # è¿‡æ»¤æ‰æ²¡æœ‰çŸ¥è¯†åº“çš„æ ·æœ¬
        filtered_samples = []
        skipped_count = 0
        for sample in all_samples:
            kb_name = self._get_kb_name_from_doc_id(sample["doc_id"])
            if self._check_kb_exists(kb_name):
                filtered_samples.append(sample)
            else:
                skipped_count += 1
        
        if skipped_count > 0:
            print(f"âš ï¸  è·³è¿‡ {skipped_count} ä¸ªæ²¡æœ‰çŸ¥è¯†åº“çš„æ ·æœ¬")
        
        all_samples = filtered_samples

        # ä»æŒ‡å®šç´¢å¼•å¼€å§‹
        if self.start_index > 0 and not self.force_rerun:
            all_samples = all_samples[self.start_index:]

        print(f"åŠ è½½äº† {len(all_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return all_samples

    def _save_results(self):
        """ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶"""
        with open(self.results_file, "w", encoding="utf-8") as f:
            json.dump(self.samples, f, ensure_ascii=False, indent=2)

    def _get_kb_name_from_doc_id(self, doc_id: str) -> str:
        """ä» doc_id è·å–çŸ¥è¯†åº“åç§°"""
        import re
        # ç§»é™¤ .pdf æ‰©å±•å
        name = doc_id.replace(".pdf", "")
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
        name = re.sub(r'[^\w\-_]', '_', name)
        # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
        name = re.sub(r'_+', '_', name)
        # ç¡®ä¿ä»¥ mmlongbench_ å¼€å¤´
        if not name.startswith("mmlongbench_"):
            name = f"mmlongbench_{name}"
        return name

    def _check_kb_exists(self, kb_name: str) -> bool:
        """æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨"""
        kb_path = self.kb_base_dir / kb_name
        rag_storage_path = kb_path / "rag_storage"
        metadata_file = kb_path / "metadata.json"
        return rag_storage_path.is_dir() and metadata_file.is_file()

    async def _get_rag_instance(self, kb_name: str) -> RAGAnything:
        """è·å–æˆ–åˆ›å»º RAG å®ä¾‹"""
        if kb_name in self.rag_instances:
            return self.rag_instances[kb_name]

        # åˆ›å»ºæ–°çš„ RAG å®ä¾‹
        working_dir = str(self.kb_base_dir / kb_name / "rag_storage")
        
        if not Path(working_dir).exists():
            raise ValueError(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {working_dir}")

        # é…ç½®
        config = RAGAnythingConfig(
            working_dir=working_dir,
            parser="mineru",
            parse_method="auto",
            enable_image_processing=True,
            enable_table_processing=True,
            enable_equation_processing=True,
        )

        # LLM é…ç½®
        api_key = os.getenv("LLM_BINDING_API_KEY")
        base_url = os.getenv("LLM_BINDING_HOST")
        model = os.getenv("LLM_MODEL", "qwen-plus")

        def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
            return openai_complete_if_cache(
                model,
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )

        def vision_model_func(
            prompt,
            system_prompt=None,
            history_messages=[],
            image_data=None,
            messages=None,
            **kwargs,
        ):
            if messages:
                return openai_complete_if_cache(
                    model,
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=messages,
                    api_key=api_key,
                    base_url=base_url,
                    **kwargs,
                )
            elif image_data:
                return openai_complete_if_cache(
                    model,
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=[
                        {"role": "system", "content": system_prompt} if system_prompt else None,
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},
                                },
                            ],
                        }
                        if image_data
                        else {"role": "user", "content": prompt},
                    ],
                    api_key=api_key,
                    base_url=base_url,
                    **kwargs,
                )
            else:
                return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

        # Embedding é…ç½®
        embedding_dim = int(os.getenv("EMBEDDING_DIM", "1024"))
        embedding_model = os.getenv("EMBEDDING_MODEL", "text-embedding-v3")
        embedding_api_key = os.getenv("EMBEDDING_BINDING_API_KEY")
        embedding_base_url = os.getenv("EMBEDDING_BINDING_HOST")

        embedding_func = EmbeddingFunc(
            embedding_dim=embedding_dim,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model=embedding_model,
                api_key=embedding_api_key,
                base_url=embedding_base_url,
            ),
        )

        # åˆ›å»º RAG å®ä¾‹
        rag = RAGAnything(
            config=config,
            llm_model_func=llm_model_func,
            vision_model_func=vision_model_func,
            embedding_func=embedding_func,
        )

        # åˆå§‹åŒ– LightRAGï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
        init_result = await rag._ensure_lightrag_initialized()
        if not init_result.get("success", False):
            raise ValueError(f"Failed to initialize LightRAG: {init_result.get('error', 'Unknown error')}")

        self.rag_instances[kb_name] = rag
        return rag

    async def test_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæ ·æœ¬"""
        if "score" in sample and not self.force_rerun:
            return sample

        question = sample["question"]
        doc_id = sample["doc_id"]

        print(f"\nå¤„ç†é—®é¢˜: {question[:80]}...")
        print(f"æ–‡æ¡£: {doc_id}")

        try:
            kb_name = self._get_kb_name_from_doc_id(doc_id)
            print(f"ä½¿ç”¨çŸ¥è¯†åº“: {kb_name}")

            # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            if not self._check_kb_exists(kb_name):
                error_msg = f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_name}"
                print(f"âš ï¸  {error_msg}")
                sample["response"] = "Knowledge base not found"
                sample["pred"] = "Knowledge base not found"
                sample["score"] = 0.0
                sample["error"] = error_msg
                sample["kb_name"] = kb_name
                return sample

            # è·å– RAG å®ä¾‹
            if self._first_query:
                print(f"â³ é¦–æ¬¡æŸ¥è¯¢éœ€è¦åˆå§‹åŒ–çŸ¥è¯†åº“ï¼Œè¯·ç¨å€™...")
                self._first_query = False
            
            print(f"ğŸ” å¼€å§‹ RAG æŸ¥è¯¢ (hybrid mode)...")
            sys.stdout.flush()
            
            try:
                rag = await asyncio.wait_for(
                    self._get_rag_instance(kb_name),
                    timeout=60.0
                )
                
                # æ‰§è¡ŒæŸ¥è¯¢
                answer = await asyncio.wait_for(
                    rag.aquery(question, mode="hybrid"),
                    timeout=120.0
                )
                
                print(f"âœ… RAG æŸ¥è¯¢å®Œæˆ")
                
            except asyncio.TimeoutError:
                error_msg = "RAG query timeout"
                print(f"â±ï¸  {error_msg}")
                sample["response"] = "Timeout"
                sample["pred"] = "Timeout"
                sample["score"] = 0.0
                sample["error"] = error_msg
                sample["kb_name"] = kb_name
                return sample

            # è¯„ä¼°ç­”æ¡ˆ
            try:
                eval_result = await self.llm_evaluator.evaluate_single_answer(
                    question=question,
                    expected_answer=str(sample["answer"]),
                    generated_answer=answer,
                    evidence_pages=str(sample.get("evidence_pages", "")),
                    evidence_sources=str(sample.get("evidence_sources", "")),
                    doc_id=sample["doc_id"],
                    evaluation_type="accuracy_only"
                )
                
                # ä»è¯„ä¼°ç»“æœä¸­æå–åˆ†æ•°
                # eval_result å¯èƒ½åŒ…å« "accuracy" æˆ– "overall_score" ç­‰å­—æ®µ
                if "scores" in eval_result and "overall_accuracy" in eval_result["scores"]:
                    score = eval_result["scores"]["overall_accuracy"]
                elif "accuracy" in eval_result:
                    score = eval_result["accuracy"] if eval_result["accuracy"] is not None else 0.0
                elif "overall_score" in eval_result:
                    score = eval_result["overall_score"] if eval_result["overall_score"] is not None else 0.0
                else:
                    # é»˜è®¤å€¼
                    score = 0.0
                
                # å½’ä¸€åŒ–åˆ†æ•°åˆ° 0-1 èŒƒå›´
                if isinstance(score, (int, float)) and score > 1:
                    score = score / 100.0
                    
                sample["response"] = answer
                sample["pred"] = answer
                sample["score"] = score
                sample["eval_reasoning"] = eval_result.get("reasoning", "")
                sample["kb_name"] = kb_name
                
                # æ˜¾ç¤ºç»“æœ
                score_icon = "âœ…" if score >= 0.5 else "âŒ"
                print(f"\n{'='*80}")
                print(f"ğŸ“ é—®é¢˜: {question}")
                print(f"\nğŸ’¬ RAGå®Œæ•´è¾“å‡º:")
                print(answer)
                print(f"\nâœ“  æ­£ç¡®ç­”æ¡ˆ: {sample['answer']}")
                print(f"{score_icon} å¾—åˆ†: {score}")
                
                # æ˜¾ç¤ºå®Œæ•´çš„è¯„ä¼°æ¨ç†
                eval_reasoning = eval_result.get("reasoning", "")
                if eval_reasoning:
                    print(f"\nğŸ“‹ è¯„ä¼°æ¨ç†:")
                    print(eval_reasoning)
                print('='*80)

            except Exception as e:
                print(f"âš ï¸  è¯„ä¼°å¤±è´¥: {e}")
                sample["response"] = answer
                sample["pred"] = answer
                sample["score"] = 0.0
                sample["error"] = f"Evaluation error: {str(e)}"
                sample["kb_name"] = kb_name

        except Exception as e:
            sample["response"] = f"Error: {str(e)}"
            sample["score"] = 0.0
            sample["error"] = str(e)
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")

        return sample

    async def run_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "=" * 80)
        print("å¼€å§‹ RAG Native æµ‹è¯• (ä½¿ç”¨ RAGAnything åŸç”Ÿ API)")
        print("=" * 80)

        self.results_file.parent.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ æ€»å…± {len(self.samples)} ä¸ªæ ·æœ¬å¾…æµ‹è¯•")
        print(f"ğŸ’¾ ç»“æœå°†ä¿å­˜åˆ°: {self.results_file}")
        print(f"ğŸ“‚ çŸ¥è¯†åº“ç›®å½•: {self.kb_base_dir}")
        print("\nå¼€å§‹å¤„ç†æ ·æœ¬...\n")

        for i, sample in enumerate(tqdm(self.samples, desc="æµ‹è¯•è¿›åº¦", ncols=80, mininterval=0.1)):
            print(f"\n{'='*60}")
            print(f"[{i+1}/{len(self.samples)}] å¼€å§‹å¤„ç†æ ·æœ¬...")
            sys.stdout.flush()
            
            if "score" in sample and not self.force_rerun:
                completed_samples = [s for s in self.samples if "score" in s]
                if completed_samples and (i % 5 == 0 or i == len(self.samples) - 1):
                    total_score = sum(s.get("score", 0) for s in completed_samples)
                    current_acc = total_score / len(completed_samples)
                    print(f"ğŸ“Š ç´¯è®¡å‡†ç¡®ç‡: {current_acc:.2%} ({int(total_score)}/{len(completed_samples)})")
                continue

            try:
                sample = await self.test_sample(sample)
                self._save_results()
                
                # ç»Ÿè®¡å·²å®Œæˆçš„æ ·æœ¬ï¼ˆåŒ…æ‹¬æœ¬è½®æ–°å®Œæˆçš„ï¼‰
                completed_samples = [s for s in self.samples[:i+1] if "score" in s]
                if completed_samples:
                    total_score = sum(s.get("score", 0) for s in completed_samples)
                    current_acc = total_score / len(completed_samples)
                    print(f"ğŸ“Š ç´¯è®¡å‡†ç¡®ç‡: {current_acc:.2%} ({total_score:.1f}/{len(completed_samples)})")

            except KeyboardInterrupt:
                print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•ï¼Œä¿å­˜å½“å‰ç»“æœ...")
                self._save_results()
                break
            except Exception as e:
                print(f"âŒ å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {e}")
                sample["error"] = str(e)
                sample["score"] = 0.0
                self._save_results()
                continue

        print("\n" + "=" * 80)
        print("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
        print("=" * 80)
        self._generate_report()

    def _generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        completed_samples = [s for s in self.samples if "score" in s]
        if not completed_samples:
            print("æ²¡æœ‰å®Œæˆçš„æ ·æœ¬ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
            return

        total_score = sum(s.get("score", 0) for s in completed_samples)
        total_count = len(completed_samples)
        accuracy = total_score / total_count if total_count > 0 else 0

        correct = sum(1 for s in completed_samples if s.get("score", 0) >= 0.5)
        incorrect = total_count - correct

        # æŒ‰çŸ¥è¯†åº“ç»Ÿè®¡
        kb_stats = {}
        for s in completed_samples:
            kb = s.get("kb_name", "unknown")
            if kb not in kb_stats:
                kb_stats[kb] = {"total": 0, "score": 0}
            kb_stats[kb]["total"] += 1
            kb_stats[kb]["score"] += s.get("score", 0)

        report_lines = [
            "\n" + "=" * 80,
            f"RAG Native æµ‹è¯•æŠ¥å‘Š (ä½¿ç”¨ RAGAnything åŸç”Ÿ API)",
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "=" * 80,
            f"Overall Accuracy: {accuracy:.4f} | Question Number: {total_count}",
            f"Correct: {correct} | Incorrect: {incorrect}",
            "",
            "æŒ‰çŸ¥è¯†åº“ç»Ÿè®¡:",
        ]

        for kb, stats in sorted(kb_stats.items()):
            kb_acc = stats["score"] / stats["total"] if stats["total"] > 0 else 0
            report_lines.append(f"  {kb}: Accuracy: {kb_acc:.4f} | Questions: {stats['total']}")

        report_text = "\n".join(report_lines)
        print(report_text)

        # ä¿å­˜æŠ¥å‘Š
        report_file = self.results_file.parent / "report_rag_native.txt"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report_text)
        
        print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


async def main():
    parser = argparse.ArgumentParser(description="RAG Native æµ‹è¯• - ä½¿ç”¨ RAGAnything åŸç”Ÿ API")
    parser.add_argument(
        "--samples",
        type=str,
        default=str(mmlongbench_root / "data" / "samples.json"),
        help="æ ·æœ¬æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--results",
        type=str,
        default="test_results/results_rag_native.json",
        help="ç»“æœæ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--kb_dir",
        type=str,
        default="../../data/knowledge_bases",
        help="çŸ¥è¯†åº“åŸºç¡€ç›®å½•",
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°",
    )
    parser.add_argument(
        "--start_index",
        type=int,
        default=0,
        help="èµ·å§‹ç´¢å¼•",
    )
    parser.add_argument(
        "--force_rerun",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°è¿è¡Œæ‰€æœ‰æµ‹è¯•",
    )

    args = parser.parse_args()

    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    kb_dir = Path(args.kb_dir)
    if not kb_dir.is_absolute():
        kb_dir = Path(__file__).parent / kb_dir
    kb_dir = kb_dir.resolve()

    tester = RAGNativeTester(
        samples_path=args.samples,
        results_file=args.results,
        kb_base_dir=str(kb_dir),
        max_samples=args.max_samples,
        start_index=args.start_index,
        force_rerun=args.force_rerun,
    )

    await tester.run_tests()


if __name__ == "__main__":
    asyncio.run(main())

