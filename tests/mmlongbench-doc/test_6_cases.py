#!/usr/bin/env python3
"""
æµ‹è¯•6ä¸ª"è¿‡æ—©æ”¾å¼ƒ"çš„ä»£è¡¨æ€§æ¡ˆä¾‹
éªŒè¯promptæ”¹è¿›åçš„æ•ˆæœ
"""

import asyncio
import json
import logging
import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# åˆ‡æ¢å·¥ä½œç›®å½•åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼ˆç¡®ä¿ç›¸å¯¹è·¯å¾„æ­£ç¡®è§£æï¼‰
os.chdir(project_root)

from src.agents.solve.main_solver import MainSolver
from src.knowledge.initializer import KnowledgeBaseInitializer
from llm_answer_evaluator import LLMAnswerEvaluator

# ç®€å•çš„è¯„ä¼°å™¨é…ç½®ç±»
class SimplEvaluatorConfig:
    def __init__(self, output_dir: str, api_key: str, base_url: str, model: str = "gpt-4o-mini", quiet: bool = False):
        self.output_dir = output_dir
        self.api_key = api_key if api_key else os.environ.get("OPENAI_API_KEY")
        self.base_url = base_url if base_url else "https://api.openai.com/v1"
        self.model = model
        self.quiet = quiet

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# 6ä¸ªä»£è¡¨æ€§æ¡ˆä¾‹
TEST_CASES = [
    {
        "case_id": 1,
        "doc_id": "Independents-Report.pdf",
        "question": "What's the percentage of people who are democrats and voted in the last election compared to the entire population in 2018?",
        "expected_answer": "18.29%",
        "error_type": "calculation - needs decomposition"
    },
    {
        "case_id": 2,
        "doc_id": "earlybird-110722143746-phpapp02_95.pdf",
        "question": "Which two magazines' opinions are selected to illustrate the situation of German venture capital?",
        "expected_answer": "['The Economist', 'TIME']",
        "error_type": "visual/text - needs diverse queries"
    },
    {
        "case_id": 3,
        "doc_id": "reportq32015-151009093138-lva1-app6891_95.pdf",
        "question": "Which APP on APPOTA platform is top 10 Vietnam Android App, but not top 10 Vietnam iOS App?",
        "expected_answer": "UC Browser Tiáº¿ng Viá»‡t",
        "error_type": "set comparison - needs list operation"
    },
    {
        "case_id": 4,
        "doc_id": "0e94b4197b10096b1f4c699701570fbf.pdf",
        "question": "Which continent has the most number of registered participant for advanced science course in CTBTO?",
        "expected_answer": "Europe",
        "error_type": "visual data - clue tracking failure"
    },
    {
        "case_id": 5,
        "doc_id": "reportq32015-151009093138-lva1-app6891_95.pdf",
        "question": "Between Java and WP, how large is the difference in percentage of their global developers mindshare?",
        "expected_answer": "17.5",
        "error_type": "calculation - synonym query needed"
    },
    {
        "case_id": 6,
        "doc_id": "reportq32015-151009093138-lva1-app6891_95.pdf",
        "question": "Which news appear in both Vietnam mobile news and APPOTA news?",
        "expected_answer": "Bluebird Award",
        "error_type": "semantic understanding - question misinterpretation"
    }
]


class SixCaseTester:
    def __init__(
        self,
        kb_base_dir: Path,
        output_dir: Path
    ):
        self.kb_base_dir = kb_base_dir
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        eval_config = SimplEvaluatorConfig(
            output_dir=str(output_dir),
            api_key=os.environ.get("OPENAI_API_KEY"),
            base_url="https://api.openai.com/v1",
            model="gpt-4o-mini",
            quiet=False
        )
        self.evaluator = LLMAnswerEvaluator(eval_config)
        
        self.results = []
        
    def _get_kb_name_from_doc_id(self, doc_id: str) -> str:
        """ä»doc_idç”ŸæˆçŸ¥è¯†åº“åç§° - ä¿æŒåŸå§‹å‘½åæ–¹å¼"""
        # ç§»é™¤ .pdf åç¼€
        base_name = doc_id.replace('.pdf', '')
        # åªæ›¿æ¢ç‚¹å·ï¼Œä¿ç•™æ¨ªçº¿ï¼ˆä¸ä¹‹å‰æµ‹è¯•ä¸€è‡´ï¼‰
        kb_name = base_name.replace('.', '_')
        return f"mmlongbench_{kb_name}"
    
    def _check_kb_exists(self, kb_name: str) -> bool:
        """æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨"""
        kb_path = self.kb_base_dir / kb_name
        
        # æ£€æŸ¥ rag_storage ç›®å½•å’Œ metadata.json
        rag_storage = kb_path / "rag_storage"
        metadata_file = kb_path / "metadata.json"
        
        exists = rag_storage.exists() and metadata_file.exists()
        
        if not exists:
            logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_path}")
            logger.warning(f"  - rag_storageå­˜åœ¨: {rag_storage.exists()}")
            logger.warning(f"  - metadata.jsonå­˜åœ¨: {metadata_file.exists()}")
        
        return exists
    
    async def test_single_case(self, case: Dict[str, Any]) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªæ¡ˆä¾‹"""
        case_id = case['case_id']
        doc_id = case['doc_id']
        question = case['question']
        expected_answer = case['expected_answer']
        error_type = case['error_type']
        
        kb_name = self._get_kb_name_from_doc_id(doc_id)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"æ¡ˆä¾‹ {case_id}/{len(TEST_CASES)}: {error_type}")
        logger.info(f"æ–‡æ¡£: {doc_id}")
        logger.info(f"çŸ¥è¯†åº“: {kb_name}")
        logger.info(f"é—®é¢˜: {question[:100]}...")
        logger.info(f"æ­£ç¡®ç­”æ¡ˆ: {expected_answer}")
        logger.info(f"{'='*80}\n")
        
        # æ£€æŸ¥çŸ¥è¯†åº“
        if not self._check_kb_exists(kb_name):
            logger.error(f"âŒ çŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œè·³è¿‡æ­¤æ¡ˆä¾‹")
            return {
                'case_id': case_id,
                'doc_id': doc_id,
                'question': question,
                'expected_answer': expected_answer,
                'error_type': error_type,
                'status': 'skipped',
                'reason': 'Knowledge base not found'
            }
        
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            solver_output_dir = self.output_dir / f"case_{case_id}_outputs"
            solver_output_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»º Main Solver
            solver = MainSolver(
                kb_name=kb_name,
                output_base_dir=str(solver_output_dir),
            )
            
            # è¿è¡Œ solver
            logger.info(f"å¼€å§‹è¿è¡Œ Solver...")
            start_time = datetime.now()
            
            result = await solver.solve(question=question)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ“ Solver å®Œæˆ (è€—æ—¶: {elapsed:.1f}ç§’)")
            
            # æå–ç­”æ¡ˆ
            concise_answer = result.get('concise_answer', '')
            response = result.get('response', '')
            
            logger.info(f"\nç³»ç»Ÿè¾“å‡º: {concise_answer}")
            
            # è¯„ä¼°ç­”æ¡ˆ
            logger.info(f"å¼€å§‹è¯„ä¼°ç­”æ¡ˆ...")
            eval_result = await self.evaluator.evaluate_answer(
                question=question,
                generated_answer=concise_answer,
                expected_answer=expected_answer,
                full_response=response
            )
            
            score = eval_result.get('accuracy', 0.0)
            reasoning = eval_result.get('reasoning', '')
            
            logger.info(f"{'='*80}")
            if score >= 0.5:
                logger.info(f"âœ… æ¡ˆä¾‹ {case_id} - æ­£ç¡®ï¼ (å¾—åˆ†: {score})")
            else:
                logger.info(f"âŒ æ¡ˆä¾‹ {case_id} - é”™è¯¯ (å¾—åˆ†: {score})")
            logger.info(f"è¯„ä¼°ç†ç”±: {reasoning[:200]}...")
            logger.info(f"{'='*80}\n")
            
            # ä¿å­˜ç»“æœ
            case_result = {
                'case_id': case_id,
                'doc_id': doc_id,
                'kb_name': kb_name,
                'question': question,
                'expected_answer': expected_answer,
                'error_type': error_type,
                'concise_answer': concise_answer,
                'full_response': response,
                'score': score,
                'eval_reasoning': reasoning,
                'elapsed_seconds': elapsed,
                'status': 'completed'
            }
            
            return case_result
            
        except Exception as e:
            logger.error(f"âŒ æ¡ˆä¾‹ {case_id} æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return {
                'case_id': case_id,
                'doc_id': doc_id,
                'question': question,
                'expected_answer': expected_answer,
                'error_type': error_type,
                'status': 'failed',
                'error': str(e)
            }
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•æ¡ˆä¾‹"""
        logger.info(f"\nğŸš€ å¼€å§‹æµ‹è¯• 6 ä¸ªä»£è¡¨æ€§æ¡ˆä¾‹")
        logger.info(f"çŸ¥è¯†åº“ç›®å½•: {self.kb_base_dir}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}\n")
        
        for case in TEST_CASES:
            result = await self.test_single_case(case)
            self.results.append(result)
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report()
    
    def _generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info(f"\n{'='*80}")
        logger.info("æµ‹è¯•æ€»ç»“")
        logger.info(f"{'='*80}\n")
        
        completed = [r for r in self.results if r['status'] == 'completed']
        skipped = [r for r in self.results if r['status'] == 'skipped']
        failed = [r for r in self.results if r['status'] == 'failed']
        
        if completed:
            correct = [r for r in completed if r['score'] >= 0.5]
            incorrect = [r for r in completed if r['score'] < 0.5]
            
            accuracy = len(correct) / len(completed) * 100 if completed else 0
            
            logger.info(f"æ€»æµ‹è¯•: {len(TEST_CASES)} ä¸ªæ¡ˆä¾‹")
            logger.info(f"å®Œæˆ: {len(completed)} ä¸ª")
            logger.info(f"è·³è¿‡: {len(skipped)} ä¸ª")
            logger.info(f"å¤±è´¥: {len(failed)} ä¸ª")
            logger.info(f"\nâœ“ æ­£ç¡®: {len(correct)} ä¸ª")
            logger.info(f"âœ— é”™è¯¯: {len(incorrect)} ä¸ª")
            logger.info(f"\nå‡†ç¡®ç‡: {accuracy:.1f}% ({len(correct)}/{len(completed)})")
            
            # è¯¦ç»†ç»“æœ
            logger.info(f"\nè¯¦ç»†ç»“æœ:")
            for r in completed:
                status_icon = "âœ…" if r['score'] >= 0.5 else "âŒ"
                logger.info(f"{status_icon} æ¡ˆä¾‹ {r['case_id']}: {r['error_type']}")
                logger.info(f"   é—®é¢˜: {r['question'][:80]}...")
                logger.info(f"   æ­£ç¡®: {r['expected_answer']}")
                logger.info(f"   è¾“å‡º: {r.get('concise_answer', 'N/A')}")
                logger.info(f"   å¾—åˆ†: {r['score']}")
                logger.info("")
            
            # å¯¹æ¯”åŸºçº¿
            logger.info(f"\n{'='*80}")
            logger.info("å¯¹æ¯”åŸºçº¿ (æ”¹è¿›å‰è¿™6ä¸ªæ¡ˆä¾‹å…¨é”™)")
            logger.info(f"{'='*80}")
            logger.info(f"æ”¹è¿›å‰: 0/6 = 0%")
            logger.info(f"æ”¹è¿›å: {len(correct)}/{len(completed)} = {accuracy:.1f}%")
            improvement = len(correct)
            logger.info(f"æå‡: +{improvement} ä¸ªæ­£ç¡®ç­”æ¡ˆ (+{improvement/6*100:.0f}%)")
        
        # ä¿å­˜ JSON ç»“æœ
        result_file = self.output_dir / "6_cases_results.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"\nç»“æœå·²ä¿å­˜è‡³: {result_file}")


async def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®è·¯å¾„
    kb_base_dir = Path("/Users/howard/Documents/forks/DeepTutor/data/knowledge_bases")
    output_dir = Path(__file__).parent / "test_results" / "6_cases"
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = SixCaseTester(
        kb_base_dir=kb_base_dir,
        output_dir=output_dir
    )
    
    # è¿è¡Œæµ‹è¯•
    await tester.run_all_tests()


if __name__ == "__main__":
    asyncio.run(main())
