#!/usr/bin/env python
"""
æµ‹è¯•è„šæœ¬ï¼šç›´æ¥ä½¿ç”¨ RAGAnything (hybrid mode) å›ç­” MMLongBench-Doc é—®é¢˜

åŠŸèƒ½ï¼š
1. è¯»å– MMLongBench-Doc çš„ samples.json
2. å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œç›´æ¥ä½¿ç”¨ RAG hybrid æœç´¢è·å–ç­”æ¡ˆ
3. ä½¿ç”¨ LLM-as-a-Judge è¯„ä¼°ç­”æ¡ˆå‡†ç¡®æ€§
4. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

ä¸ test_solve_mmlongbench.py çš„åŒºåˆ«ï¼š
- ä¸ä½¿ç”¨å®Œæ•´çš„ solver pipeline
- ç›´æ¥è°ƒç”¨ RAG hybrid æœç´¢
- æ›´å¿«é€Ÿï¼Œç”¨äºæµ‹è¯• RAG çš„åŸºç¡€æ£€ç´¢èƒ½åŠ›
"""

import argparse
import asyncio
import json
import os
import sys
import warnings
from pathlib import Path
from typing import Any, Dict, List
from datetime import datetime

# æŠ‘åˆ¶å¸¸è§çš„å¼‚æ­¥ç›¸å…³è­¦å‘Š
warnings.filterwarnings("ignore", message=".*no current event loop.*")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="asyncio")

# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸º WARNINGï¼Œå…³é—­ INFO æ—¥å¿—
import logging
logging.getLogger().setLevel(logging.WARNING)
for logger_name in ["httpx", "httpcore", "openai", "urllib3", "asyncio", "lightrag", "raganything"]:
    logging.getLogger(logger_name).setLevel(logging.WARNING)

# ç‰¹åˆ«æŠ‘åˆ¶ LightRAG çš„è¯¦ç»†æ—¥å¿—
logging.getLogger("lightrag").setLevel(logging.ERROR)
logging.getLogger("raganything").setLevel(logging.ERROR)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# MMLongBench-Doc æ•°æ®è·¯å¾„
mmlongbench_root = project_root.parent / "MMLongBench-Doc"

from dotenv import load_dotenv
from tqdm import tqdm

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env", override=False)

# å¯¼å…¥ RAG å·¥å…·
from src.tools.rag_tool import rag_search

# å¯¼å…¥ LLM è¯„ä¼°æ¨¡å—
from llm_answer_evaluator import LLMAnswerEvaluator


class LLMEvaluatorConfig:
    """LLMè¯„ä¼°å™¨é…ç½®"""
    def __init__(self, output_dir: str, api_key: str, base_url: str, model: str = "gpt-4o", quiet: bool = False):
        self.output_dir = output_dir
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.quiet = quiet


class RAGDirectTester:
    """RAG ç›´æ¥æµ‹è¯•å™¨"""

    def __init__(
        self,
        samples_path: str,
        results_file: str,
        kb_base_dir: str,
        max_samples: int = None,
        start_index: int = 0,
        force_rerun: bool = False,
        skip_missing_kb: bool = True,
    ):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨

        Args:
            samples_path: æ ·æœ¬æ–‡ä»¶è·¯å¾„
            results_file: ç»“æœæ–‡ä»¶è·¯å¾„
            kb_base_dir: çŸ¥è¯†åº“åŸºç¡€ç›®å½•
            max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°
            start_index: èµ·å§‹ç´¢å¼•
            force_rerun: æ˜¯å¦å¼ºåˆ¶é‡æ–°è¿è¡Œ
            skip_missing_kb: æ˜¯å¦è·³è¿‡æ²¡æœ‰çŸ¥è¯†åº“çš„æ ·æœ¬
        """
        self.samples_path = Path(samples_path)
        self.results_file = Path(results_file)
        self.kb_base_dir = Path(kb_base_dir)
        self.max_samples = max_samples
        self.start_index = start_index
        self.force_rerun = force_rerun
        self.skip_missing_kb = skip_missing_kb
        
        # ç”¨äºè·Ÿè¸ªæ˜¯å¦å·²ç»æç¤ºè¿‡åˆå§‹åŒ–
        self._first_query = True

        # åŠ è½½æ ·æœ¬
        self.samples = self._load_samples()

        # åˆå§‹åŒ– LLM è¯„ä¼°å™¨
        llm_api_key = os.getenv("LLM_API_KEY")
        llm_base_url = os.getenv("LLM_BASE_URL")
        llm_model = os.getenv("LLM_MODEL", "gpt-4o")
        
        evaluator_config = LLMEvaluatorConfig(
            output_dir=str(self.results_file.parent / "evaluations"),
            api_key=llm_api_key,
            base_url=llm_base_url,
            model=llm_model,
            quiet=True
        )
        self.llm_evaluator = LLMAnswerEvaluator(evaluator_config)

        # æ–‡æ¡£IDåˆ°çŸ¥è¯†åº“åç§°çš„æ˜ å°„
        self.kb_cache = {}

    def _load_samples(self) -> List[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•æ ·æœ¬"""
        # åŠ è½½æ‰€æœ‰æ ·æœ¬
        with open(self.samples_path, "r", encoding="utf-8") as f:
            all_samples = json.load(f)

        # å¦‚æœæŒ‡å®šäº† max_samplesï¼Œåˆ™æˆªæ–­
        if self.max_samples:
            all_samples = all_samples[: self.max_samples]

        # åŠ è½½å·²å®Œæˆçš„ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.results_file.exists() and not self.force_rerun:
            print(f"ä»ç°æœ‰ç»“æœæ–‡ä»¶åŠ è½½: {self.results_file}")
            with open(self.results_file, "r", encoding="utf-8") as f:
                existing_results = json.load(f)
            
            # åˆ›å»ºç»“æœå­—å…¸
            results_dict = {}
            for result in existing_results:
                key = (result.get("question", ""), result.get("doc_id", ""))
                results_dict[key] = result
            
            # åˆå¹¶å·²å®Œæˆçš„ç»“æœåˆ°æ ·æœ¬ä¸­
            merged_count = 0
            for sample in all_samples:
                key = (sample.get("question", ""), sample.get("doc_id", ""))
                if key in results_dict:
                    existing = results_dict[key]
                    sample.update(existing)
                    merged_count += 1
            
            if merged_count > 0:
                print(f"  åˆå¹¶äº† {merged_count} ä¸ªå·²å®Œæˆçš„ç»“æœ")

        # ä»æŒ‡å®šç´¢å¼•å¼€å§‹
        if self.start_index > 0:
            all_samples = all_samples[self.start_index:]

        # å¦‚æœè®¾ç½®äº† skip_missing_kbï¼Œè¿‡æ»¤æ‰æ²¡æœ‰çŸ¥è¯†åº“çš„æ ·æœ¬
        if self.skip_missing_kb:
            filtered_samples = []
            for sample in all_samples:
                kb_name = self._get_kb_name_from_doc_id(sample.get("doc_id", ""))
                if self._check_kb_exists(kb_name):
                    filtered_samples.append(sample)
            
            skipped_count = len(all_samples) - len(filtered_samples)
            if skipped_count > 0:
                print(f"âš ï¸  è·³è¿‡ {skipped_count} ä¸ªæ²¡æœ‰çŸ¥è¯†åº“çš„æ ·æœ¬")
            all_samples = filtered_samples

        print(f"åŠ è½½äº† {len(all_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return all_samples

    def _get_kb_name_from_doc_id(self, doc_id: str) -> str:
        """ä»æ–‡æ¡£IDç”ŸæˆçŸ¥è¯†åº“åç§°"""
        import re
        # ç§»é™¤ .pdf æ‰©å±•å
        name = doc_id.replace(".pdf", "")
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
        name = re.sub(r'[^\w\-_]', '_', name)
        # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
        name = re.sub(r'_+', '_', name)
        # ç¡®ä¿ä»¥ mmlongbench_ å¼€å¤´
        if not name.startswith("mmlongbench_"):
            name = f"mmlongbench_{name}"
        return name

    def _check_kb_exists(self, kb_name: str) -> bool:
        """æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨"""
        kb_dir = self.kb_base_dir / kb_name
        # DeepTutor çŸ¥è¯†åº“çš„æ ‡å¿—ï¼šå­˜åœ¨ rag_storage ç›®å½•å’Œ metadata.json æ–‡ä»¶
        return kb_dir.exists() and (kb_dir / "rag_storage").exists() and (kb_dir / "metadata.json").exists()

    def _extract_answer_from_rag(self, rag_result: Dict[str, Any]) -> str:
        """ä» RAG ç»“æœä¸­æå–ç­”æ¡ˆ"""
        # RAG è¿”å›çš„ç»“æœç»“æ„ï¼š{"answer": "...", "sources": [...]}
        answer = rag_result.get("answer", "")
        if not answer:
            return "Not answerable"
        return answer.strip()

    async def test_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        æµ‹è¯•å•ä¸ªæ ·æœ¬

        Args:
            sample: æµ‹è¯•æ ·æœ¬

        Returns:
            æ›´æ–°åçš„æ ·æœ¬
        """
        # å¦‚æœå·²ç»æœ‰ç»“æœä¸”ä¸æ˜¯å¼ºåˆ¶é‡æ–°è¿è¡Œï¼Œè·³è¿‡
        if "score" in sample and not self.force_rerun:
            return sample

        question = sample["question"]
        doc_id = sample["doc_id"]

        print(f"\nå¤„ç†é—®é¢˜: {question[:80]}...")
        print(f"æ–‡æ¡£: {doc_id}")

        try:
            # è·å–çŸ¥è¯†åº“åç§°
            kb_name = self._get_kb_name_from_doc_id(doc_id)
            self.kb_cache[doc_id] = kb_name

            # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            if not self._check_kb_exists(kb_name):
                # å¦‚æœ skip_missing_kb ä¸º Trueï¼Œè¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼ˆå·²åœ¨åŠ è½½æ—¶è¿‡æ»¤ï¼‰
                # ä½†ä¸ºäº†å®‰å…¨ï¼Œä»ç„¶å¤„ç†è¿™ç§æƒ…å†µ
                print(f"âš ï¸  çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_name}")
                sample["response"] = "Knowledge base not found"
                sample["score"] = 0.0
                sample["error"] = f"Knowledge base {kb_name} does not exist"
                sample["kb_name"] = kb_name
                return sample

            print(f"ä½¿ç”¨çŸ¥è¯†åº“: {kb_name}")

            # ç›´æ¥è°ƒç”¨ RAG hybrid æœç´¢
            if self._first_query:
                print(f"â³ é¦–æ¬¡æŸ¥è¯¢éœ€è¦åˆå§‹åŒ–çŸ¥è¯†åº“ï¼Œè¯·ç¨å€™...")
                self._first_query = False
            print(f"ğŸ” å¼€å§‹ RAG æŸ¥è¯¢ (hybrid mode)... (é—®é¢˜: {question[:50]}...)")
            sys.stdout.flush()
            try:
                rag_result = await asyncio.wait_for(
                    rag_search(
                        query=question,
                        kb_name=kb_name,
                        mode="hybrid",
                        kb_base_dir=str(self.kb_base_dir)  # ä¼ é€’çŸ¥è¯†åº“åŸºç¡€ç›®å½•
                    ),
                    timeout=120.0  # 120ç§’è¶…æ—¶
                )
                print(f"âœ… RAG æŸ¥è¯¢å®Œæˆ")
            except asyncio.TimeoutError:
                error_msg = "RAG query timeout (120s)"
                print(f"â±ï¸  {error_msg}")
                sample["response"] = "Timeout"
                sample["pred"] = "Timeout"
                sample["score"] = 0.0
                sample["error"] = error_msg
                sample["kb_name"] = kb_name
                return sample

            # æå–ç­”æ¡ˆ
            generated_answer = self._extract_answer_from_rag(rag_result)

            # ä½¿ç”¨ LLM-as-a-Judge è¯„ä¼°ç­”æ¡ˆ
            try:
                eval_result = await self.llm_evaluator.evaluate_single_answer(
                    question=question,
                    expected_answer=str(sample["answer"]),
                    generated_answer=generated_answer,
                    evidence_pages=str(sample.get("evidence_pages", "")),
                    evidence_sources=str(sample.get("evidence_sources", "")),
                    doc_id=sample["doc_id"],
                    evaluation_type="accuracy_only"
                )
                
                score = float(eval_result.get("accuracy", 0))
                reasoning = eval_result.get("reasoning", "")
                
            except Exception as e:
                print(f"LLMè¯„ä¼°å¤±è´¥: {e}")
                score = 0.0
                reasoning = f"Evaluation error: {str(e)}"

            # æ›´æ–°æ ·æœ¬
            sample["response"] = generated_answer
            sample["score"] = score
            sample["llm_reasoning"] = reasoning
            sample["kb_name"] = kb_name
            sample["rag_sources"] = rag_result.get("sources", [])

            # ç®€æ´è¾“å‡º
            score_icon = "âœ…" if score >= 0.5 else "âŒ"
            print(f"\nğŸ“ é—®é¢˜: {question}")
            print(f"ğŸ’¬ RAGè¾“å‡º: {generated_answer}")
            print(f"å‚è€ƒç­”æ¡ˆ: {sample['answer']}")
            print(f"{score_icon} å¾—åˆ†: {score}")

        except Exception as e:
            sample["response"] = f"Error: {str(e)}"
            sample["score"] = 0.0
            sample["error"] = str(e)
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")

        return sample

    async def run_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "=" * 80)
        print("å¼€å§‹ RAG Direct æµ‹è¯•")
        print("=" * 80)

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.results_file.parent.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ æ€»å…± {len(self.samples)} ä¸ªæ ·æœ¬å¾…æµ‹è¯•")
        print(f"ğŸ’¾ ç»“æœå°†ä¿å­˜åˆ°: {self.results_file}")
        print(f"ğŸ”„ å¼ºåˆ¶é‡æ–°è¿è¡Œ: {self.force_rerun}")
        print(f"ğŸ“‚ çŸ¥è¯†åº“ç›®å½•: {self.kb_base_dir}")
        print("\nå¼€å§‹å¤„ç†æ ·æœ¬...\n")

        # è¿è¡Œæµ‹è¯•
        for i, sample in enumerate(tqdm(self.samples, desc="æµ‹è¯•è¿›åº¦", ncols=80, mininterval=0.1)):
            print(f"\n{'='*60}")
            print(f"[{i+1}/{len(self.samples)}] å¼€å§‹å¤„ç†æ ·æœ¬...")
            sys.stdout.flush()  # å¼ºåˆ¶åˆ·æ–°è¾“å‡ºç¼“å†²åŒº
            
            # è·³è¿‡å·²å®Œæˆçš„æ ·æœ¬ï¼Œé™¤éå¼ºåˆ¶é‡æ–°è¿è¡Œ
            if "score" in sample and not self.force_rerun:
                completed_samples = [s for s in self.samples if "score" in s]
                if completed_samples:
                    total_score = sum(s.get("score", 0) for s in completed_samples)
                    current_acc = total_score / len(completed_samples)
                    if i % 5 == 0 or i == len(self.samples) - 1:
                        print(f"ğŸ“Š ç´¯è®¡å‡†ç¡®ç‡: {current_acc:.2%} ({int(total_score)}/{len(completed_samples)})")
                continue

            try:
                sample = await self.test_sample(sample)
                
                # å®æ—¶ä¿å­˜ç»“æœ
                self._save_results()
                
                # æ˜¾ç¤ºç´¯è®¡å‡†ç¡®ç‡
                completed_samples = [s for s in self.samples if "score" in s]
                if completed_samples:
                    total_score = sum(s.get("score", 0) for s in completed_samples)
                    current_acc = total_score / len(completed_samples)
                    print(f"ğŸ“Š ç´¯è®¡å‡†ç¡®ç‡: {current_acc:.2%} ({int(total_score)}/{len(completed_samples)})")

            except KeyboardInterrupt:
                print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•ï¼Œä¿å­˜å½“å‰ç»“æœ...")
                self._save_results()
                raise
            except Exception as e:
                print(f"æµ‹è¯•æ ·æœ¬æ—¶å‡ºé”™: {e}")
                continue

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_report()

    def _save_results(self):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        with open(self.results_file, "w", encoding="utf-8") as f:
            json.dump(self.samples, f, ensure_ascii=False, indent=2)

    def _generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
        print("=" * 80)

        # ç»Ÿè®¡ç»“æœ
        evaluated_samples = [s for s in self.samples if "score" in s]
        total_samples = len(evaluated_samples)
        
        if total_samples == 0:
            print("æ²¡æœ‰è¯„ä¼°ç»“æœ")
            return

        correct_samples = sum(1 for s in evaluated_samples if s.get("score", 0) >= 0.5)
        accuracy = correct_samples / total_samples if total_samples > 0 else 0

        # æŒ‰æ–‡æ¡£ç±»å‹ç»Ÿè®¡
        doc_type_stats = {}
        for sample in evaluated_samples:
            doc_id = sample.get("doc_id", "unknown")
            kb_name = sample.get("kb_name", "unknown")
            if kb_name not in doc_type_stats:
                doc_type_stats[kb_name] = {"total": 0, "correct": 0}
            doc_type_stats[kb_name]["total"] += 1
            if sample.get("score", 0) >= 0.5:
                doc_type_stats[kb_name]["correct"] += 1

        # æ‰“å°æŠ¥å‘Š
        report_lines = [
            "\n" + "=" * 80,
            f"RAG Direct æµ‹è¯•æŠ¥å‘Š (Hybrid Mode)",
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "=" * 80,
            f"Overall Accuracy: {accuracy:.4f} | Question Number: {total_samples}",
            f"Correct: {correct_samples} | Incorrect: {total_samples - correct_samples}",
            "\næŒ‰çŸ¥è¯†åº“ç»Ÿè®¡:",
        ]

        for kb_name, stats in sorted(doc_type_stats.items()):
            kb_acc = stats["correct"] / stats["total"] if stats["total"] > 0 else 0
            report_lines.append(
                f"  {kb_name}: Accuracy: {kb_acc:.4f} | Questions: {stats['total']}"
            )

        report = "\n".join(report_lines)
        print(report)

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = self.results_file.parent / "report_rag_direct.txt"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report)
        print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="RAG Direct æµ‹è¯•")
    parser.add_argument(
        "--samples",
        type=str,
        default=str(mmlongbench_root / "data" / "samples.json"),
        help="æ ·æœ¬æ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--results",
        type=str,
        default="test_results/results_rag_direct.json",
        help="ç»“æœæ–‡ä»¶è·¯å¾„",
    )
    parser.add_argument(
        "--kb_dir",
        type=str,
        default="../../data/knowledge_bases",
        help="çŸ¥è¯†åº“åŸºç¡€ç›®å½•",
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°",
    )
    parser.add_argument(
        "--start_index",
        type=int,
        default=0,
        help="èµ·å§‹ç´¢å¼•",
    )
    parser.add_argument(
        "--force_rerun",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°è¿è¡Œæ‰€æœ‰æµ‹è¯•",
    )
    parser.add_argument(
        "--include_missing_kb",
        action="store_true",
        help="åŒ…å«æ²¡æœ‰çŸ¥è¯†åº“çš„æ ·æœ¬ï¼ˆé»˜è®¤è·³è¿‡ï¼‰",
    )

    args = parser.parse_args()

    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    kb_dir = Path(args.kb_dir)
    if not kb_dir.is_absolute():
        kb_dir = Path(__file__).parent / kb_dir
    kb_dir = kb_dir.resolve()

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = RAGDirectTester(
        samples_path=args.samples,
        results_file=args.results,
        kb_base_dir=str(kb_dir),
        max_samples=args.max_samples,
        start_index=args.start_index,
        force_rerun=args.force_rerun,
        skip_missing_kb=not args.include_missing_kb,  # é»˜è®¤è·³è¿‡æ²¡æœ‰çŸ¥è¯†åº“çš„æ ·æœ¬
    )

    # è¿è¡Œæµ‹è¯•
    await tester.run_tests()


if __name__ == "__main__":
    asyncio.run(main())

