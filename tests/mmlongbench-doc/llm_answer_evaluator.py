#!/usr/bin/env python
"""
LLMç­”æ¡ˆè¯„ä¼°å™¨

åŠŸèƒ½ç‰¹ç‚¹ï¼š
1. ä½¿ç”¨LLMè¯„ä¼°RAGç³»ç»Ÿçš„ç­”æ¡ˆè´¨é‡
2. å¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®æ€§ã€ç›¸å…³æ€§ã€å®Œæ•´æ€§ã€å¿ å®æ€§ç­‰
3. ç»“æ„åŒ–çš„è¯„ä¼°ç»“æœè¾“å‡º
4. æ‰¹é‡è¯„ä¼°åŠŸèƒ½
"""

import os
import json
import asyncio
import logging
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd
from datetime import datetime

# Add project root directory to Python path
import sys
sys.path.append(str(Path(__file__).parent.parent))

from lightrag.llm.openai import openai_complete_if_cache
from dotenv import load_dotenv

load_dotenv(dotenv_path=".env", override=False)

class LLMAnswerEvaluator:
    def __init__(self, config):
        self.config = config
        self.evaluation_results = []
        self.existing_evaluations = {}  # å­˜å‚¨å·²æœ‰è¯„ä¼°ç»“æœçš„ç´¢å¼•ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾
        self.setup_logging()
        self.load_existing_evaluations()  # åŠ è½½å·²æœ‰è¯„ä¼°ç»“æœ
        
    def setup_logging(self):
        """é…ç½®æ—¥å¿—"""
        log_file = os.path.join(self.config.output_dir, "llm_evaluation.log")
        os.makedirs(self.config.output_dir, exist_ok=True)
        
        # æ ¹æ® quiet é…ç½®å†³å®šæ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°
        handlers = [logging.FileHandler(log_file, encoding='utf-8')]
        if not getattr(self.config, 'quiet', False):
            handlers.append(logging.StreamHandler(sys.stdout))
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=handlers,
            force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
        )
        self.logger = logging.getLogger('llm_evaluator')
        
        # é™é»˜æ¨¡å¼ä¸‹è®¾ç½®æ›´é«˜çš„æ—¥å¿—çº§åˆ«
        if getattr(self.config, 'quiet', False):
            self.logger.setLevel(logging.WARNING)

    def get_accuracy_evaluation_prompt(self, question: str, expected_answer: str, 
                                     generated_answer: str, evidence_pages: str = "", 
                                     evidence_sources: str = "") -> str:
        """ç”Ÿæˆå‡†ç¡®æ€§è¯„ä¼°çš„prompt"""
        prompt = f"""You are an expert evaluator tasked with assessing the accuracy of answers generated by a RAG (Retrieval-Augmented Generation) system.

**Task**: Evaluate whether the generated answer correctly responds to the given question based on the expected answer.

**Question**: {question}

**Expected Answer**: {expected_answer}

**Generated Answer**: {generated_answer}

**Evidence Pages**: {evidence_pages}

**Evidence Sources**: {evidence_sources}

**Evaluation Criteria**:
1. **Accuracy (0 or 1)**: Does the generated answer match the factual content of the expected answer?
   - 1: The generated answer is factually correct and aligns with the expected answer
   - 0: The generated answer is factually incorrect or contradicts the expected answer

**Instructions**:
- Focus on factual correctness, not writing style or format
- Consider partial matches: if the generated answer contains the correct information but includes additional context, it should still be considered accurate
- For numerical answers, check if the values match or are equivalent
- For list answers, check if all key elements are present
- If the expected answer is "Not answerable" and the generated answer indicates inability to answer, consider it accurate

**Output Format**:
Please respond with a JSON object containing only:
{{
    "accuracy": 0 or 1,
    "reasoning": "Brief explanation of your evaluation"
}}
"""
        return prompt

    def get_comprehensive_evaluation_prompt(self, question: str, expected_answer: str,
                                          generated_answer: str, evidence_pages: str = "",
                                          evidence_sources: str = "") -> str:
        """ç”Ÿæˆç»¼åˆè¯„ä¼°çš„prompt"""
        prompt = f"""You are an expert evaluator assessing the quality of answers generated by a RAG system across multiple dimensions.

**Question**: {question}

**Expected Answer**: {expected_answer}

**Generated Answer**: {generated_answer}

**Evidence Pages**: {evidence_pages}

**Evidence Sources**: {evidence_sources}

**Evaluation Dimensions** (Rate each from 0 to 1):

1. **Accuracy (0 or 1)**: Factual correctness compared to expected answer
   - 1: Factually correct and aligns with expected answer
   - 0: Factually incorrect or contradicts expected answer

2. **Relevance (0 to 1)**: How well the answer addresses the question
   - 1.0: Directly and completely addresses the question
   - 0.5: Partially addresses the question
   - 0.0: Does not address the question

3. **Completeness (0 to 1)**: How comprehensive the answer is
   - 1.0: Provides complete information as expected
   - 0.5: Provides partial but useful information
   - 0.0: Incomplete or missing key information

4. **Faithfulness (0 to 1)**: Whether the answer is grounded in evidence
   - 1.0: Answer is fully supported by evidence
   - 0.5: Answer is partially supported by evidence
   - 0.0: Answer contains unsupported claims

5. **Clarity (0 to 1)**: How clear and understandable the answer is
   - 1.0: Very clear and well-structured
   - 0.5: Reasonably clear with minor issues
   - 0.0: Unclear or confusing

6. **Consistency (0 to 1)**: Internal consistency of the answer
   - 1.0: Completely consistent, no contradictions
   - 0.5: Mostly consistent with minor issues
   - 0.0: Contains contradictions or inconsistencies

**Special Cases**:
- If expected answer is "Not answerable" and generated answer appropriately indicates inability to answer, rate accuracy as 1
- Consider format requirements (e.g., if expected format is "List" but answer is string, note in reasoning)

**Output Format**:
Respond with a JSON object:
{{
    "accuracy": 0 or 1,
    "relevance": 0.0 to 1.0,
    "completeness": 0.0 to 1.0,
    "faithfulness": 0.0 to 1.0,
    "clarity": 0.0 to 1.0,
    "consistency": 0.0 to 1.0,
    "overall_score": 0.0 to 1.0,
    "reasoning": "Brief explanation of your evaluation for each dimension"
}}
"""
        return prompt

    def fix_json_format(self, json_content: str) -> str:
        """ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜"""
        import re
        
        try:
            # ç§»é™¤å¯èƒ½çš„Markdownä»£ç å—æ ‡è®°
            json_content = re.sub(r'```json\s*\n?', '', json_content)
            json_content = re.sub(r'\n?```', '', json_content)
            json_content = json_content.strip()
            
            # å¤„ç†é‡å¤çš„reasoningå­—æ®µé—®é¢˜ - ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•
            # æ‰¾åˆ°æ‰€æœ‰reasoningå­—æ®µçš„èµ·å§‹ä½ç½®
            reasoning_pattern = r'"reasoning":\s*"'
            matches = list(re.finditer(reasoning_pattern, json_content))
            
            if len(matches) > 1:
                # å¦‚æœæœ‰å¤šä¸ªreasoningå­—æ®µï¼Œä¿ç•™ç¬¬ä¸€ä¸ªå®Œæ•´çš„
                first_match_start = matches[0].start()
                
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªreasoningå­—æ®µçš„ç»“æŸä½ç½®
                quote_start = matches[0].end() - 1  # æŒ‡å‘å¼€å§‹çš„å¼•å·
                quote_count = 0
                end_pos = quote_start
                
                for i, char in enumerate(json_content[quote_start:], quote_start):
                    if char == '"' and (i == quote_start or json_content[i-1] != '\\'):
                        quote_count += 1
                        if quote_count == 2:  # æ‰¾åˆ°ç»“æŸå¼•å·
                            end_pos = i + 1
                            break
                
                # ä¿ç•™ç¬¬ä¸€ä¸ªreasoningå­—æ®µï¼Œç§»é™¤åç»­çš„
                before_first = json_content[:first_match_start]
                first_reasoning = json_content[first_match_start:end_pos]
                
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªreasoningå­—æ®µåçš„ä¸‹ä¸€ä¸ªå­—æ®µæˆ–ç»“æŸ
                after_first = json_content[end_pos:]
                # ç§»é™¤åç»­çš„reasoningå­—æ®µ
                after_first = re.sub(r',?\s*"reasoning":\s*"[^"]*(?:\\"[^"]*)*"', '', after_first)
                
                json_content = before_first + first_reasoning + after_first
            
            # ç¡®ä¿JSONå¯¹è±¡æ­£ç¡®é—­åˆ
            if json_content and not json_content.endswith('}'):
                # è®¡ç®—å¤§æ‹¬å·å¹³è¡¡
                open_braces = json_content.count('{')
                close_braces = json_content.count('}')
                if open_braces > close_braces:
                    # å…ˆç¡®ä¿æœ€åä¸€ä¸ªå­—æ®µæ­£ç¡®ç»“æŸ
                    if not json_content.rstrip().endswith(('"', '}', ']')):
                        # å¯èƒ½æœ‰æˆªæ–­çš„å­—ç¬¦ä¸²ï¼Œå°è¯•é—­åˆ
                        if '"' in json_content and json_content.count('"') % 2 == 1:
                            json_content = json_content.rstrip() + '"'
                    # æ·»åŠ ç¼ºå¤±çš„å¤§æ‹¬å·
                    json_content += '}' * (open_braces - close_braces)
            
            # ç§»é™¤æ§åˆ¶å­—ç¬¦
            json_content = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', json_content)
            
            return json_content.strip()
            
        except Exception as e:
            self.logger.warning(f"ä¿®å¤JSONæ ¼å¼æ—¶å‡ºé”™: {e}")
            return json_content
    
    def create_fallback_evaluation(self, response: str) -> Dict[str, Any]:
        """å½“JSONè§£æå¤±è´¥æ—¶åˆ›å»ºåŸºæœ¬çš„è¯„ä¼°ç»“æœ"""
        import re
        
        # å°è¯•ä»å“åº”ä¸­æå–å…³é”®ä¿¡æ¯
        fallback_result = {
            "accuracy": None,
            "relevance": None,
            "completeness": None,
            "faithfulness": None,
            "clarity": None,
            "consistency": None,
            "overall_score": None,
            "reasoning": "JSONè§£æå¤±è´¥ï¼Œæ— æ³•è·å–è¯¦ç»†è¯„ä¼°"
        }
        
        # å°è¯•æå–å‡†ç¡®æ€§å€¼
        accuracy_patterns = [
            r'"accuracy":\s*(\d+)',
            r'accuracy.*?(\d+)',
            r'å‡†ç¡®.*?(\d+)',
            r'correct.*?(\d+)',
            r'accurate.*?(\d+)'
        ]
        
        for pattern in accuracy_patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                try:
                    accuracy_value = int(match.group(1))
                    if accuracy_value in [0, 1]:
                        fallback_result["accuracy"] = accuracy_value
                        break
                except ValueError:
                    continue
        
        # å¦‚æœæ‰¾ä¸åˆ°å‡†ç¡®æ€§å€¼ï¼Œå°è¯•æ ¹æ®å…³é”®è¯åˆ¤æ–­
        if fallback_result["accuracy"] is None:
            positive_keywords = ["correct", "accurate", "right", "yes", "true", "match"]
            negative_keywords = ["incorrect", "wrong", "false", "no", "mismatch", "contradiction"]
            
            response_lower = response.lower()
            if any(keyword in response_lower for keyword in positive_keywords):
                fallback_result["accuracy"] = 1
            elif any(keyword in response_lower for keyword in negative_keywords):
                fallback_result["accuracy"] = 0
        
        # è®¾ç½®é»˜è®¤å€¼ï¼ˆå¦‚æœä»ç„¶ä¸ºNoneï¼‰
        if fallback_result["accuracy"] is None:
            fallback_result["accuracy"] = 0  # ä¿å®ˆä¼°è®¡
        
        # å¯¹äºç»¼åˆè¯„ä¼°ï¼Œå°è¯•æå–å…¶ä»–æŒ‡æ ‡
        if "relevance" in response.lower():
            relevance_match = re.search(r'"relevance":\s*([\d.]+)', response, re.IGNORECASE)
            if relevance_match:
                try:
                    fallback_result["relevance"] = float(relevance_match.group(1))
                except ValueError:
                    pass
        
        self.logger.info(f"åˆ›å»ºfallbackè¯„ä¼°ç»“æœ: accuracy={fallback_result['accuracy']}")
        return fallback_result

    async def evaluate_single_answer(self, question: str, expected_answer: str,
                                   generated_answer: str, evidence_pages: str = "",
                                   evidence_sources: str = "", doc_id: str = "",
                                   evaluation_type: str = "comprehensive") -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªç­”æ¡ˆ"""
        start_time = time.time()
        
        try:
            if evaluation_type == "accuracy_only":
                prompt = self.get_accuracy_evaluation_prompt(
                    question, expected_answer, generated_answer, evidence_pages, evidence_sources
                )
            else:
                prompt = self.get_comprehensive_evaluation_prompt(
                    question, expected_answer, generated_answer, evidence_pages, evidence_sources
                )
            
            self.logger.info(f"å¼€å§‹è¯„ä¼°ç­”æ¡ˆ (ç±»å‹: {evaluation_type})")
            self.logger.info(f"é—®é¢˜: {question[:100]}...")
            
            # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
            eval_model = getattr(self.config, 'model', 'gpt-4o')  # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
            response = await openai_complete_if_cache(
                model=eval_model,
                prompt=prompt,
                system_prompt="You are an expert evaluator. Provide accurate and fair evaluations based on the given criteria.",
                api_key=self.config.api_key,
                base_url=self.config.base_url,
                temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿è¯„ä¼°ä¸€è‡´æ€§
                max_tokens=1000
            )
            
            evaluation_time = time.time() - start_time
            
            # è§£æJSONå“åº”
            try:
                evaluation_result = json.loads(response)
            except json.JSONDecodeError:
                import re
                
                # é¦–å…ˆå°è¯•æå–è¢«```json```åŒ…å›´çš„å†…å®¹
                json_code_block_match = re.search(r'```json\s*\n(.*?)\n```', response, re.DOTALL)
                if json_code_block_match:
                    json_content = json_code_block_match.group(1).strip()
                    try:
                        evaluation_result = json.loads(json_content)
                    except json.JSONDecodeError as e:
                        self.logger.warning(f"ä»£ç å—ä¸­çš„JSONè§£æå¤±è´¥: {e}")
                        # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜ï¼ˆå¦‚é‡å¤çš„reasoningå­—æ®µï¼‰
                        json_content = self.fix_json_format(json_content)
                        try:
                            evaluation_result = json.loads(json_content)
                        except json.JSONDecodeError:
                            # æœ€åçš„fallbackï¼šåˆ›å»ºåŸºæœ¬çš„è¯„ä¼°ç»“æœ
                            self.logger.warning("JSONä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„ä¼°ç»“æœ")
                            evaluation_result = self.create_fallback_evaluation(response)
                else:
                    # å°è¯•æå–ä»»ä½•JSONå¯¹è±¡
                    json_match = re.search(r'\{.*\}', response, re.DOTALL)
                    if json_match:
                        json_content = json_match.group().strip()
                        try:
                            evaluation_result = json.loads(json_content)
                        except json.JSONDecodeError:
                            json_content = self.fix_json_format(json_content)
                            try:
                                evaluation_result = json.loads(json_content)
                            except json.JSONDecodeError:
                                # æœ€åçš„fallbackï¼šåˆ›å»ºåŸºæœ¬çš„è¯„ä¼°ç»“æœ
                                self.logger.warning("JSONä¿®å¤å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„ä¼°ç»“æœ")
                                evaluation_result = self.create_fallback_evaluation(response)
                    else:
                        raise ValueError("æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„JSON")
            
            # æ·»åŠ å…ƒæ•°æ®
            evaluation_result.update({
                "doc_id": doc_id,
                "question": question,
                "expected_answer": expected_answer,
                "generated_answer": generated_answer,
                "evidence_pages": evidence_pages,
                "evidence_sources": evidence_sources,
                "evaluation_type": evaluation_type,
                "evaluation_time": evaluation_time,
                "timestamp": datetime.now().isoformat(),
                "success": True,
                "error": None
            })
            
            self.logger.info(f"è¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {evaluation_time:.2f}ç§’")
            return evaluation_result
            
        except Exception as e:
            evaluation_time = time.time() - start_time
            error_msg = f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
            self.logger.error(error_msg)
            
            return {
                "doc_id": doc_id,
                "question": question,
                "expected_answer": expected_answer,
                "generated_answer": generated_answer,
                "evidence_pages": evidence_pages,
                "evidence_sources": evidence_sources,
                "evaluation_type": evaluation_type,
                "evaluation_time": evaluation_time,
                "timestamp": datetime.now().isoformat(),
                "success": False,
                "error": error_msg,
                "accuracy": None,
                "reasoning": None
            }

    async def evaluate_rag_results(self, results_file: str, evaluation_type: str = "comprehensive",
                                 max_evaluations: int = None) -> None:
        """è¯„ä¼°RAGç»“æœæ–‡ä»¶"""
        self.logger.info(f"å¼€å§‹è¯„ä¼°RAGç»“æœæ–‡ä»¶: {results_file}")
        
        # åŠ è½½RAGè¯„ä¼°ç»“æœ
        with open(results_file, 'r', encoding='utf-8') as f:
            rag_results = json.load(f)
        
        if max_evaluations:
            rag_results = rag_results[:max_evaluations]
            self.logger.info(f"é™åˆ¶è¯„ä¼°æ•°é‡ä¸º: {max_evaluations}")
        
        total_items = len(rag_results)
        self.logger.info(f"å¼€å§‹è¯„ä¼° {total_items} ä¸ªç»“æœ")
        
        # æ£€æµ‹ç»“æœç±»å‹
        if rag_results and "vlm_result" in rag_results[0]:
            self.logger.info("æ£€æµ‹åˆ°VLMç»“æœæ ¼å¼")
            await self.evaluate_vlm_results(rag_results, evaluation_type)
        else:
            self.logger.info("æ£€æµ‹åˆ°RAGç»“æœæ ¼å¼")
            await self.evaluate_standard_rag_results(rag_results, evaluation_type)

    async def evaluate_multiple_results(self, results_files: List[str], evaluation_type: str = "comprehensive",
                                      max_evaluations: int = None) -> None:
        """è¯„ä¼°å¤šä¸ªç»“æœæ–‡ä»¶ï¼ˆRAGå’ŒVLMï¼‰"""
        self.logger.info(f"å¼€å§‹è¯„ä¼°å¤šä¸ªç»“æœæ–‡ä»¶: {results_files}")
        
        start_time = time.time()
        all_results_by_type = {}
        
        # æŒ‰æ–‡ä»¶ç±»å‹åˆ†åˆ«åŠ è½½ç»“æœ
        for results_file in results_files:
            self.logger.info(f"åŠ è½½ç»“æœæ–‡ä»¶: {results_file}")
            
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            # æ£€æµ‹æ–‡ä»¶ç±»å‹å¹¶æ ‡è®°
            if results and "vlm_result" in results[0]:
                file_type = "vlm"
                self.logger.info(f"æ£€æµ‹åˆ°VLMç»“æœæ ¼å¼: {len(results)} ä¸ªç»“æœ")
            else:
                file_type = "rag"
                self.logger.info(f"æ£€æµ‹åˆ°RAGç»“æœæ ¼å¼: {len(results)} ä¸ªç»“æœ")
            
            # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ æ–‡ä»¶ç±»å‹æ ‡è®°
            for result in results:
                result["_file_type"] = file_type
                result["_source_file"] = results_file
            
            all_results_by_type[file_type] = {
                "results": results,
                "source_file": results_file
            }
        
        # å¦‚æœæœ‰é™åˆ¶ï¼ŒæŒ‰é—®é¢˜æ•°é‡é™åˆ¶è€Œä¸æ˜¯ç»“æœæ•°é‡
        if max_evaluations:
            for file_type in all_results_by_type:
                original_count = len(all_results_by_type[file_type]["results"])
                all_results_by_type[file_type]["results"] = all_results_by_type[file_type]["results"][:max_evaluations]
                limited_count = len(all_results_by_type[file_type]["results"])
                self.logger.info(f"é™åˆ¶{file_type}ç»“æœæ•°é‡: {original_count} -> {limited_count}")
        
        # åˆå¹¶æ‰€æœ‰ç»“æœè¿›è¡Œè¯„ä¼°
        all_results = []
        for file_type, data in all_results_by_type.items():
            all_results.extend(data["results"])
        
        total_items = len(all_results)
        skipped_count = 0
        completed_count = 0
        new_evaluations = []
        
        # ç»Ÿè®¡å·²æœ‰è¯„ä¼°
        if self.existing_evaluations:
            existing_count = len(self.existing_evaluations)
            self.logger.info(f"å‘ç° {existing_count} ä¸ªå·²æœ‰çš„LLMè¯„ä¼°ç»“æœï¼Œå°†è·³è¿‡é‡å¤è¯„ä¼°")
        
        self.logger.info(f"å¼€å§‹è¯„ä¼°æ€»è®¡ {total_items} ä¸ªç»“æœ")
        
        # è¯„ä¼°æ‰€æœ‰ç»“æœ
        for i, result in enumerate(all_results, 1):
            self.logger.info(f"\nğŸ“‹ [{i}/{total_items}] è¯„ä¼°{result['_file_type'].upper()}ç»“æœ")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ç±»å‹çš„errorï¼Œå¦‚æœæœ‰åˆ™è·³è¿‡
            if self.has_any_error(result):
                skipped_count += 1
                self.logger.info(f"  â­ï¸ è·³è¿‡æœ‰é”™è¯¯çš„æŸ¥è¯¢: {self.get_error_summary(result)}")
                continue
            
            if result["_file_type"] == "vlm":
                # è¯„ä¼°VLMç»“æœ
                if result.get("vlm_result") and result["vlm_result"].get("success"):
                    method = "vlm"
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè¯„ä¼°
                    if self.is_evaluation_completed(result["doc_id"], result["question"], method):
                        completed_count += 1
                        self.logger.info(f"  âœ… è·³è¿‡å·²å®Œæˆçš„{method}è¯„ä¼°")
                        # å°†å·²æœ‰ç»“æœæ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                        existing_eval = self.get_existing_evaluation(result["doc_id"], result["question"], method)
                        if existing_eval:
                            existing_eval["source_file"] = result["_source_file"]  # æ›´æ–°æºæ–‡ä»¶ä¿¡æ¯
                            new_evaluations.append(existing_eval)
                        continue
                    
                    eval_result = await self.evaluate_single_answer(
                        question=result["question"],
                        expected_answer=str(result["expected_answer"]),
                        generated_answer=str(result["vlm_result"]["result"]),
                        evidence_pages=result.get("evidence_pages", ""),
                        evidence_sources=result.get("evidence_sources", ""),
                        doc_id=result["doc_id"],
                        evaluation_type=evaluation_type
                    )
                    eval_result["method"] = method
                    eval_result["model"] = result["vlm_result"].get("model", "unknown")
                    eval_result["source_file"] = result["_source_file"]
                    
                    new_evaluations.append(eval_result)
                    
                    # æ›´æ–°ç´¢å¼•
                    key = (result["doc_id"], result["question"], method)
                    self.existing_evaluations[key] = eval_result
                    
            else:  # RAGç»“æœ
                # è¯„ä¼°hybridç»“æœ
                if result.get("hybrid_result") and result["hybrid_result"].get("success"):
                    method = "hybrid"
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè¯„ä¼°
                    if self.is_evaluation_completed(result["doc_id"], result["question"], method):
                        completed_count += 1
                        self.logger.info(f"  âœ… è·³è¿‡å·²å®Œæˆçš„{method}è¯„ä¼°")
                        # å°†å·²æœ‰ç»“æœæ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                        existing_eval = self.get_existing_evaluation(result["doc_id"], result["question"], method)
                        if existing_eval:
                            existing_eval["source_file"] = result["_source_file"]
                            new_evaluations.append(existing_eval)
                    else:
                        hybrid_eval = await self.evaluate_single_answer(
                            question=result["question"],
                            expected_answer=str(result["expected_answer"]),
                            generated_answer=str(result["hybrid_result"]["result"]),
                            evidence_pages=result.get("evidence_pages", ""),
                            evidence_sources=result.get("evidence_sources", ""),
                            doc_id=result["doc_id"],
                            evaluation_type=evaluation_type
                        )
                        hybrid_eval["method"] = method
                        hybrid_eval["source_file"] = result["_source_file"]
                        
                        new_evaluations.append(hybrid_eval)
                        
                        # æ›´æ–°ç´¢å¼•
                        key = (result["doc_id"], result["question"], method)
                        self.existing_evaluations[key] = hybrid_eval
                
                # è¯„ä¼°mixç»“æœ
                if result.get("mix_result") and result["mix_result"].get("success"):
                    method = "mix"
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè¯„ä¼°
                    if self.is_evaluation_completed(result["doc_id"], result["question"], method):
                        completed_count += 1
                        self.logger.info(f"  âœ… è·³è¿‡å·²å®Œæˆçš„{method}è¯„ä¼°")
                        # å°†å·²æœ‰ç»“æœæ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                        existing_eval = self.get_existing_evaluation(result["doc_id"], result["question"], method)
                        if existing_eval:
                            existing_eval["source_file"] = result["_source_file"]
                            new_evaluations.append(existing_eval)
                    else:
                        mix_eval = await self.evaluate_single_answer(
                            question=result["question"],
                            expected_answer=str(result["expected_answer"]),
                            generated_answer=str(result["mix_result"]["result"]),
                            evidence_pages=result.get("evidence_pages", ""),
                            evidence_sources=result.get("evidence_sources", ""),
                            doc_id=result["doc_id"],
                            evaluation_type=evaluation_type
                        )
                        mix_eval["method"] = method
                        mix_eval["source_file"] = result["_source_file"]
                        
                        new_evaluations.append(mix_eval)
                        
                        # æ›´æ–°ç´¢å¼•
                        key = (result["doc_id"], result["question"], method)
                        self.existing_evaluations[key] = mix_eval
                
                # è¯„ä¼°naiveç»“æœ
                if result.get("naive_result") and result["naive_result"].get("success"):
                    method = "naive"
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè¯„ä¼°
                    if self.is_evaluation_completed(result["doc_id"], result["question"], method):
                        completed_count += 1
                        self.logger.info(f"  âœ… è·³è¿‡å·²å®Œæˆçš„{method}è¯„ä¼°")
                        # å°†å·²æœ‰ç»“æœæ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                        existing_eval = self.get_existing_evaluation(result["doc_id"], result["question"], method)
                        if existing_eval:
                            existing_eval["source_file"] = result["_source_file"]
                            new_evaluations.append(existing_eval)
                    else:
                        naive_eval = await self.evaluate_single_answer(
                            question=result["question"],
                            expected_answer=str(result["expected_answer"]),
                            generated_answer=str(result["naive_result"]["result"]),
                            evidence_pages=result.get("evidence_pages", ""),
                            evidence_sources=result.get("evidence_sources", ""),
                            doc_id=result["doc_id"],
                            evaluation_type=evaluation_type
                        )
                        naive_eval["method"] = method
                        naive_eval["source_file"] = result["_source_file"]
                        
                        new_evaluations.append(naive_eval)
                        
                        # æ›´æ–°ç´¢å¼•
                        key = (result["doc_id"], result["question"], method)
                        self.existing_evaluations[key] = naive_eval
            
            # æ¯10ä¸ªç»“æœä¿å­˜ä¸€æ¬¡
            if i % 10 == 0:
                # æ›´æ–°evaluation_resultså¹¶ä¿å­˜
                self.update_evaluation_results_list(new_evaluations)
                self.save_evaluation_results()
                self.logger.info(f"å·²ä¿å­˜å‰ {len(self.evaluation_results)} ä¸ªè¯„ä¼°ç»“æœ")
                new_evaluations = []  # æ¸…ç©ºä¸´æ—¶åˆ—è¡¨
        
        total_time = time.time() - start_time
        
        # æ›´æ–°å¹¶ä¿å­˜æœ€ç»ˆç»“æœ
        if new_evaluations:
            self.update_evaluation_results_list(new_evaluations)
        self.save_evaluation_results()
        
        # è®°å½•ç»Ÿè®¡
        new_eval_count = len(self.evaluation_results) - len(self.existing_evaluations) + len(new_evaluations)
        if skipped_count > 0:
            self.logger.info(f"â­ï¸ è·³è¿‡äº† {skipped_count} ä¸ªæœ‰é”™è¯¯çš„æŸ¥è¯¢")
        if completed_count > 0:
            self.logger.info(f"âœ… è·³è¿‡äº† {completed_count} ä¸ªå·²å®Œæˆçš„è¯„ä¼°")
        self.logger.info(f"ğŸ†• æ–°å®Œæˆäº† {new_eval_count} ä¸ªè¯„ä¼°")
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        self.generate_evaluation_report(total_time)

    async def evaluate_vlm_results(self, vlm_results: List[Dict], evaluation_type: str):
        """è¯„ä¼°VLMç»“æœ"""
        start_time = time.time()
        skipped_count = 0
        completed_count = 0
        new_evaluations = []
        
        # ç»Ÿè®¡å·²æœ‰è¯„ä¼°
        if self.existing_evaluations:
            existing_count = len(self.existing_evaluations)
            self.logger.info(f"å‘ç° {existing_count} ä¸ªå·²æœ‰çš„LLMè¯„ä¼°ç»“æœï¼Œå°†è·³è¿‡é‡å¤è¯„ä¼°")
        
        for i, vlm_result in enumerate(vlm_results, 1):
            self.logger.info(f"\nğŸ“‹ [{i}/{len(vlm_results)}] è¯„ä¼°VLMç»“æœ")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ç±»å‹çš„errorï¼Œå¦‚æœæœ‰åˆ™è·³è¿‡
            if self.has_any_error(vlm_result):
                skipped_count += 1
                self.logger.info(f"  â­ï¸ è·³è¿‡æœ‰é”™è¯¯çš„VLMæŸ¥è¯¢: {self.get_error_summary(vlm_result)}")
                continue
            
            # è¯„ä¼°VLMç»“æœ
            if vlm_result.get("vlm_result") and vlm_result["vlm_result"].get("success"):
                method = "vlm"
                
                # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè¯„ä¼°
                if self.is_evaluation_completed(vlm_result["doc_id"], vlm_result["question"], method):
                    completed_count += 1
                    self.logger.info(f"  âœ… è·³è¿‡å·²å®Œæˆçš„{method}è¯„ä¼°")
                    # å°†å·²æœ‰ç»“æœæ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                    existing_eval = self.get_existing_evaluation(vlm_result["doc_id"], vlm_result["question"], method)
                    if existing_eval:
                        new_evaluations.append(existing_eval)
                    continue
                
                vlm_eval = await self.evaluate_single_answer(
                    question=vlm_result["question"],
                    expected_answer=str(vlm_result["expected_answer"]),
                    generated_answer=str(vlm_result["vlm_result"]["result"]),
                    evidence_pages=vlm_result.get("evidence_pages", ""),
                    evidence_sources=vlm_result.get("evidence_sources", ""),
                    doc_id=vlm_result["doc_id"],
                    evaluation_type=evaluation_type
                )
                vlm_eval["method"] = method
                vlm_eval["model"] = vlm_result["vlm_result"].get("model", "unknown")
                
                new_evaluations.append(vlm_eval)
                
                # æ›´æ–°ç´¢å¼•
                key = (vlm_result["doc_id"], vlm_result["question"], method)
                self.existing_evaluations[key] = vlm_eval
            
            # æ¯10ä¸ªç»“æœä¿å­˜ä¸€æ¬¡
            if i % 10 == 0:
                # æ›´æ–°evaluation_resultså¹¶ä¿å­˜
                self.update_evaluation_results_list(new_evaluations)
                self.save_evaluation_results()
                self.logger.info(f"å·²ä¿å­˜å‰ {len(self.evaluation_results)} ä¸ªè¯„ä¼°ç»“æœ")
                new_evaluations = []  # æ¸…ç©ºä¸´æ—¶åˆ—è¡¨
        
        total_time = time.time() - start_time
        
        # æ›´æ–°å¹¶ä¿å­˜æœ€ç»ˆç»“æœ
        if new_evaluations:
            self.update_evaluation_results_list(new_evaluations)
        self.save_evaluation_results()
        
        # è®°å½•ç»Ÿè®¡
        new_eval_count = len(new_evaluations) if new_evaluations else 0
        if skipped_count > 0:
            self.logger.info(f"â­ï¸ è·³è¿‡äº† {skipped_count} ä¸ªæœ‰é”™è¯¯çš„VLMæŸ¥è¯¢")
        if completed_count > 0:
            self.logger.info(f"âœ… è·³è¿‡äº† {completed_count} ä¸ªå·²å®Œæˆçš„è¯„ä¼°")
        self.logger.info(f"ğŸ†• æ–°å®Œæˆäº† {new_eval_count} ä¸ªè¯„ä¼°")
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        self.generate_evaluation_report(total_time)

    async def evaluate_standard_rag_results(self, rag_results: List[Dict], evaluation_type: str):
        """è¯„ä¼°æ ‡å‡†RAGç»“æœï¼ˆhybridã€mixå’Œnaiveï¼‰"""
        start_time = time.time()
        skipped_count = 0
        completed_count = 0
        new_evaluations = []
        
        # ç»Ÿè®¡å·²æœ‰è¯„ä¼°
        if self.existing_evaluations:
            existing_count = len(self.existing_evaluations)
            self.logger.info(f"å‘ç° {existing_count} ä¸ªå·²æœ‰çš„LLMè¯„ä¼°ç»“æœï¼Œå°†è·³è¿‡é‡å¤è¯„ä¼°")
        
        for i, rag_result in enumerate(rag_results, 1):
            self.logger.info(f"\nğŸ“‹ [{i}/{len(rag_results)}] è¯„ä¼°RAGç»“æœ")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ç±»å‹çš„errorï¼Œå¦‚æœæœ‰åˆ™è·³è¿‡
            if self.has_any_error(rag_result):
                skipped_count += 1
                self.logger.info(f"  â­ï¸ è·³è¿‡æœ‰é”™è¯¯çš„RAGæŸ¥è¯¢: {self.get_error_summary(rag_result)}")
                continue
            
            # è¯„ä¼°hybridç»“æœ
            if rag_result.get("hybrid_result") and rag_result["hybrid_result"].get("success"):
                method = "hybrid"
                
                # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè¯„ä¼°
                if self.is_evaluation_completed(rag_result["doc_id"], rag_result["question"], method):
                    completed_count += 1
                    self.logger.info(f"  âœ… è·³è¿‡å·²å®Œæˆçš„{method}è¯„ä¼°")
                    # å°†å·²æœ‰ç»“æœæ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                    existing_eval = self.get_existing_evaluation(rag_result["doc_id"], rag_result["question"], method)
                    if existing_eval:
                        new_evaluations.append(existing_eval)
                else:
                    hybrid_eval = await self.evaluate_single_answer(
                        question=rag_result["question"],
                        expected_answer=str(rag_result["expected_answer"]),
                        generated_answer=str(rag_result["hybrid_result"]["result"]),
                        evidence_pages=rag_result.get("evidence_pages", ""),
                        evidence_sources=rag_result.get("evidence_sources", ""),
                        doc_id=rag_result["doc_id"],
                        evaluation_type=evaluation_type
                    )
                    hybrid_eval["method"] = method
                    
                    new_evaluations.append(hybrid_eval)
                    
                    # æ›´æ–°ç´¢å¼•
                    key = (rag_result["doc_id"], rag_result["question"], method)
                    self.existing_evaluations[key] = hybrid_eval
            
            # è¯„ä¼°mixç»“æœ
            if rag_result.get("mix_result") and rag_result["mix_result"].get("success"):
                method = "mix"
                
                # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè¯„ä¼°
                if self.is_evaluation_completed(rag_result["doc_id"], rag_result["question"], method):
                    completed_count += 1
                    self.logger.info(f"  âœ… è·³è¿‡å·²å®Œæˆçš„{method}è¯„ä¼°")
                    # å°†å·²æœ‰ç»“æœæ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                    existing_eval = self.get_existing_evaluation(rag_result["doc_id"], rag_result["question"], method)
                    if existing_eval:
                        new_evaluations.append(existing_eval)
                else:
                    mix_eval = await self.evaluate_single_answer(
                        question=rag_result["question"],
                        expected_answer=str(rag_result["expected_answer"]),
                        generated_answer=str(rag_result["mix_result"]["result"]),
                        evidence_pages=rag_result.get("evidence_pages", ""),
                        evidence_sources=rag_result.get("evidence_sources", ""),
                        doc_id=rag_result["doc_id"],
                        evaluation_type=evaluation_type
                    )
                    mix_eval["method"] = method
                    
                    new_evaluations.append(mix_eval)
                    
                    # æ›´æ–°ç´¢å¼•
                    key = (rag_result["doc_id"], rag_result["question"], method)
                    self.existing_evaluations[key] = mix_eval
            
            # è¯„ä¼°naiveç»“æœ
            if rag_result.get("naive_result") and rag_result["naive_result"].get("success"):
                method = "naive"
                
                # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè¯„ä¼°
                if self.is_evaluation_completed(rag_result["doc_id"], rag_result["question"], method):
                    completed_count += 1
                    self.logger.info(f"  âœ… è·³è¿‡å·²å®Œæˆçš„{method}è¯„ä¼°")
                    # å°†å·²æœ‰ç»“æœæ·»åŠ åˆ°å½“å‰æ‰¹æ¬¡
                    existing_eval = self.get_existing_evaluation(rag_result["doc_id"], rag_result["question"], method)
                    if existing_eval:
                        new_evaluations.append(existing_eval)
                else:
                    naive_eval = await self.evaluate_single_answer(
                        question=rag_result["question"],
                        expected_answer=str(rag_result["expected_answer"]),
                        generated_answer=str(rag_result["naive_result"]["result"]),
                        evidence_pages=rag_result.get("evidence_pages", ""),
                        evidence_sources=rag_result.get("evidence_sources", ""),
                        doc_id=rag_result["doc_id"],
                        evaluation_type=evaluation_type
                    )
                    naive_eval["method"] = method
                    
                    new_evaluations.append(naive_eval)
                    
                    # æ›´æ–°ç´¢å¼•
                    key = (rag_result["doc_id"], rag_result["question"], method)
                    self.existing_evaluations[key] = naive_eval
            
            # æ¯10ä¸ªç»“æœä¿å­˜ä¸€æ¬¡
            if i % 10 == 0:
                # æ›´æ–°evaluation_resultså¹¶ä¿å­˜
                self.update_evaluation_results_list(new_evaluations)
                self.save_evaluation_results()
                self.logger.info(f"å·²ä¿å­˜å‰ {len(self.evaluation_results)} ä¸ªè¯„ä¼°ç»“æœ")
                new_evaluations = []  # æ¸…ç©ºä¸´æ—¶åˆ—è¡¨
        
        total_time = time.time() - start_time
        
        # æ›´æ–°å¹¶ä¿å­˜æœ€ç»ˆç»“æœ
        if new_evaluations:
            self.update_evaluation_results_list(new_evaluations)
        self.save_evaluation_results()
        
        # è®°å½•ç»Ÿè®¡
        new_eval_count = len(new_evaluations) if new_evaluations else 0
        if skipped_count > 0:
            self.logger.info(f"â­ï¸ è·³è¿‡äº† {skipped_count} ä¸ªæœ‰é”™è¯¯çš„RAGæŸ¥è¯¢")
        if completed_count > 0:
            self.logger.info(f"âœ… è·³è¿‡äº† {completed_count} ä¸ªå·²å®Œæˆçš„è¯„ä¼°")
        self.logger.info(f"ğŸ†• æ–°å®Œæˆäº† {new_eval_count} ä¸ªè¯„ä¼°")
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        self.generate_evaluation_report(total_time)

    def save_evaluation_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        # ä¿å­˜è¯¦ç»†çš„JSONç»“æœ
        results_file = os.path.join(self.config.output_dir, "llm_evaluation_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.evaluation_results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜CSVæ‘˜è¦
        summary_data = []
        for result in self.evaluation_results:
            summary_data.append({
                "doc_id": result["doc_id"],
                "method": result.get("method", ""),
                "source_file": result.get("source_file", ""),
                "question": result["question"][:100] + "..." if len(result["question"]) > 100 else result["question"],
                "expected_answer": str(result["expected_answer"])[:100] + "..." if len(str(result["expected_answer"])) > 100 else str(result["expected_answer"]),
                "generated_answer": str(result["generated_answer"])[:200] + "..." if len(str(result["generated_answer"])) > 200 else str(result["generated_answer"]),
                "accuracy": result.get("accuracy"),
                "relevance": result.get("relevance"),
                "completeness": result.get("completeness"),
                "faithfulness": result.get("faithfulness"),
                "clarity": result.get("clarity"),
                "consistency": result.get("consistency"),
                "overall_score": result.get("overall_score"),
                "evaluation_time": result.get("evaluation_time"),
                "success": result.get("success"),
                "error": result.get("error", "")
            })
        
        summary_file = os.path.join(self.config.output_dir, "llm_evaluation_summary.csv")
        df = pd.DataFrame(summary_data)
        df.to_csv(summary_file, index=False, encoding='utf-8')

    def generate_evaluation_report(self, total_time: float):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        total_evaluations = len(self.evaluation_results)
        successful_evaluations = [r for r in self.evaluation_results if r.get("success")]
        
        if not successful_evaluations:
            self.logger.warning("æ²¡æœ‰æˆåŠŸçš„è¯„ä¼°ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return
        
        # æŒ‰æ–¹æ³•åˆ†ç»„ç»Ÿè®¡
        method_stats = {}
        source_file_stats = {}
        
        for result in successful_evaluations:
            method = result.get("method", "unknown")
            source_file = result.get("source_file", "unknown")
            
            # æ–¹æ³•ç»Ÿè®¡
            if method not in method_stats:
                method_stats[method] = []
            method_stats[method].append(result)
            
            # æºæ–‡ä»¶ç»Ÿè®¡
            if source_file not in source_file_stats:
                source_file_stats[source_file] = {}
            if method not in source_file_stats[source_file]:
                source_file_stats[source_file][method] = []
            source_file_stats[source_file][method].append(result)
        
        report = f"""
# LLMç­”æ¡ˆè¯„ä¼°æŠ¥å‘Š

## æ€»ä½“ç»Ÿè®¡
- æ€»è¯„ä¼°æ•°: {total_evaluations}
- æˆåŠŸè¯„ä¼°æ•°: {len(successful_evaluations)}
- å¤±è´¥è¯„ä¼°æ•°: {total_evaluations - len(successful_evaluations)}
- æ€»è€—æ—¶: {total_time:.2f}ç§’
- å¹³å‡æ¯è¯„ä¼°è€—æ—¶: {total_time/total_evaluations:.2f}ç§’

## è¯„ä¼°é…ç½®
- è¯„ä¼°æ¨¡å‹: {getattr(self.config, 'model', 'gpt-4o')}
- è¯„ä¼°æ¸©åº¦: 0.1
- æœ€å¤§tokens: 1000

## æ•°æ®æºç»Ÿè®¡
"""

        # æ˜¾ç¤ºæ•°æ®æºä¿¡æ¯
        for source_file, methods in source_file_stats.items():
            file_name = os.path.basename(source_file)
            total_from_source = sum(len(results) for results in methods.values())
            report += f"- {file_name}: {total_from_source} ä¸ªè¯„ä¼° ({', '.join(methods.keys())})\n"

        # ä¸ºæ¯ç§æ–¹æ³•ç”Ÿæˆç»Ÿè®¡
        for method, results in method_stats.items():
            accuracy_scores = [r.get("accuracy") for r in results if r.get("accuracy") is not None]
            
            report += f"""
## {method.upper()}æ–¹æ³•è¯„ä¼°ç»“æœ

### å‡†ç¡®æ€§ç»Ÿè®¡
- è¯„ä¼°æ ·æœ¬æ•°: {len(results)}
- å‡†ç¡®ç­”æ¡ˆæ•°: {sum(accuracy_scores)}
- å‡†ç¡®ç‡: {(sum(accuracy_scores) / len(accuracy_scores) * 100):.1f}%

"""
            
            # å¦‚æœæœ‰å…¶ä»–æŒ‡æ ‡ï¼Œä¹ŸåŒ…å«åœ¨æŠ¥å‘Šä¸­
            if any(r.get("relevance") is not None for r in results):
                relevance_scores = [r.get("relevance") for r in results if r.get("relevance") is not None]
                completeness_scores = [r.get("completeness") for r in results if r.get("completeness") is not None]
                faithfulness_scores = [r.get("faithfulness") for r in results if r.get("faithfulness") is not None]
                clarity_scores = [r.get("clarity") for r in results if r.get("clarity") is not None]
                consistency_scores = [r.get("consistency") for r in results if r.get("consistency") is not None]
                overall_scores = [r.get("overall_score") for r in results if r.get("overall_score") is not None]
                
                if relevance_scores:
                    report += f"""### å¤šç»´åº¦è¯„ä¼°ç»Ÿè®¡
- ç›¸å…³æ€§å¹³å‡åˆ†: {(sum(relevance_scores) / len(relevance_scores)):.3f}
- å®Œæ•´æ€§å¹³å‡åˆ†: {(sum(completeness_scores) / len(completeness_scores)):.3f}
- å¿ å®æ€§å¹³å‡åˆ†: {(sum(faithfulness_scores) / len(faithfulness_scores)):.3f}
- æ¸…æ™°åº¦å¹³å‡åˆ†: {(sum(clarity_scores) / len(clarity_scores)):.3f}
- ä¸€è‡´æ€§å¹³å‡åˆ†: {(sum(consistency_scores) / len(consistency_scores)):.3f}
- æ€»ä½“è¯„åˆ†å¹³å‡: {(sum(overall_scores) / len(overall_scores)):.3f}

"""

        # æ–¹æ³•å¯¹æ¯”
        if len(method_stats) > 1:
            report += """
## æ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | æ ·æœ¬æ•° | å‡†ç¡®ç‡ | å¹³å‡æ€»ä½“è¯„åˆ† |
|------|--------|--------|--------------|
"""
            for method, results in method_stats.items():
                accuracy_scores = [r.get("accuracy") for r in results if r.get("accuracy") is not None]
                overall_scores = [r.get("overall_score") for r in results if r.get("overall_score") is not None]
                
                accuracy_rate = (sum(accuracy_scores) / len(accuracy_scores) * 100) if accuracy_scores else 0
                avg_overall = (sum(overall_scores) / len(overall_scores)) if overall_scores else 0
                
                report += f"| {method} | {len(results)} | {accuracy_rate:.1f}% | {avg_overall:.3f} |\n"

        # æŒ‰æ–‡æ¡£åˆ†æï¼ˆæ˜¾ç¤ºæ‰€æœ‰æ–‡æ¡£ï¼Œä¸åªæ˜¯å¤šæ–¹æ³•çš„ï¼‰
        doc_comparison = {}
        for result in successful_evaluations:
            doc_id = result["doc_id"]
            method = result.get("method", "unknown")
            
            if doc_id not in doc_comparison:
                doc_comparison[doc_id] = {}
            if method not in doc_comparison[doc_id]:
                doc_comparison[doc_id][method] = []
            doc_comparison[doc_id][method].append(result)

        # æ˜¾ç¤ºæ‰€æœ‰æ–‡æ¡£çš„åˆ†æï¼ˆä¸è®ºæ–¹æ³•æ•°é‡ï¼‰
        if doc_comparison:
            # æ‰¾åˆ°è¢«å¤šç§æ–¹æ³•å¤„ç†çš„æ–‡æ¡£
            multi_method_docs = {doc_id: methods for doc_id, methods in doc_comparison.items() if len(methods) > 1}
            single_method_docs = {doc_id: methods for doc_id, methods in doc_comparison.items() if len(methods) == 1}
            
            if multi_method_docs:
                report += f"""
## ç›¸åŒæ–‡æ¡£çš„æ–¹æ³•å¯¹æ¯”

å…±æœ‰ {len(multi_method_docs)} ä¸ªæ–‡æ¡£è¢«å¤šç§æ–¹æ³•å¤„ç†ï¼š

| æ–‡æ¡£ID | æ–¹æ³• | å‡†ç¡®ç­”æ¡ˆæ•°/æ€»é—®é¢˜æ•° | å‡†ç¡®ç‡ |
|--------|------|-------------------|--------|
"""
                for doc_id, methods in multi_method_docs.items():
                    for method, results in methods.items():
                        accuracy_scores = [r.get("accuracy") for r in results if r.get("accuracy") is not None]
                        accuracy_count = sum(accuracy_scores)
                        total_count = len(accuracy_scores)
                        accuracy_rate = (accuracy_count / total_count * 100) if total_count > 0 else 0
                        
                        short_doc_id = doc_id[:30] + "..." if len(doc_id) > 30 else doc_id
                        report += f"| {short_doc_id} | {method} | {accuracy_count}/{total_count} | {accuracy_rate:.1f}% |\n"
            
            if single_method_docs:
                # é€‰æ‹©æ˜¾ç¤ºçš„æ–¹æ³•ï¼ˆä¼˜å…ˆVLMï¼Œç„¶åæ˜¯å…¶ä»–æ–¹æ³•ï¼‰
                available_methods = list(method_stats.keys())
                primary_method = "vlm" if "vlm" in available_methods else available_methods[0]
                
                report += f"""
## æ–‡æ¡£çº§åˆ«è¯„ä¼°ç»“æœ ({primary_method.upper()}æ–¹æ³•)

æŒ‰æ–‡æ¡£åˆ†æå‡†ç¡®ç‡è¡¨ç°ï¼š

| æ–‡æ¡£ID | å‡†ç¡®ç­”æ¡ˆæ•°/æ€»é—®é¢˜æ•° | å‡†ç¡®ç‡ | æ–‡æ¡£ç±»å‹ |
|--------|-------------------|--------|----------|
"""
                # æŒ‰å‡†ç¡®ç‡æ’åºæ˜¾ç¤ºæ–‡æ¡£
                doc_performance = []
                for doc_id, methods in single_method_docs.items():
                    if primary_method in methods:
                        results = methods[primary_method]
                        accuracy_scores = [r.get("accuracy") for r in results if r.get("accuracy") is not None]
                        accuracy_count = sum(accuracy_scores)
                        total_count = len(accuracy_scores)
                        accuracy_rate = (accuracy_count / total_count * 100) if total_count > 0 else 0
                        
                        # è·å–æ–‡æ¡£ç±»å‹ä¿¡æ¯
                        doc_types = set()
                        for r in results:
                            evidence_sources = r.get("evidence_sources", "")
                            if "arxiv" in evidence_sources.lower():
                                doc_types.add("å­¦æœ¯è®ºæ–‡")
                            elif "10k" in doc_id.lower():
                                doc_types.add("è´¢æŠ¥")
                            elif any(keyword in doc_id.lower() for keyword in ["manual", "guide", "handbook"]):
                                doc_types.add("æ‰‹å†ŒæŒ‡å—")
                            elif any(keyword in doc_id.lower() for keyword in ["report", "survey"]):
                                doc_types.add("è°ƒç ”æŠ¥å‘Š")
                            else:
                                doc_types.add("å…¶ä»–")
                        
                        doc_type = ", ".join(doc_types) if doc_types else "æœªçŸ¥"
                        
                        doc_performance.append({
                            "doc_id": doc_id,
                            "accuracy_count": accuracy_count,
                            "total_count": total_count,
                            "accuracy_rate": accuracy_rate,
                            "doc_type": doc_type
                        })
                
                # æŒ‰å‡†ç¡®ç‡é™åºæ’åº
                doc_performance.sort(key=lambda x: x["accuracy_rate"], reverse=True)
                
                # æ˜¾ç¤ºå‰30ä¸ªæ–‡æ¡£ï¼ˆé¿å…æŠ¥å‘Šè¿‡é•¿ï¼‰
                display_docs = doc_performance[:30]
                for doc_info in display_docs:
                    short_doc_id = doc_info["doc_id"][:30] + "..." if len(doc_info["doc_id"]) > 30 else doc_info["doc_id"]
                    report += f"| {short_doc_id} | {doc_info['accuracy_count']}/{doc_info['total_count']} | {doc_info['accuracy_rate']:.1f}% | {doc_info['doc_type']} |\n"
                
                if len(doc_performance) > 30:
                    report += f"\næ³¨ï¼šä»…æ˜¾ç¤ºå‰30ä¸ªæ–‡æ¡£ï¼Œæ€»å…±æœ‰{len(doc_performance)}ä¸ªæ–‡æ¡£è¢«è¯„ä¼°ã€‚\n"
                
                # æŒ‰æ–‡æ¡£ç±»å‹ç»Ÿè®¡
                type_stats = {}
                for doc_info in doc_performance:
                    doc_type = doc_info["doc_type"]
                    if doc_type not in type_stats:
                        type_stats[doc_type] = {"count": 0, "total_questions": 0, "correct_answers": 0}
                    type_stats[doc_type]["count"] += 1
                    type_stats[doc_type]["total_questions"] += doc_info["total_count"]
                    type_stats[doc_type]["correct_answers"] += doc_info["accuracy_count"]
                
                report += f"""
### æŒ‰æ–‡æ¡£ç±»å‹ç»Ÿè®¡

| æ–‡æ¡£ç±»å‹ | æ–‡æ¡£æ•°é‡ | æ€»é—®é¢˜æ•° | æ­£ç¡®ç­”æ¡ˆæ•° | å‡†ç¡®ç‡ |
|----------|----------|----------|------------|--------|
"""
                for doc_type, stats in type_stats.items():
                    accuracy_rate = (stats["correct_answers"] / stats["total_questions"] * 100) if stats["total_questions"] > 0 else 0
                    report += f"| {doc_type} | {stats['count']} | {stats['total_questions']} | {stats['correct_answers']} | {accuracy_rate:.1f}% |\n"

        report += f"""

## è¾“å‡ºæ–‡ä»¶
- è¯¦ç»†è¯„ä¼°ç»“æœ: {os.path.join(self.config.output_dir, 'llm_evaluation_results.json')}
- è¯„ä¼°ç»“æœæ‘˜è¦: {os.path.join(self.config.output_dir, 'llm_evaluation_summary.csv')}
- è¯„ä¼°æ—¥å¿—: {os.path.join(self.config.output_dir, 'llm_evaluation.log')}
"""

        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.config.output_dir, "llm_evaluation_report.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info("\n" + "="*50)
        self.logger.info("LLMè¯„ä¼°å®Œæˆ!")
        self.logger.info(f"è¯¦ç»†ç»“æœ: {os.path.join(self.config.output_dir, 'llm_evaluation_results.json')}")
        self.logger.info(f"ç»“æœæ‘˜è¦: {os.path.join(self.config.output_dir, 'llm_evaluation_summary.csv')}")
        self.logger.info(f"è¯„ä¼°æŠ¥å‘Š: {report_file}")
        self.logger.info("="*50)

    def has_any_error(self, result: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰ä»»ä½•ç±»å‹çš„é”™è¯¯"""
        # æ£€æŸ¥é¡¶å±‚error
        if result.get("error"):
            return True
        
        # æ£€æŸ¥VLMç»“æœçš„error
        vlm_result = result.get("vlm_result")
        if vlm_result and (vlm_result.get("error") or not vlm_result.get("success", True)):
            return True
        
        # æ£€æŸ¥RAG hybridç»“æœçš„error
        hybrid_result = result.get("hybrid_result")
        if hybrid_result and (hybrid_result.get("error") or not hybrid_result.get("success", True)):
            return True
        
        # æ£€æŸ¥RAG mixç»“æœçš„error
        mix_result = result.get("mix_result")
        if mix_result and (mix_result.get("error") or not mix_result.get("success", True)):
            return True
        
        # æ£€æŸ¥RAG naiveç»“æœçš„error
        naive_result = result.get("naive_result")
        if naive_result and (naive_result.get("error") or not naive_result.get("success", True)):
            return True
        
        return False
    
    def get_error_summary(self, result: Dict[str, Any]) -> str:
        """è·å–é”™è¯¯æ‘˜è¦ä¿¡æ¯"""
        errors = []
        
        # é¡¶å±‚error
        if result.get("error"):
            error_msg = result["error"]
            if "æ— æ³•æ‰¾åˆ°æ–‡æ¡£" in error_msg:
                errors.append("æ–‡æ¡£ä¸å­˜åœ¨")
            else:
                errors.append(f"é¡¶å±‚é”™è¯¯: {error_msg[:50]}...")
        
        # VLMé”™è¯¯
        vlm_result = result.get("vlm_result")
        if vlm_result and (vlm_result.get("error") or not vlm_result.get("success", True)):
            error_msg = vlm_result.get("error", "VLMæŸ¥è¯¢å¤±è´¥")
            if "context_length_exceeded" in error_msg:
                errors.append("VLM-Tokenè¶…é™")
            else:
                errors.append(f"VLMé”™è¯¯: {error_msg[:50]}...")
        
        # RAGé”™è¯¯
        hybrid_result = result.get("hybrid_result")
        if hybrid_result and (hybrid_result.get("error") or not hybrid_result.get("success", True)):
            error_msg = hybrid_result.get("error", "HybridæŸ¥è¯¢å¤±è´¥")
            errors.append(f"Hybridé”™è¯¯: {error_msg[:50]}...")
        
        mix_result = result.get("mix_result")
        if mix_result and (mix_result.get("error") or not mix_result.get("success", True)):
            error_msg = mix_result.get("error", "MixæŸ¥è¯¢å¤±è´¥")
            errors.append(f"Mixé”™è¯¯: {error_msg[:50]}...")
        
        naive_result = result.get("naive_result")
        if naive_result and (naive_result.get("error") or not naive_result.get("success", True)):
            error_msg = naive_result.get("error", "NaiveæŸ¥è¯¢å¤±è´¥")
            errors.append(f"Naiveé”™è¯¯: {error_msg[:50]}...")
        
        return "; ".join(errors) if errors else "æœªçŸ¥é”™è¯¯"

    def load_existing_evaluations(self):
        """åŠ è½½å·²æœ‰çš„è¯„ä¼°ç»“æœ"""
        results_file = os.path.join(self.config.output_dir, "llm_evaluation_results.json")
        
        if os.path.exists(results_file):
            try:
                with open(results_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                
                self.evaluation_results = existing_data
                self.logger.info(f"åŠ è½½äº† {len(existing_data)} ä¸ªå·²æœ‰çš„LLMè¯„ä¼°ç»“æœ")
                
                # æ„å»ºå¿«é€ŸæŸ¥æ‰¾ç´¢å¼•ï¼š{(doc_id, question, method): result}
                for result in existing_data:
                    key = (result["doc_id"], result["question"], result.get("method", ""))
                    self.existing_evaluations[key] = result
                
                self.logger.info(f"æ„å»ºäº† {len(self.existing_evaluations)} ä¸ªLLMè¯„ä¼°ç»“æœçš„æŸ¥æ‰¾ç´¢å¼•")
                
            except Exception as e:
                self.logger.warning(f"åŠ è½½å·²æœ‰LLMè¯„ä¼°ç»“æœæ—¶å‡ºé”™ï¼š{e}ï¼Œå°†ä»å¤´å¼€å§‹è¯„ä¼°")
                self.evaluation_results = []
                self.existing_evaluations = {}
        else:
            self.logger.info("æœªæ‰¾åˆ°å·²æœ‰LLMè¯„ä¼°ç»“æœæ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹è¯„ä¼°")

    def is_evaluation_completed(self, doc_id: str, question: str, method: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šçš„è¯„ä¼°æ˜¯å¦å·²å®Œæˆ"""
        key = (doc_id, question, method)
        if key in self.existing_evaluations:
            result = self.existing_evaluations[key]
            # æ£€æŸ¥è¯„ä¼°æ˜¯å¦æˆåŠŸå®Œæˆ
            if result.get("success") is True and result.get("accuracy") is not None:
                return True
        return False

    def get_existing_evaluation(self, doc_id: str, question: str, method: str):
        """è·å–å·²å­˜åœ¨çš„è¯„ä¼°ç»“æœ"""
        key = (doc_id, question, method)
        return self.existing_evaluations.get(key)

    def update_evaluation_results_list(self, new_results: List[Dict[str, Any]]):
        """æ›´æ–°è¯„ä¼°ç»“æœåˆ—è¡¨ï¼Œç§»é™¤æ—§çš„ç»“æœå¹¶æ·»åŠ æ–°çš„ç»“æœ"""
        # ä¸ºäº†é¿å…é‡å¤ï¼Œå…ˆç§»é™¤å·²å­˜åœ¨çš„ç›¸åŒè¯„ä¼°
        existing_keys = set()
        for result in new_results:
            key = (result["doc_id"], result["question"], result.get("method", ""))
            existing_keys.add(key)
        
        # ç§»é™¤æ—§çš„é‡å¤ç»“æœ
        self.evaluation_results = [
            r for r in self.evaluation_results 
            if (r["doc_id"], r["question"], r.get("method", "")) not in existing_keys
        ]
        
        # æ·»åŠ æ–°ç»“æœ
        self.evaluation_results.extend(new_results)
        
        self.logger.debug(f"æ›´æ–°LLMè¯„ä¼°ç»“æœï¼Œå½“å‰æ€»ç»“æœæ•°: {len(self.evaluation_results)}")

class EvaluationConfig:
    def __init__(self, args):
        # æ”¯æŒå¤šä¸ªç»“æœæ–‡ä»¶
        if hasattr(args, 'rag_results_files') and args.rag_results_files:
            self.rag_results_files = args.rag_results_files
            self.rag_results_file = None  # å…¼å®¹æ€§
        else:
            self.rag_results_file = args.rag_results_file
            self.rag_results_files = [args.rag_results_file] if args.rag_results_file else []
        
        self.output_dir = args.output_dir
        self.api_key = args.api_key
        self.base_url = args.base_url
        self.evaluation_type = args.evaluation_type
        self.max_evaluations = args.max_evaluations

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="LLMç­”æ¡ˆè¯„ä¼°å™¨")
    
    # æ”¯æŒå¤šä¸ªæ–‡ä»¶è¾“å…¥
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        "--rag-results-file", "-r",
        help="å•ä¸ªRAGè¯„ä¼°ç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰"
    )
    group.add_argument(
        "--rag-results-files", "-rf", 
        nargs="+",
        help="å¤šä¸ªç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒRAGå’ŒVLMæ··åˆï¼ŒJSONæ ¼å¼ï¼‰"
    )
    
    parser.add_argument(
        "--output-dir", "-o",
        default="./evaluation_results",
        help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼š./llm_evaluation_resultsï¼‰"
    )
    parser.add_argument(
        "--api-key",
        default=os.getenv("LLM_BINDING_API_KEY"),
        help="OpenAI APIå¯†é’¥ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡LLM_BINDING_API_KEYè¯»å–ï¼‰",
    )
    parser.add_argument(
        "--base-url",
        default=os.getenv("LLM_BINDING_HOST"),
        help="APIåŸºç¡€URLï¼ˆå¯é€‰ï¼‰",
    )
    parser.add_argument(
        "--evaluation-type", "-t",
        choices=["accuracy_only", "comprehensive"],
        default="comprehensive",
        help="è¯„ä¼°ç±»å‹ï¼šä»…å‡†ç¡®æ€§æˆ–ç»¼åˆè¯„ä¼°ï¼ˆé»˜è®¤ï¼šcomprehensiveï¼‰"
    )
    parser.add_argument(
        "--max-evaluations", "-m",
        type=int,
        help="æœ€å¤§è¯„ä¼°æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼Œç•™ç©ºåˆ™è¯„ä¼°æ‰€æœ‰ç»“æœï¼‰"
    )

    args = parser.parse_args()

    # æ£€æŸ¥APIå¯†é’¥
    if not args.api_key:
        print("é”™è¯¯ï¼šéœ€è¦OpenAI APIå¯†é’¥")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡LLM_BINDING_API_KEYæˆ–ä½¿ç”¨--api-keyé€‰é¡¹")
        return

    # æ£€æŸ¥ç»“æœæ–‡ä»¶
    files_to_check = []
    if args.rag_results_file:
        files_to_check = [args.rag_results_file]
    elif args.rag_results_files:
        files_to_check = args.rag_results_files

    for file_path in files_to_check:
        if not os.path.exists(file_path):
            print(f"é”™è¯¯ï¼šç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
            return

    # åˆ›å»ºé…ç½®
    config = EvaluationConfig(args)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = LLMAnswerEvaluator(config)
    
    # å¼€å§‹è¯„ä¼°
    print("LLMç­”æ¡ˆè¯„ä¼°å™¨")
    print("=" * 50)
    
    if config.rag_results_file:
        print(f"ç»“æœæ–‡ä»¶ï¼š{config.rag_results_file}")
    else:
        print(f"ç»“æœæ–‡ä»¶ï¼š{config.rag_results_files}")
        
    print(f"è¾“å‡ºç›®å½•ï¼š{config.output_dir}")
    print(f"è¯„ä¼°ç±»å‹ï¼š{config.evaluation_type}")
    if config.max_evaluations:
        print(f"æœ€å¤§è¯„ä¼°æ•°ï¼š{config.max_evaluations}")
    print("=" * 50)
    print("\nå¼€å§‹è¯„ä¼°...")
    
    # è¿è¡Œè¯„ä¼°
    try:
        if len(config.rag_results_files) > 1:
            # å¤šæ–‡ä»¶è¯„ä¼°
            asyncio.run(evaluator.evaluate_multiple_results(
                config.rag_results_files, 
                config.evaluation_type,
                config.max_evaluations
            ))
        else:
            # å•æ–‡ä»¶è¯„ä¼°ï¼ˆå…¼å®¹æ€§ï¼‰
            asyncio.run(evaluator.evaluate_rag_results(
                config.rag_results_files[0], 
                config.evaluation_type,
                config.max_evaluations
            ))
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"\nè¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 